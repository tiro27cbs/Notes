<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.42">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>10&nbsp; Gradient Descent: Optimization in Machine Learning ‚Äì Machine Learning and Deep Learning Notes</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script><script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./chapter9.html" rel="next">
<link href="./chapter7.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-2f5df379a58b258e96c21c0638c20c03.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-8da5b4427184b79ecddefad3d342027e.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" integrity="sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==" crossorigin="anonymous"></script>

<script type="application/javascript">define('jquery', [],function() {return window.jQuery;})</script>


</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./chapter8.html"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Gradient Descent: Optimization in Machine Learning</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Machine Learning and Deep Learning Notes</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">üöÄ Introduction</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Fundamentals of Machine Learning</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Building an End-to-End Machine Learning Pipeline</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter3.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Unsupervised Learning: Clustering Techniques</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter4.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Supervised Learning: Regression and Classification</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter5.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Dimensionality Reduction Methods</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter6.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Tree-Based Models and Ensemble Learning</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter6v2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Support Vector Machines and Model Evaluation</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter7.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Outlier Detection and Recommendation Systems</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter8.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Gradient Descent: Optimization in Machine Learning</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter9.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Neural Networks and Deep Learning Foundations</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter10.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Batch Normalization, RNN, Distributed Deep Learning and Tensorflow</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter11.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Variants of Recurrent Neural Networks (RNNs): LSTM, GRU, BiLSTM</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter12.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Autoencoders</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./summary.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Summary</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#the-intuition-behind-gradient-descent" id="toc-the-intuition-behind-gradient-descent" class="nav-link active" data-scroll-target="#the-intuition-behind-gradient-descent"><span class="header-section-number">10.1</span> The Intuition Behind Gradient Descent</a></li>
  <li><a href="#mathematical-foundation" id="toc-mathematical-foundation" class="nav-link" data-scroll-target="#mathematical-foundation"><span class="header-section-number">10.2</span> Mathematical Foundation</a></li>
  <li><a href="#how-gradient-descent-works" id="toc-how-gradient-descent-works" class="nav-link" data-scroll-target="#how-gradient-descent-works"><span class="header-section-number">10.3</span> How Gradient Descent Works</a></li>
  <li><a href="#the-learning-rate-a-delicate-balance" id="toc-the-learning-rate-a-delicate-balance" class="nav-link" data-scroll-target="#the-learning-rate-a-delicate-balance"><span class="header-section-number">10.4</span> The Learning Rate: A Delicate Balance</a>
  <ul class="collapse">
  <li><a href="#the-goldilocks-principle-in-learning-rates" id="toc-the-goldilocks-principle-in-learning-rates" class="nav-link" data-scroll-target="#the-goldilocks-principle-in-learning-rates"><span class="header-section-number">10.4.1</span> The Goldilocks Principle in Learning Rates</a></li>
  </ul></li>
  <li><a href="#challenges-in-gradient-descent" id="toc-challenges-in-gradient-descent" class="nav-link" data-scroll-target="#challenges-in-gradient-descent"><span class="header-section-number">10.5</span> Challenges in Gradient Descent</a>
  <ul class="collapse">
  <li><a href="#non-convex-cost-functions" id="toc-non-convex-cost-functions" class="nav-link" data-scroll-target="#non-convex-cost-functions"><span class="header-section-number">10.5.1</span> 1. Non-Convex Cost Functions</a></li>
  <li><a href="#the-ravine-problem" id="toc-the-ravine-problem" class="nav-link" data-scroll-target="#the-ravine-problem"><span class="header-section-number">10.5.2</span> 2. The Ravine Problem</a></li>
  <li><a href="#feature-scaling-issues" id="toc-feature-scaling-issues" class="nav-link" data-scroll-target="#feature-scaling-issues"><span class="header-section-number">10.5.3</span> 3. Feature Scaling Issues</a></li>
  <li><a href="#vanishing-and-exploding-gradients" id="toc-vanishing-and-exploding-gradients" class="nav-link" data-scroll-target="#vanishing-and-exploding-gradients"><span class="header-section-number">10.5.4</span> 4. Vanishing and Exploding Gradients</a></li>
  </ul></li>
  <li><a href="#types-of-gradient-descent" id="toc-types-of-gradient-descent" class="nav-link" data-scroll-target="#types-of-gradient-descent"><span class="header-section-number">11</span> Types of Gradient Descent</a>
  <ul class="collapse">
  <li><a href="#batch-gradient-descent" id="toc-batch-gradient-descent" class="nav-link" data-scroll-target="#batch-gradient-descent"><span class="header-section-number">11.1</span> 1. Batch Gradient Descent</a>
  <ul class="collapse">
  <li><a href="#conceptual-understanding" id="toc-conceptual-understanding" class="nav-link" data-scroll-target="#conceptual-understanding"><span class="header-section-number">11.1.1</span> Conceptual Understanding</a></li>
  <li><a href="#key-properties" id="toc-key-properties" class="nav-link" data-scroll-target="#key-properties"><span class="header-section-number">11.1.2</span> Key Properties</a></li>
  </ul></li>
  <li><a href="#stochastic-gradient-descent-sgd" id="toc-stochastic-gradient-descent-sgd" class="nav-link" data-scroll-target="#stochastic-gradient-descent-sgd"><span class="header-section-number">11.2</span> 2. Stochastic Gradient Descent (SGD)</a>
  <ul class="collapse">
  <li><a href="#conceptual-understanding-1" id="toc-conceptual-understanding-1" class="nav-link" data-scroll-target="#conceptual-understanding-1"><span class="header-section-number">11.2.1</span> Conceptual Understanding</a></li>
  <li><a href="#key-properties-1" id="toc-key-properties-1" class="nav-link" data-scroll-target="#key-properties-1"><span class="header-section-number">11.2.2</span> Key Properties</a></li>
  <li><a href="#learning-rate-scheduling-in-sgd" id="toc-learning-rate-scheduling-in-sgd" class="nav-link" data-scroll-target="#learning-rate-scheduling-in-sgd"><span class="header-section-number">11.2.3</span> Learning Rate Scheduling in SGD</a></li>
  </ul></li>
  <li><a href="#mini-batch-gradient-descent" id="toc-mini-batch-gradient-descent" class="nav-link" data-scroll-target="#mini-batch-gradient-descent"><span class="header-section-number">11.3</span> 3. Mini-Batch Gradient Descent</a>
  <ul class="collapse">
  <li><a href="#conceptual-understanding-2" id="toc-conceptual-understanding-2" class="nav-link" data-scroll-target="#conceptual-understanding-2"><span class="header-section-number">11.3.1</span> Conceptual Understanding</a></li>
  <li><a href="#key-properties-2" id="toc-key-properties-2" class="nav-link" data-scroll-target="#key-properties-2"><span class="header-section-number">11.3.2</span> Key Properties</a></li>
  </ul></li>
  <li><a href="#theoretical-understanding-of-batch-size-impact" id="toc-theoretical-understanding-of-batch-size-impact" class="nav-link" data-scroll-target="#theoretical-understanding-of-batch-size-impact"><span class="header-section-number">11.4</span> Theoretical Understanding of Batch Size Impact</a></li>
  <li><a href="#comparison-of-gradient-descent-variants" id="toc-comparison-of-gradient-descent-variants" class="nav-link" data-scroll-target="#comparison-of-gradient-descent-variants"><span class="header-section-number">11.5</span> Comparison of Gradient Descent Variants</a></li>
  <li><a href="#when-to-use-each-variant" id="toc-when-to-use-each-variant" class="nav-link" data-scroll-target="#when-to-use-each-variant"><span class="header-section-number">11.6</span> When to Use Each Variant</a></li>
  </ul></li>
  <li><a href="#advanced-optimization-techniques" id="toc-advanced-optimization-techniques" class="nav-link" data-scroll-target="#advanced-optimization-techniques"><span class="header-section-number">12</span> Advanced Optimization Techniques</a>
  <ul class="collapse">
  <li><a href="#beyond-vanilla-gradient-descent" id="toc-beyond-vanilla-gradient-descent" class="nav-link" data-scroll-target="#beyond-vanilla-gradient-descent"><span class="header-section-number">12.1</span> Beyond Vanilla Gradient Descent</a>
  <ul class="collapse">
  <li><a href="#momentum" id="toc-momentum" class="nav-link" data-scroll-target="#momentum"><span class="header-section-number">12.1.1</span> 1. Momentum</a></li>
  <li><a href="#rmsprop-root-mean-square-propagation" id="toc-rmsprop-root-mean-square-propagation" class="nav-link" data-scroll-target="#rmsprop-root-mean-square-propagation"><span class="header-section-number">12.1.2</span> 2. RMSprop (Root Mean Square Propagation)</a></li>
  <li><a href="#adam-adaptive-moment-estimation" id="toc-adam-adaptive-moment-estimation" class="nav-link" data-scroll-target="#adam-adaptive-moment-estimation"><span class="header-section-number">12.1.3</span> 3. Adam (Adaptive Moment Estimation)</a></li>
  </ul></li>
  <li><a href="#regularization" id="toc-regularization" class="nav-link" data-scroll-target="#regularization"><span class="header-section-number">12.2</span> Regularization</a>
  <ul class="collapse">
  <li><a href="#l1-regularization-lasso" id="toc-l1-regularization-lasso" class="nav-link" data-scroll-target="#l1-regularization-lasso"><span class="header-section-number">12.2.1</span> L1 Regularization (Lasso)</a></li>
  <li><a href="#l2-regularization-ridge" id="toc-l2-regularization-ridge" class="nav-link" data-scroll-target="#l2-regularization-ridge"><span class="header-section-number">12.2.2</span> L2 Regularization (Ridge)</a></li>
  <li><a href="#elastic-net" id="toc-elastic-net" class="nav-link" data-scroll-target="#elastic-net"><span class="header-section-number">12.2.3</span> Elastic Net</a></li>
  </ul></li>
  <li><a href="#early-stopping" id="toc-early-stopping" class="nav-link" data-scroll-target="#early-stopping"><span class="header-section-number">12.3</span> Early Stopping</a>
  <ul class="collapse">
  <li><a href="#conceptual-understanding-3" id="toc-conceptual-understanding-3" class="nav-link" data-scroll-target="#conceptual-understanding-3"><span class="header-section-number">12.3.1</span> Conceptual Understanding</a></li>
  </ul></li>
  <li><a href="#early-stopping-1" id="toc-early-stopping-1" class="nav-link" data-scroll-target="#early-stopping-1"><span class="header-section-number">12.4</span> Early Stopping</a></li>
  <li><a href="#batch-normalization" id="toc-batch-normalization" class="nav-link" data-scroll-target="#batch-normalization"><span class="header-section-number">12.5</span> Batch Normalization</a></li>
  </ul></li>
  <li><a href="#implementing-gradient-descent-in-popular-libraries" id="toc-implementing-gradient-descent-in-popular-libraries" class="nav-link" data-scroll-target="#implementing-gradient-descent-in-popular-libraries"><span class="header-section-number">13</span> Implementing Gradient Descent in Popular Libraries</a>
  <ul class="collapse">
  <li><a href="#using-scikit-learn" id="toc-using-scikit-learn" class="nav-link" data-scroll-target="#using-scikit-learn"><span class="header-section-number">13.1</span> Using Scikit-Learn</a></li>
  <li><a href="#using-tensorflowkeras" id="toc-using-tensorflowkeras" class="nav-link" data-scroll-target="#using-tensorflowkeras"><span class="header-section-number">13.2</span> Using TensorFlow/Keras</a></li>
  </ul></li>
  <li><a href="#conclusion" id="toc-conclusion" class="nav-link" data-scroll-target="#conclusion"><span class="header-section-number">14</span> Conclusion</a>
  <ul class="collapse">
  <li><a href="#references" id="toc-references" class="nav-link" data-scroll-target="#references"><span class="header-section-number">14.1</span> References</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Gradient Descent: Optimization in Machine Learning</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<p>Gradient Descent is a fundamental optimization algorithm in machine learning used to minimize a cost function by iteratively adjusting model parameters. It‚Äôs the engine that powers many machine learning algorithms, from simple linear regression to complex neural networks.</p>
<p>At its core, gradient descent is based on a simple yet powerful idea: to find the minimum of a function, we can follow the direction of the steepest descent. Mathematically, this direction is given by the negative gradient of the function.</p>
<section id="the-intuition-behind-gradient-descent" class="level2" data-number="10.1">
<h2 data-number="10.1" class="anchored" data-anchor-id="the-intuition-behind-gradient-descent"><span class="header-section-number">10.1</span> The Intuition Behind Gradient Descent</h2>
<p>Think of gradient descent as descending a mountain in foggy conditions where you can only see a small area around you. To reach the bottom (the minimum), you:</p>
<ol type="1">
<li>Feel the ground around you to determine the steepest downward direction (calculate the gradient)</li>
<li>Take a step in that direction (update parameters)</li>
<li>Repeat until you reach a point where all directions lead upward (convergence)</li>
</ol>
<p>The cost function represents the ‚Äúmountain landscape‚Äù in this analogy, with its valleys corresponding to low error and its peaks to high error.</p>
</section>
<section id="mathematical-foundation" class="level2" data-number="10.2">
<h2 data-number="10.2" class="anchored" data-anchor-id="mathematical-foundation"><span class="header-section-number">10.2</span> Mathematical Foundation</h2>
<p>For a function J(Œ∏), where Œ∏ represents our model parameters, the gradient ‚àáJ(Œ∏) points in the direction of the steepest increase. Therefore, to minimize J(Œ∏), we update Œ∏ as follows:</p>
<p>Œ∏ = Œ∏ - Œ±¬∑‚àáJ(Œ∏)</p>
<p>Where: - Œ± is the learning rate (step size) - ‚àáJ(Œ∏) is the gradient of the cost function with respect to parameters Œ∏</p>
<p>For a linear regression problem with mean squared error (MSE) cost function, this translates to:</p>
<p>‚àáJ(Œ∏) = (1/m) ¬∑ X^T ¬∑ (X¬∑Œ∏ - y)</p>
<p>Where: - m is the number of training examples - X is the feature matrix - y is the target vector - Œ∏ is the parameter vector</p>
</section>
<section id="how-gradient-descent-works" class="level2" data-number="10.3">
<h2 data-number="10.3" class="anchored" data-anchor-id="how-gradient-descent-works"><span class="header-section-number">10.3</span> How Gradient Descent Works</h2>
<p>Gradient Descent operates through a systematic process:</p>
<ol type="1">
<li><strong>Random Initialization</strong>: Begin with arbitrary parameter values. This represents our starting point on the ‚Äúmountain.‚Äù</li>
<li><strong>Calculate the Gradient</strong>: Compute the gradient (slope) of the cost function with respect to each parameter. This tells us which direction leads downhill most steeply.</li>
<li><strong>Update Parameters</strong>: Adjust parameters in the opposite direction of the gradient. The size of this adjustment is controlled by the learning rate.</li>
<li><strong>Repeat Until Convergence</strong>: Continue until the gradient approaches zero, indicating a minimum.</li>
</ol>
<p>The algorithm‚Äôs success hinges on the learning rate, which controls the step size in each iteration.</p>
<div id="ee78a7ac" class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.datasets <span class="im">import</span> make_regression</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.preprocessing <span class="im">import</span> StandardScaler</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate a simple regression dataset</span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>X, y <span class="op">=</span> make_regression(n_samples<span class="op">=</span><span class="dv">100</span>, n_features<span class="op">=</span><span class="dv">1</span>, noise<span class="op">=</span><span class="dv">10</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> StandardScaler().fit_transform(X)</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> y.reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Split data</span></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>X_train, X_test, y_train, y_test <span class="op">=</span> train_test_split(X, y, test_size<span class="op">=</span><span class="fl">0.2</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Implement basic gradient descent for linear regression</span></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> gradient_descent(X, y, learning_rate<span class="op">=</span><span class="fl">0.01</span>, iterations<span class="op">=</span><span class="dv">1000</span>):</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Initialize parameters (weights and bias)</span></span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>    m <span class="op">=</span> X.shape[<span class="dv">0</span>]</span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>    theta <span class="op">=</span> np.zeros((<span class="dv">2</span>, <span class="dv">1</span>))  <span class="co"># [w, b]</span></span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a>    X_b <span class="op">=</span> np.c_[np.ones((m, <span class="dv">1</span>)), X]  <span class="co"># Add intercept term</span></span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a>    <span class="co"># To store cost history</span></span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a>    cost_history <span class="op">=</span> []</span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(iterations):</span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Calculate predictions</span></span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a>        predictions <span class="op">=</span> X_b.dot(theta)</span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Calculate error</span></span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a>        error <span class="op">=</span> predictions <span class="op">-</span> y</span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Calculate gradients</span></span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a>        gradients <span class="op">=</span> (<span class="dv">1</span><span class="op">/</span>m) <span class="op">*</span> X_b.T.dot(error)</span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Update parameters</span></span>
<span id="cb1-36"><a href="#cb1-36" aria-hidden="true" tabindex="-1"></a>        theta <span class="op">=</span> theta <span class="op">-</span> learning_rate <span class="op">*</span> gradients</span>
<span id="cb1-37"><a href="#cb1-37" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb1-38"><a href="#cb1-38" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Calculate cost (MSE)</span></span>
<span id="cb1-39"><a href="#cb1-39" aria-hidden="true" tabindex="-1"></a>        cost <span class="op">=</span> (<span class="dv">1</span><span class="op">/</span>(<span class="dv">2</span><span class="op">*</span>m)) <span class="op">*</span> np.<span class="bu">sum</span>(np.square(error))</span>
<span id="cb1-40"><a href="#cb1-40" aria-hidden="true" tabindex="-1"></a>        cost_history.append(cost)</span>
<span id="cb1-41"><a href="#cb1-41" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb1-42"><a href="#cb1-42" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> theta, cost_history</span>
<span id="cb1-43"><a href="#cb1-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-44"><a href="#cb1-44" aria-hidden="true" tabindex="-1"></a><span class="co"># Run gradient descent</span></span>
<span id="cb1-45"><a href="#cb1-45" aria-hidden="true" tabindex="-1"></a>theta, cost_history <span class="op">=</span> gradient_descent(X_train, y_train)</span>
<span id="cb1-46"><a href="#cb1-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-47"><a href="#cb1-47" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualize cost over iterations</span></span>
<span id="cb1-48"><a href="#cb1-48" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">6</span>))</span>
<span id="cb1-49"><a href="#cb1-49" aria-hidden="true" tabindex="-1"></a>plt.plot(cost_history)</span>
<span id="cb1-50"><a href="#cb1-50" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Iterations'</span>)</span>
<span id="cb1-51"><a href="#cb1-51" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Cost'</span>)</span>
<span id="cb1-52"><a href="#cb1-52" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Cost vs. Iterations'</span>)</span>
<span id="cb1-53"><a href="#cb1-53" aria-hidden="true" tabindex="-1"></a>plt.grid(<span class="va">True</span>)</span>
<span id="cb1-54"><a href="#cb1-54" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb1-55"><a href="#cb1-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-56"><a href="#cb1-56" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualize the regression line</span></span>
<span id="cb1-57"><a href="#cb1-57" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">6</span>))</span>
<span id="cb1-58"><a href="#cb1-58" aria-hidden="true" tabindex="-1"></a>plt.scatter(X_train, y_train, alpha<span class="op">=</span><span class="fl">0.7</span>)</span>
<span id="cb1-59"><a href="#cb1-59" aria-hidden="true" tabindex="-1"></a>X_new <span class="op">=</span> np.array([[np.<span class="bu">min</span>(X_train)], [np.<span class="bu">max</span>(X_train)]])</span>
<span id="cb1-60"><a href="#cb1-60" aria-hidden="true" tabindex="-1"></a>X_new_b <span class="op">=</span> np.c_[np.ones((<span class="dv">2</span>, <span class="dv">1</span>)), X_new]</span>
<span id="cb1-61"><a href="#cb1-61" aria-hidden="true" tabindex="-1"></a>y_predict <span class="op">=</span> X_new_b.dot(theta)</span>
<span id="cb1-62"><a href="#cb1-62" aria-hidden="true" tabindex="-1"></a>plt.plot(X_new, y_predict, <span class="st">"r-"</span>, linewidth<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb1-63"><a href="#cb1-63" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Feature'</span>)</span>
<span id="cb1-64"><a href="#cb1-64" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Target'</span>)</span>
<span id="cb1-65"><a href="#cb1-65" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Linear Regression with Gradient Descent'</span>)</span>
<span id="cb1-66"><a href="#cb1-66" aria-hidden="true" tabindex="-1"></a>plt.grid(<span class="va">True</span>)</span>
<span id="cb1-67"><a href="#cb1-67" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb1-68"><a href="#cb1-68" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-69"><a href="#cb1-69" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Optimized parameters: w = </span><span class="sc">{</span>theta[<span class="dv">1</span>][<span class="dv">0</span>]<span class="sc">:.4f}</span><span class="ss">, b = </span><span class="sc">{</span>theta[<span class="dv">0</span>][<span class="dv">0</span>]<span class="sc">:.4f}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="chapter8_files/figure-html/cell-2-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="chapter8_files/figure-html/cell-2-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Optimized parameters: w = 39.9781, b = -4.4957</code></pre>
</div>
</div>
</section>
<section id="the-learning-rate-a-delicate-balance" class="level2" data-number="10.4">
<h2 data-number="10.4" class="anchored" data-anchor-id="the-learning-rate-a-delicate-balance"><span class="header-section-number">10.4</span> The Learning Rate: A Delicate Balance</h2>
<p>The learning rate (Œ±) is a hyperparameter that determines how large each step is during optimization. Its selection involves a fundamental trade-off:</p>
<ul>
<li><strong>Too Small</strong>: Convergence is extremely slow, requiring many iterations. This is computationally inefficient but less likely to miss the minimum.</li>
<li><strong>Too Large</strong>: The algorithm overshoots minima, potentially diverging instead of converging. This manifests as increasing cost values over iterations.</li>
</ul>
<p>An optimal rate ensures steady progress toward the minimum without overshooting. In practice, finding the right learning rate often involves experimentation.</p>
<section id="the-goldilocks-principle-in-learning-rates" class="level3" data-number="10.4.1">
<h3 data-number="10.4.1" class="anchored" data-anchor-id="the-goldilocks-principle-in-learning-rates"><span class="header-section-number">10.4.1</span> The Goldilocks Principle in Learning Rates</h3>
<p>Think of learning rate selection as the ‚ÄúGoldilocks principle‚Äù ‚Äì not too hot, not too cold, but just right:</p>
<ol type="1">
<li><strong>‚ÄúToo cold‚Äù (small Œ±)</strong>: The algorithm moves very cautiously, taking tiny steps. While it‚Äôs unlikely to overshoot, it might take an impractically long time to reach the minimum.</li>
<li><strong>‚ÄúToo hot‚Äù (large Œ±)</strong>: The algorithm takes aggressive steps and might bounce around or completely miss the minimum, potentially even diverging (costs increase).</li>
<li><strong>‚ÄúJust right‚Äù (optimal Œ±)</strong>: The algorithm makes steady progress, converging to the minimum efficiently.</li>
</ol>
<p>Learning rate scheduling techniques (discussed later) attempt to get the best of both worlds by starting with larger steps and gradually reducing them.</p>
<div id="4ce08b12" class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> compare_learning_rates(X, y, learning_rates<span class="op">=</span>[<span class="fl">0.001</span>, <span class="fl">0.01</span>, <span class="fl">0.1</span>, <span class="fl">0.5</span>], iterations<span class="op">=</span><span class="dv">100</span>):</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>    plt.figure(figsize<span class="op">=</span>(<span class="dv">12</span>, <span class="dv">8</span>))</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>    m <span class="op">=</span> X.shape[<span class="dv">0</span>]</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>    X_b <span class="op">=</span> np.c_[np.ones((m, <span class="dv">1</span>)), X]</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> lr <span class="kw">in</span> learning_rates:</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>        theta <span class="op">=</span> np.zeros((<span class="dv">2</span>, <span class="dv">1</span>))</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>        cost_history <span class="op">=</span> []</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(iterations):</span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a>            predictions <span class="op">=</span> X_b.dot(theta)</span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a>            error <span class="op">=</span> predictions <span class="op">-</span> y</span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a>            gradients <span class="op">=</span> (<span class="dv">1</span><span class="op">/</span>m) <span class="op">*</span> X_b.T.dot(error)</span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a>            theta <span class="op">=</span> theta <span class="op">-</span> lr <span class="op">*</span> gradients</span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a>            cost <span class="op">=</span> (<span class="dv">1</span><span class="op">/</span>(<span class="dv">2</span><span class="op">*</span>m)) <span class="op">*</span> np.<span class="bu">sum</span>(np.square(error))</span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a>            cost_history.append(cost)</span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a>        plt.plot(cost_history, label<span class="op">=</span><span class="ss">f'Œ± = </span><span class="sc">{</span>lr<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb3-19"><a href="#cb3-19" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb3-20"><a href="#cb3-20" aria-hidden="true" tabindex="-1"></a>    plt.xlabel(<span class="st">'Iterations'</span>)</span>
<span id="cb3-21"><a href="#cb3-21" aria-hidden="true" tabindex="-1"></a>    plt.ylabel(<span class="st">'Cost'</span>)</span>
<span id="cb3-22"><a href="#cb3-22" aria-hidden="true" tabindex="-1"></a>    plt.title(<span class="st">'Impact of Learning Rate on Convergence'</span>)</span>
<span id="cb3-23"><a href="#cb3-23" aria-hidden="true" tabindex="-1"></a>    plt.legend()</span>
<span id="cb3-24"><a href="#cb3-24" aria-hidden="true" tabindex="-1"></a>    plt.grid(<span class="va">True</span>)</span>
<span id="cb3-25"><a href="#cb3-25" aria-hidden="true" tabindex="-1"></a>    plt.show()</span>
<span id="cb3-26"><a href="#cb3-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-27"><a href="#cb3-27" aria-hidden="true" tabindex="-1"></a><span class="co"># Compare different learning rates</span></span>
<span id="cb3-28"><a href="#cb3-28" aria-hidden="true" tabindex="-1"></a>compare_learning_rates(X_train, y_train)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="chapter8_files/figure-html/cell-3-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
</section>
<section id="challenges-in-gradient-descent" class="level2" data-number="10.5">
<h2 data-number="10.5" class="anchored" data-anchor-id="challenges-in-gradient-descent"><span class="header-section-number">10.5</span> Challenges in Gradient Descent</h2>
<p>Gradient Descent faces several challenges that arise from the nature of optimization landscapes:</p>
<section id="non-convex-cost-functions" class="level3" data-number="10.5.1">
<h3 data-number="10.5.1" class="anchored" data-anchor-id="non-convex-cost-functions"><span class="header-section-number">10.5.1</span> 1. Non-Convex Cost Functions</h3>
<p>In many machine learning models, especially neural networks, the cost function isn‚Äôt convex‚Äîmeaning it has multiple local minima, saddle points, and plateaus. This creates several challenges:</p>
<ul>
<li><strong>Local Minima</strong>: The algorithm might settle in a suboptimal local minimum rather than finding the global minimum. This is particularly problematic in deep learning.</li>
<li><strong>Saddle Points</strong>: Points where the gradient is zero in all directions, but it‚Äôs neither a maximum nor a minimum. These can slow down convergence significantly.</li>
<li><strong>Plateaus</strong>: Flat regions in the cost function where the gradient is close to zero, slowing progress despite being far from a minimum.</li>
</ul>
</section>
<section id="the-ravine-problem" class="level3" data-number="10.5.2">
<h3 data-number="10.5.2" class="anchored" data-anchor-id="the-ravine-problem"><span class="header-section-number">10.5.2</span> 2. The Ravine Problem</h3>
<p>In some cost functions, the surface has elongated valley-like structures (ravines). In these cases, the gradient often points across the ravine rather than along it toward the minimum. This causes the algorithm to oscillate from one side to another, making slow progress.</p>
</section>
<section id="feature-scaling-issues" class="level3" data-number="10.5.3">
<h3 data-number="10.5.3" class="anchored" data-anchor-id="feature-scaling-issues"><span class="header-section-number">10.5.3</span> 3. Feature Scaling Issues</h3>
<p>When features have different scales, the cost function contours become elongated ellipses rather than circles. This creates a similar ravine problem, where the gradient doesn‚Äôt point directly toward the minimum.</p>
<p>To address this issue, feature scaling techniques like standardization are essential:</p>
<div id="c6a06fb6" class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Demonstrating the importance of feature scaling</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>X_multi, y_multi <span class="op">=</span> make_regression(n_samples<span class="op">=</span><span class="dv">100</span>, n_features<span class="op">=</span><span class="dv">2</span>, noise<span class="op">=</span><span class="dv">10</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Scale one feature to be much larger</span></span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>X_multi[:, <span class="dv">1</span>] <span class="op">=</span> X_multi[:, <span class="dv">1</span>] <span class="op">*</span> <span class="dv">100</span></span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Split data</span></span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>X_train_unscaled, X_test_unscaled, y_train_multi, y_test_multi <span class="op">=</span> train_test_split(</span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a>    X_multi, y_multi.reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>), test_size<span class="op">=</span><span class="fl">0.2</span>, random_state<span class="op">=</span><span class="dv">42</span></span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Create scaled version</span></span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a>scaler <span class="op">=</span> StandardScaler()</span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a>X_train_scaled <span class="op">=</span> scaler.fit_transform(X_train_unscaled)</span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Run gradient descent on both scaled and unscaled data</span></span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> run_gd_experiment(X, y, title, learning_rate<span class="op">=</span><span class="fl">0.01</span>, iterations<span class="op">=</span><span class="dv">500</span>):</span>
<span id="cb4-18"><a href="#cb4-18" aria-hidden="true" tabindex="-1"></a>    m <span class="op">=</span> X.shape[<span class="dv">0</span>]</span>
<span id="cb4-19"><a href="#cb4-19" aria-hidden="true" tabindex="-1"></a>    n_features <span class="op">=</span> X.shape[<span class="dv">1</span>]</span>
<span id="cb4-20"><a href="#cb4-20" aria-hidden="true" tabindex="-1"></a>    theta <span class="op">=</span> np.zeros((n_features <span class="op">+</span> <span class="dv">1</span>, <span class="dv">1</span>))</span>
<span id="cb4-21"><a href="#cb4-21" aria-hidden="true" tabindex="-1"></a>    X_b <span class="op">=</span> np.c_[np.ones((m, <span class="dv">1</span>)), X]</span>
<span id="cb4-22"><a href="#cb4-22" aria-hidden="true" tabindex="-1"></a>    cost_history <span class="op">=</span> []</span>
<span id="cb4-23"><a href="#cb4-23" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb4-24"><a href="#cb4-24" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(iterations):</span>
<span id="cb4-25"><a href="#cb4-25" aria-hidden="true" tabindex="-1"></a>        predictions <span class="op">=</span> X_b.dot(theta)</span>
<span id="cb4-26"><a href="#cb4-26" aria-hidden="true" tabindex="-1"></a>        error <span class="op">=</span> predictions <span class="op">-</span> y</span>
<span id="cb4-27"><a href="#cb4-27" aria-hidden="true" tabindex="-1"></a>        gradients <span class="op">=</span> (<span class="dv">1</span><span class="op">/</span>m) <span class="op">*</span> X_b.T.dot(error)</span>
<span id="cb4-28"><a href="#cb4-28" aria-hidden="true" tabindex="-1"></a>        theta <span class="op">=</span> theta <span class="op">-</span> learning_rate <span class="op">*</span> gradients</span>
<span id="cb4-29"><a href="#cb4-29" aria-hidden="true" tabindex="-1"></a>        cost <span class="op">=</span> (<span class="dv">1</span><span class="op">/</span>(<span class="dv">2</span><span class="op">*</span>m)) <span class="op">*</span> np.<span class="bu">sum</span>(np.square(error))</span>
<span id="cb4-30"><a href="#cb4-30" aria-hidden="true" tabindex="-1"></a>        cost_history.append(cost)</span>
<span id="cb4-31"><a href="#cb4-31" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb4-32"><a href="#cb4-32" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> cost_history</span>
<span id="cb4-33"><a href="#cb4-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-34"><a href="#cb4-34" aria-hidden="true" tabindex="-1"></a><span class="co"># Compare convergence with and without scaling</span></span>
<span id="cb4-35"><a href="#cb4-35" aria-hidden="true" tabindex="-1"></a>cost_unscaled <span class="op">=</span> run_gd_experiment(X_train_unscaled, y_train_multi, <span class="st">"Unscaled"</span>)</span>
<span id="cb4-36"><a href="#cb4-36" aria-hidden="true" tabindex="-1"></a>cost_scaled <span class="op">=</span> run_gd_experiment(X_train_scaled, y_train_multi, <span class="st">"Scaled"</span>)</span>
<span id="cb4-37"><a href="#cb4-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-38"><a href="#cb4-38" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">6</span>))</span>
<span id="cb4-39"><a href="#cb4-39" aria-hidden="true" tabindex="-1"></a>plt.plot(cost_unscaled, label<span class="op">=</span><span class="st">'Unscaled Features'</span>)</span>
<span id="cb4-40"><a href="#cb4-40" aria-hidden="true" tabindex="-1"></a>plt.plot(cost_scaled, label<span class="op">=</span><span class="st">'Scaled Features'</span>)</span>
<span id="cb4-41"><a href="#cb4-41" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Iterations'</span>)</span>
<span id="cb4-42"><a href="#cb4-42" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Cost'</span>)</span>
<span id="cb4-43"><a href="#cb4-43" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Impact of Feature Scaling on Convergence'</span>)</span>
<span id="cb4-44"><a href="#cb4-44" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb4-45"><a href="#cb4-45" aria-hidden="true" tabindex="-1"></a>plt.grid(<span class="va">True</span>)</span>
<span id="cb4-46"><a href="#cb4-46" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>C:\Users\roess\AppData\Local\Temp\ipykernel_16444\433023471.py:29: RuntimeWarning:

overflow encountered in square

C:\Users\roess\Documents\repos\Notes\myvenv312\Lib\site-packages\numpy\_core\fromnumeric.py:86: RuntimeWarning:

overflow encountered in reduce

C:\Users\roess\AppData\Local\Temp\ipykernel_16444\433023471.py:28: RuntimeWarning:

invalid value encountered in subtract
</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="chapter8_files/figure-html/cell-4-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="vanishing-and-exploding-gradients" class="level3" data-number="10.5.4">
<h3 data-number="10.5.4" class="anchored" data-anchor-id="vanishing-and-exploding-gradients"><span class="header-section-number">10.5.4</span> 4. Vanishing and Exploding Gradients</h3>
<p>In deep neural networks, gradients can either: - Become extremely small (vanish) as they‚Äôre propagated backward through many layers - Become extremely large (explode) in certain network architectures</p>
<p>Both problems hamper effective training. Modern techniques like batch normalization, residual connections, and careful weight initialization help address these issues.</p>
</section>
</section>
<section id="types-of-gradient-descent" class="level1" data-number="11">
<h1 data-number="11"><span class="header-section-number">11</span> Types of Gradient Descent</h1>
<p>There are three main variants of Gradient Descent, each with its own strengths and use cases. The primary difference is how much data they use to compute each parameter update.</p>
<section id="batch-gradient-descent" class="level2" data-number="11.1">
<h2 data-number="11.1" class="anchored" data-anchor-id="batch-gradient-descent"><span class="header-section-number">11.1</span> 1. Batch Gradient Descent</h2>
<p>Batch Gradient Descent calculates the gradient using the entire dataset before updating parameters. This ensures consistent steps toward the minimum but can be computationally expensive for large datasets.</p>
<section id="conceptual-understanding" class="level3" data-number="11.1.1">
<h3 data-number="11.1.1" class="anchored" data-anchor-id="conceptual-understanding"><span class="header-section-number">11.1.1</span> Conceptual Understanding</h3>
<p>Think of Batch GD as carefully surveying the entire mountain before taking each step. This provides the most accurate direction but requires significant effort before making any progress.</p>
</section>
<section id="key-properties" class="level3" data-number="11.1.2">
<h3 data-number="11.1.2" class="anchored" data-anchor-id="key-properties"><span class="header-section-number">11.1.2</span> Key Properties</h3>
<ul>
<li><strong>Deterministic</strong>: Always takes the same path for the same starting point</li>
<li><strong>Memory-intensive</strong>: Must process the entire dataset in memory</li>
<li><strong>Smooth convergence</strong>: Progress is steady with minimal fluctuations</li>
<li><strong>Computationally expensive</strong>: Especially for large datasets</li>
</ul>
<div id="0b3a6fb6" class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> batch_gradient_descent(X, y, learning_rate<span class="op">=</span><span class="fl">0.01</span>, iterations<span class="op">=</span><span class="dv">1000</span>):</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>    m <span class="op">=</span> X.shape[<span class="dv">0</span>]</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>    n_features <span class="op">=</span> X.shape[<span class="dv">1</span>]</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>    theta <span class="op">=</span> np.zeros((n_features <span class="op">+</span> <span class="dv">1</span>, <span class="dv">1</span>))</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>    X_b <span class="op">=</span> np.c_[np.ones((m, <span class="dv">1</span>)), X]</span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>    cost_history <span class="op">=</span> []</span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(iterations):</span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Use entire dataset for each update</span></span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a>        predictions <span class="op">=</span> X_b.dot(theta)</span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a>        error <span class="op">=</span> predictions <span class="op">-</span> y</span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a>        gradients <span class="op">=</span> (<span class="dv">1</span><span class="op">/</span>m) <span class="op">*</span> X_b.T.dot(error)</span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a>        theta <span class="op">=</span> theta <span class="op">-</span> learning_rate <span class="op">*</span> gradients</span>
<span id="cb6-14"><a href="#cb6-14" aria-hidden="true" tabindex="-1"></a>        cost <span class="op">=</span> (<span class="dv">1</span><span class="op">/</span>(<span class="dv">2</span><span class="op">*</span>m)) <span class="op">*</span> np.<span class="bu">sum</span>(np.square(error))</span>
<span id="cb6-15"><a href="#cb6-15" aria-hidden="true" tabindex="-1"></a>        cost_history.append(cost)</span>
<span id="cb6-16"><a href="#cb6-16" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb6-17"><a href="#cb6-17" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Early stopping condition (optional)</span></span>
<span id="cb6-18"><a href="#cb6-18" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> i <span class="op">&gt;</span> <span class="dv">0</span> <span class="kw">and</span> <span class="bu">abs</span>(cost_history[i] <span class="op">-</span> cost_history[i<span class="op">-</span><span class="dv">1</span>]) <span class="op">&lt;</span> <span class="fl">1e-10</span>:</span>
<span id="cb6-19"><a href="#cb6-19" aria-hidden="true" tabindex="-1"></a>            <span class="cf">break</span></span>
<span id="cb6-20"><a href="#cb6-20" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb6-21"><a href="#cb6-21" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> theta, cost_history</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
</section>
<section id="stochastic-gradient-descent-sgd" class="level2" data-number="11.2">
<h2 data-number="11.2" class="anchored" data-anchor-id="stochastic-gradient-descent-sgd"><span class="header-section-number">11.2</span> 2. Stochastic Gradient Descent (SGD)</h2>
<p>Stochastic Gradient Descent updates parameters using a single randomly selected training instance at each step. This makes it faster and more memory-efficient but introduces more randomness.</p>
<section id="conceptual-understanding-1" class="level3" data-number="11.2.1">
<h3 data-number="11.2.1" class="anchored" data-anchor-id="conceptual-understanding-1"><span class="header-section-number">11.2.1</span> Conceptual Understanding</h3>
<p>SGD is like taking quick steps based on whatever small part of the mountain you can immediately see. The direction might not be perfect, but you can take many more steps in the same amount of time.</p>
</section>
<section id="key-properties-1" class="level3" data-number="11.2.2">
<h3 data-number="11.2.2" class="anchored" data-anchor-id="key-properties-1"><span class="header-section-number">11.2.2</span> Key Properties</h3>
<ul>
<li><strong>Randomized</strong>: Takes a different path each time</li>
<li><strong>Memory-efficient</strong>: Processes one example at a time</li>
<li><strong>Noisy convergence</strong>: Path includes fluctuations and bounces</li>
<li><strong>Ability to escape local minima</strong>: Randomness helps jump out of suboptimal valleys</li>
<li><strong>Can work with streaming data</strong>: Doesn‚Äôt need the entire dataset at once</li>
</ul>
<div id="64c001d2" class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> stochastic_gradient_descent(X, y, learning_rate<span class="op">=</span><span class="fl">0.01</span>, epochs<span class="op">=</span><span class="dv">10</span>):</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>    m <span class="op">=</span> X.shape[<span class="dv">0</span>]</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>    n_features <span class="op">=</span> X.shape[<span class="dv">1</span>]</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>    theta <span class="op">=</span> np.zeros((n_features <span class="op">+</span> <span class="dv">1</span>, <span class="dv">1</span>))</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>    X_b <span class="op">=</span> np.c_[np.ones((m, <span class="dv">1</span>)), X]</span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>    cost_history <span class="op">=</span> []</span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(epochs):</span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Shuffle data at the beginning of each epoch</span></span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a>        indices <span class="op">=</span> np.random.permutation(m)</span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a>        X_shuffled <span class="op">=</span> X_b[indices]</span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a>        y_shuffled <span class="op">=</span> y[indices]</span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb7-14"><a href="#cb7-14" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(m):</span>
<span id="cb7-15"><a href="#cb7-15" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Use a single instance for each update</span></span>
<span id="cb7-16"><a href="#cb7-16" aria-hidden="true" tabindex="-1"></a>            xi <span class="op">=</span> X_shuffled[i:i<span class="op">+</span><span class="dv">1</span>]</span>
<span id="cb7-17"><a href="#cb7-17" aria-hidden="true" tabindex="-1"></a>            yi <span class="op">=</span> y_shuffled[i:i<span class="op">+</span><span class="dv">1</span>]</span>
<span id="cb7-18"><a href="#cb7-18" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb7-19"><a href="#cb7-19" aria-hidden="true" tabindex="-1"></a>            prediction <span class="op">=</span> xi.dot(theta)</span>
<span id="cb7-20"><a href="#cb7-20" aria-hidden="true" tabindex="-1"></a>            error <span class="op">=</span> prediction <span class="op">-</span> yi</span>
<span id="cb7-21"><a href="#cb7-21" aria-hidden="true" tabindex="-1"></a>            gradients <span class="op">=</span> xi.T.dot(error)</span>
<span id="cb7-22"><a href="#cb7-22" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb7-23"><a href="#cb7-23" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Learning rate scheduling (optional)</span></span>
<span id="cb7-24"><a href="#cb7-24" aria-hidden="true" tabindex="-1"></a>            lr <span class="op">=</span> learning_rate <span class="op">/</span> (epoch <span class="op">*</span> m <span class="op">+</span> i <span class="op">+</span> <span class="dv">1</span>)<span class="op">**</span><span class="fl">0.5</span></span>
<span id="cb7-25"><a href="#cb7-25" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb7-26"><a href="#cb7-26" aria-hidden="true" tabindex="-1"></a>            theta <span class="op">=</span> theta <span class="op">-</span> lr <span class="op">*</span> gradients</span>
<span id="cb7-27"><a href="#cb7-27" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb7-28"><a href="#cb7-28" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Calculate cost after each epoch (using full dataset)</span></span>
<span id="cb7-29"><a href="#cb7-29" aria-hidden="true" tabindex="-1"></a>        predictions <span class="op">=</span> X_b.dot(theta)</span>
<span id="cb7-30"><a href="#cb7-30" aria-hidden="true" tabindex="-1"></a>        cost <span class="op">=</span> (<span class="dv">1</span><span class="op">/</span>(<span class="dv">2</span><span class="op">*</span>m)) <span class="op">*</span> np.<span class="bu">sum</span>(np.square(predictions <span class="op">-</span> y))</span>
<span id="cb7-31"><a href="#cb7-31" aria-hidden="true" tabindex="-1"></a>        cost_history.append(cost)</span>
<span id="cb7-32"><a href="#cb7-32" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb7-33"><a href="#cb7-33" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> theta, cost_history</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="learning-rate-scheduling-in-sgd" class="level3" data-number="11.2.3">
<h3 data-number="11.2.3" class="anchored" data-anchor-id="learning-rate-scheduling-in-sgd"><span class="header-section-number">11.2.3</span> Learning Rate Scheduling in SGD</h3>
<p>Learning rate scheduling gradually reduces the learning rate over time, helping SGD settle at a minimum. This combines the benefits of larger initial steps (faster progress) with smaller later steps (precision).</p>
<p>Common scheduling strategies include:</p>
<ol type="1">
<li><strong>Time-based decay</strong>: Œ± = Œ±‚ÇÄ / (1 + kt)</li>
<li><strong>Step decay</strong>: Œ± = Œ±‚ÇÄ √ó 0.5^(t/œÑ)</li>
<li><strong>Exponential decay</strong>: Œ± = Œ±‚ÇÄ √ó e^(-kt)</li>
</ol>
<p>Where: - Œ±‚ÇÄ is the initial learning rate - t is the iteration number or epoch - k and œÑ are hyperparameters</p>
<div id="e9791033" class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> learning_rate_schedule(initial_lr, epoch, decay_rate<span class="op">=</span><span class="fl">0.01</span>):</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> initial_lr <span class="op">/</span> (<span class="dv">1</span> <span class="op">+</span> decay_rate <span class="op">*</span> epoch)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
</section>
<section id="mini-batch-gradient-descent" class="level2" data-number="11.3">
<h2 data-number="11.3" class="anchored" data-anchor-id="mini-batch-gradient-descent"><span class="header-section-number">11.3</span> 3. Mini-Batch Gradient Descent</h2>
<p>Mini-Batch Gradient Descent combines the best of both worlds: it updates parameters using small random subsets (mini-batches) of the training data.</p>
<section id="conceptual-understanding-2" class="level3" data-number="11.3.1">
<h3 data-number="11.3.1" class="anchored" data-anchor-id="conceptual-understanding-2"><span class="header-section-number">11.3.1</span> Conceptual Understanding</h3>
<p>This is like surveying a representative sample of the mountain before each step. The direction isn‚Äôt as precise as Batch GD but is more accurate than SGD, while maintaining computational efficiency.</p>
</section>
<section id="key-properties-2" class="level3" data-number="11.3.2">
<h3 data-number="11.3.2" class="anchored" data-anchor-id="key-properties-2"><span class="header-section-number">11.3.2</span> Key Properties</h3>
<ul>
<li><strong>Semi-randomized</strong>: Less variance than SGD, more than Batch GD</li>
<li><strong>Good memory efficiency</strong>: Only needs to process a batch at a time</li>
<li><strong>Vectorization-friendly</strong>: Can leverage parallel processing</li>
<li><strong>Balance of stability and speed</strong>: Smoother convergence than SGD, faster than Batch GD</li>
<li><strong>Industry standard</strong>: Most practical implementations use this approach</li>
</ul>
<div id="ce338798" class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> mini_batch_gradient_descent(X, y, batch_size<span class="op">=</span><span class="dv">32</span>, learning_rate<span class="op">=</span><span class="fl">0.01</span>, epochs<span class="op">=</span><span class="dv">10</span>):</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>    m <span class="op">=</span> X.shape[<span class="dv">0</span>]</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>    n_features <span class="op">=</span> X.shape[<span class="dv">1</span>]</span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a>    theta <span class="op">=</span> np.zeros((n_features <span class="op">+</span> <span class="dv">1</span>, <span class="dv">1</span>))</span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a>    X_b <span class="op">=</span> np.c_[np.ones((m, <span class="dv">1</span>)), X]</span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a>    cost_history <span class="op">=</span> []</span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a>    n_batches <span class="op">=</span> <span class="bu">int</span>(np.ceil(m <span class="op">/</span> batch_size))</span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(epochs):</span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Shuffle data at the beginning of each epoch</span></span>
<span id="cb9-12"><a href="#cb9-12" aria-hidden="true" tabindex="-1"></a>        indices <span class="op">=</span> np.random.permutation(m)</span>
<span id="cb9-13"><a href="#cb9-13" aria-hidden="true" tabindex="-1"></a>        X_shuffled <span class="op">=</span> X_b[indices]</span>
<span id="cb9-14"><a href="#cb9-14" aria-hidden="true" tabindex="-1"></a>        y_shuffled <span class="op">=</span> y[indices]</span>
<span id="cb9-15"><a href="#cb9-15" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb9-16"><a href="#cb9-16" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> batch <span class="kw">in</span> <span class="bu">range</span>(n_batches):</span>
<span id="cb9-17"><a href="#cb9-17" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Calculate batch indices</span></span>
<span id="cb9-18"><a href="#cb9-18" aria-hidden="true" tabindex="-1"></a>            start_idx <span class="op">=</span> batch <span class="op">*</span> batch_size</span>
<span id="cb9-19"><a href="#cb9-19" aria-hidden="true" tabindex="-1"></a>            end_idx <span class="op">=</span> <span class="bu">min</span>((batch <span class="op">+</span> <span class="dv">1</span>) <span class="op">*</span> batch_size, m)</span>
<span id="cb9-20"><a href="#cb9-20" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb9-21"><a href="#cb9-21" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Extract mini-batch</span></span>
<span id="cb9-22"><a href="#cb9-22" aria-hidden="true" tabindex="-1"></a>            X_mini <span class="op">=</span> X_shuffled[start_idx:end_idx]</span>
<span id="cb9-23"><a href="#cb9-23" aria-hidden="true" tabindex="-1"></a>            y_mini <span class="op">=</span> y_shuffled[start_idx:end_idx]</span>
<span id="cb9-24"><a href="#cb9-24" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb9-25"><a href="#cb9-25" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Update parameters using mini-batch</span></span>
<span id="cb9-26"><a href="#cb9-26" aria-hidden="true" tabindex="-1"></a>            predictions <span class="op">=</span> X_mini.dot(theta)</span>
<span id="cb9-27"><a href="#cb9-27" aria-hidden="true" tabindex="-1"></a>            error <span class="op">=</span> predictions <span class="op">-</span> y_mini</span>
<span id="cb9-28"><a href="#cb9-28" aria-hidden="true" tabindex="-1"></a>            gradients <span class="op">=</span> (<span class="dv">1</span><span class="op">/</span><span class="bu">len</span>(X_mini)) <span class="op">*</span> X_mini.T.dot(error)</span>
<span id="cb9-29"><a href="#cb9-29" aria-hidden="true" tabindex="-1"></a>            theta <span class="op">=</span> theta <span class="op">-</span> learning_rate <span class="op">*</span> gradients</span>
<span id="cb9-30"><a href="#cb9-30" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb9-31"><a href="#cb9-31" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Calculate cost after each epoch (using full dataset)</span></span>
<span id="cb9-32"><a href="#cb9-32" aria-hidden="true" tabindex="-1"></a>        predictions <span class="op">=</span> X_b.dot(theta)</span>
<span id="cb9-33"><a href="#cb9-33" aria-hidden="true" tabindex="-1"></a>        cost <span class="op">=</span> (<span class="dv">1</span><span class="op">/</span>(<span class="dv">2</span><span class="op">*</span>m)) <span class="op">*</span> np.<span class="bu">sum</span>(np.square(predictions <span class="op">-</span> y))</span>
<span id="cb9-34"><a href="#cb9-34" aria-hidden="true" tabindex="-1"></a>        cost_history.append(cost)</span>
<span id="cb9-35"><a href="#cb9-35" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb9-36"><a href="#cb9-36" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> theta, cost_history</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
</section>
<section id="theoretical-understanding-of-batch-size-impact" class="level2" data-number="11.4">
<h2 data-number="11.4" class="anchored" data-anchor-id="theoretical-understanding-of-batch-size-impact"><span class="header-section-number">11.4</span> Theoretical Understanding of Batch Size Impact</h2>
<p>The batch size directly affects:</p>
<ol type="1">
<li><strong>Gradient estimation quality</strong>: Larger batches provide more accurate gradient estimates</li>
<li><strong>Update frequency</strong>: Smaller batches allow more frequent updates</li>
<li><strong>Generalization performance</strong>: Research suggests very large batches may lead to poorer generalization</li>
<li><strong>Hardware utilization</strong>: Optimal batch sizes leverage hardware parallelism</li>
</ol>
</section>
<section id="comparison-of-gradient-descent-variants" class="level2" data-number="11.5">
<h2 data-number="11.5" class="anchored" data-anchor-id="comparison-of-gradient-descent-variants"><span class="header-section-number">11.5</span> Comparison of Gradient Descent Variants</h2>
<p>Let‚Äôs compare all three approaches:</p>
<div id="ee07e7e8" class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> compare_gd_variants(X, y):</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Prepare data</span></span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a>    m <span class="op">=</span> X.shape[<span class="dv">0</span>]</span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a>    X_scaled <span class="op">=</span> StandardScaler().fit_transform(X)</span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Run Batch GD</span></span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a>    theta_bgd, cost_bgd <span class="op">=</span> batch_gradient_descent(X_scaled, y, learning_rate<span class="op">=</span><span class="fl">0.1</span>, iterations<span class="op">=</span><span class="dv">100</span>)</span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Run SGD</span></span>
<span id="cb10-10"><a href="#cb10-10" aria-hidden="true" tabindex="-1"></a>    theta_sgd, cost_sgd <span class="op">=</span> stochastic_gradient_descent(X_scaled, y, learning_rate<span class="op">=</span><span class="fl">0.1</span>, epochs<span class="op">=</span><span class="dv">20</span>)</span>
<span id="cb10-11"><a href="#cb10-11" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb10-12"><a href="#cb10-12" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Run Mini-batch GD</span></span>
<span id="cb10-13"><a href="#cb10-13" aria-hidden="true" tabindex="-1"></a>    theta_mbgd, cost_mbgd <span class="op">=</span> mini_batch_gradient_descent(X_scaled, y, batch_size<span class="op">=</span><span class="dv">16</span>, learning_rate<span class="op">=</span><span class="fl">0.1</span>, epochs<span class="op">=</span><span class="dv">20</span>)</span>
<span id="cb10-14"><a href="#cb10-14" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb10-15"><a href="#cb10-15" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Visualize convergence</span></span>
<span id="cb10-16"><a href="#cb10-16" aria-hidden="true" tabindex="-1"></a>    plt.figure(figsize<span class="op">=</span>(<span class="dv">12</span>, <span class="dv">8</span>))</span>
<span id="cb10-17"><a href="#cb10-17" aria-hidden="true" tabindex="-1"></a>    plt.plot(cost_bgd, label<span class="op">=</span><span class="st">'Batch GD'</span>)</span>
<span id="cb10-18"><a href="#cb10-18" aria-hidden="true" tabindex="-1"></a>    plt.plot(<span class="bu">range</span>(<span class="dv">0</span>, <span class="bu">len</span>(cost_sgd) <span class="op">*</span> <span class="dv">5</span>, <span class="dv">5</span>), cost_sgd, label<span class="op">=</span><span class="st">'SGD'</span>)  <span class="co"># Adjusted for epoch count</span></span>
<span id="cb10-19"><a href="#cb10-19" aria-hidden="true" tabindex="-1"></a>    plt.plot(<span class="bu">range</span>(<span class="dv">0</span>, <span class="bu">len</span>(cost_mbgd) <span class="op">*</span> <span class="dv">5</span>, <span class="dv">5</span>), cost_mbgd, label<span class="op">=</span><span class="st">'Mini-batch GD'</span>)  <span class="co"># Adjusted for epoch count</span></span>
<span id="cb10-20"><a href="#cb10-20" aria-hidden="true" tabindex="-1"></a>    plt.xlabel(<span class="st">'Iterations/Epochs'</span>)</span>
<span id="cb10-21"><a href="#cb10-21" aria-hidden="true" tabindex="-1"></a>    plt.ylabel(<span class="st">'Cost'</span>)</span>
<span id="cb10-22"><a href="#cb10-22" aria-hidden="true" tabindex="-1"></a>    plt.title(<span class="st">'Convergence Comparison of Gradient Descent Variants'</span>)</span>
<span id="cb10-23"><a href="#cb10-23" aria-hidden="true" tabindex="-1"></a>    plt.legend()</span>
<span id="cb10-24"><a href="#cb10-24" aria-hidden="true" tabindex="-1"></a>    plt.grid(<span class="va">True</span>)</span>
<span id="cb10-25"><a href="#cb10-25" aria-hidden="true" tabindex="-1"></a>    plt.show()</span>
<span id="cb10-26"><a href="#cb10-26" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb10-27"><a href="#cb10-27" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> theta_bgd, theta_sgd, theta_mbgd</span>
<span id="cb10-28"><a href="#cb10-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-29"><a href="#cb10-29" aria-hidden="true" tabindex="-1"></a><span class="co"># Compare all three variants</span></span>
<span id="cb10-30"><a href="#cb10-30" aria-hidden="true" tabindex="-1"></a>theta_bgd, theta_sgd, theta_mbgd <span class="op">=</span> compare_gd_variants(X_train, y_train)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="chapter8_files/figure-html/cell-9-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="when-to-use-each-variant" class="level2" data-number="11.6">
<h2 data-number="11.6" class="anchored" data-anchor-id="when-to-use-each-variant"><span class="header-section-number">11.6</span> When to Use Each Variant</h2>
<table class="caption-top table">
<colgroup>
<col style="width: 13%">
<col style="width: 20%">
<col style="width: 23%">
<col style="width: 30%">
<col style="width: 12%">
</colgroup>
<thead>
<tr class="header">
<th>Algorithm</th>
<th>Large Datasets</th>
<th>Memory Efficiency</th>
<th>Convergence Stability</th>
<th>Use Case</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Batch GD</td>
<td>Slow</td>
<td>High memory usage</td>
<td>Very stable</td>
<td>Small datasets, need for deterministic solution</td>
</tr>
<tr class="even">
<td>SGD</td>
<td>Fast</td>
<td>Very efficient</td>
<td>Noisy, may not settle exactly</td>
<td>Very large datasets, online learning</td>
</tr>
<tr class="odd">
<td>Mini-batch GD</td>
<td>Fast</td>
<td>Efficient</td>
<td>Good balance</td>
<td>Most practical applications, especially deep learning</td>
</tr>
</tbody>
</table>
</section>
</section>
<section id="advanced-optimization-techniques" class="level1" data-number="12">
<h1 data-number="12"><span class="header-section-number">12</span> Advanced Optimization Techniques</h1>
<section id="beyond-vanilla-gradient-descent" class="level2" data-number="12.1">
<h2 data-number="12.1" class="anchored" data-anchor-id="beyond-vanilla-gradient-descent"><span class="header-section-number">12.1</span> Beyond Vanilla Gradient Descent</h2>
<p>While gradient descent is powerful, several advanced optimization algorithms build upon it to address its limitations:</p>
<section id="momentum" class="level3" data-number="12.1.1">
<h3 data-number="12.1.1" class="anchored" data-anchor-id="momentum"><span class="header-section-number">12.1.1</span> 1. Momentum</h3>
<p>Momentum adds a ‚Äúvelocity‚Äù term that accumulates past gradients, helping overcome: - Small local variations (noise) - Plateaus where gradients are small - The ravine problem by dampening oscillations</p>
<p>The update rule becomes: v = Œ≥v - Œ±‚àáJ(Œ∏) Œ∏ = Œ∏ + v</p>
<p>Where: - v is the velocity vector (initialized to zeros) - Œ≥ is the momentum coefficient (typically 0.9) - Œ± is the learning rate</p>
<p>Conceptually, momentum is like rolling a ball down the hill ‚Äì it builds up velocity in consistent directions and dampens oscillations perpendicular to the main direction.</p>
</section>
<section id="rmsprop-root-mean-square-propagation" class="level3" data-number="12.1.2">
<h3 data-number="12.1.2" class="anchored" data-anchor-id="rmsprop-root-mean-square-propagation"><span class="header-section-number">12.1.2</span> 2. RMSprop (Root Mean Square Propagation)</h3>
<p>RMSprop adapts the learning rate for each parameter based on the historical gradient magnitudes:</p>
<p>s = Œ≤s + (1-Œ≤)(‚àáJ(Œ∏))¬≤ Œ∏ = Œ∏ - Œ±/‚àö(s+Œµ) * ‚àáJ(Œ∏)</p>
<p>Where: - s tracks the exponentially weighted average of squared gradients - Œ≤ is typically 0.9 - Œµ is a small value to prevent division by zero</p>
<p>This allows different parameters to have different effective learning rates based on their gradient histories.</p>
</section>
<section id="adam-adaptive-moment-estimation" class="level3" data-number="12.1.3">
<h3 data-number="12.1.3" class="anchored" data-anchor-id="adam-adaptive-moment-estimation"><span class="header-section-number">12.1.3</span> 3. Adam (Adaptive Moment Estimation)</h3>
<p>Adam combines ideas from both momentum and RMSprop:</p>
<p>m = Œ≤‚ÇÅm + (1-Œ≤‚ÇÅ)‚àáJ(Œ∏) // First moment (momentum) v = Œ≤‚ÇÇv + (1-Œ≤‚ÇÇ)(‚àáJ(Œ∏))¬≤ // Second moment (RMSprop) mÃÇ = m/(1-Œ≤‚ÇÅ·µó) // Bias correction vÃÇ = v/(1-Œ≤‚ÇÇ·µó) // Bias correction Œ∏ = Œ∏ - Œ± * mÃÇ/‚àö(vÃÇ+Œµ)</p>
<p>Where typical values are Œ≤‚ÇÅ=0.9, Œ≤‚ÇÇ=0.999, and Œµ=10‚Åª‚Å∏.</p>
<p>Adam is currently one of the most popular optimization algorithms for deep learning due to its excellent performance across a wide range of problems.</p>
</section>
</section>
<section id="regularization" class="level2" data-number="12.2">
<h2 data-number="12.2" class="anchored" data-anchor-id="regularization"><span class="header-section-number">12.2</span> Regularization</h2>
<p>Regularization prevents overfitting by adding a penalty to the cost function based on the size of model parameters. This creates a balance between fitting the training data well and maintaining simpler models.</p>
<section id="l1-regularization-lasso" class="level3" data-number="12.2.1">
<h3 data-number="12.2.1" class="anchored" data-anchor-id="l1-regularization-lasso"><span class="header-section-number">12.2.1</span> L1 Regularization (Lasso)</h3>
<p>L1 regularization adds a penalty proportional to the absolute value of weights, often resulting in sparse models with some coefficients set to zero (feature selection).</p>
<p>J_L1(Œ∏) = J(Œ∏) + Œ±‚àë|Œ∏·µ¢|</p>
<p>The gradient includes an additional term: sign(Œ∏·µ¢) for each parameter.</p>
<div id="dead98ef" class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> lasso_gradient_descent(X, y, learning_rate<span class="op">=</span><span class="fl">0.01</span>, iterations<span class="op">=</span><span class="dv">1000</span>, alpha<span class="op">=</span><span class="fl">0.1</span>):</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>    m <span class="op">=</span> X.shape[<span class="dv">0</span>]</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>    n_features <span class="op">=</span> X.shape[<span class="dv">1</span>]</span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a>    theta <span class="op">=</span> np.zeros((n_features <span class="op">+</span> <span class="dv">1</span>, <span class="dv">1</span>))</span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a>    X_b <span class="op">=</span> np.c_[np.ones((m, <span class="dv">1</span>)), X]</span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a>    cost_history <span class="op">=</span> []</span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(iterations):</span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"></a>        predictions <span class="op">=</span> X_b.dot(theta)</span>
<span id="cb11-10"><a href="#cb11-10" aria-hidden="true" tabindex="-1"></a>        error <span class="op">=</span> predictions <span class="op">-</span> y</span>
<span id="cb11-11"><a href="#cb11-11" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb11-12"><a href="#cb11-12" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Gradient for the intercept (no regularization)</span></span>
<span id="cb11-13"><a href="#cb11-13" aria-hidden="true" tabindex="-1"></a>        gradient_intercept <span class="op">=</span> (<span class="dv">1</span><span class="op">/</span>m) <span class="op">*</span> X_b[:, <span class="dv">0</span>:<span class="dv">1</span>].T.dot(error)</span>
<span id="cb11-14"><a href="#cb11-14" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb11-15"><a href="#cb11-15" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Gradient for the weights (with L1 regularization)</span></span>
<span id="cb11-16"><a href="#cb11-16" aria-hidden="true" tabindex="-1"></a>        gradients_weights <span class="op">=</span> (<span class="dv">1</span><span class="op">/</span>m) <span class="op">*</span> X_b[:, <span class="dv">1</span>:].T.dot(error) <span class="op">+</span> (alpha<span class="op">/</span>m) <span class="op">*</span> np.sign(theta[<span class="dv">1</span>:])</span>
<span id="cb11-17"><a href="#cb11-17" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb11-18"><a href="#cb11-18" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Update parameters</span></span>
<span id="cb11-19"><a href="#cb11-19" aria-hidden="true" tabindex="-1"></a>        theta[<span class="dv">0</span>] <span class="op">=</span> theta[<span class="dv">0</span>] <span class="op">-</span> learning_rate <span class="op">*</span> gradient_intercept</span>
<span id="cb11-20"><a href="#cb11-20" aria-hidden="true" tabindex="-1"></a>        theta[<span class="dv">1</span>:] <span class="op">=</span> theta[<span class="dv">1</span>:] <span class="op">-</span> learning_rate <span class="op">*</span> gradients_weights</span>
<span id="cb11-21"><a href="#cb11-21" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb11-22"><a href="#cb11-22" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Calculate cost with L1 regularization</span></span>
<span id="cb11-23"><a href="#cb11-23" aria-hidden="true" tabindex="-1"></a>        mse <span class="op">=</span> (<span class="dv">1</span><span class="op">/</span>(<span class="dv">2</span><span class="op">*</span>m)) <span class="op">*</span> np.<span class="bu">sum</span>(np.square(error))</span>
<span id="cb11-24"><a href="#cb11-24" aria-hidden="true" tabindex="-1"></a>        l1_penalty <span class="op">=</span> (alpha<span class="op">/</span>m) <span class="op">*</span> np.<span class="bu">sum</span>(np.<span class="bu">abs</span>(theta[<span class="dv">1</span>:]))</span>
<span id="cb11-25"><a href="#cb11-25" aria-hidden="true" tabindex="-1"></a>        cost <span class="op">=</span> mse <span class="op">+</span> l1_penalty</span>
<span id="cb11-26"><a href="#cb11-26" aria-hidden="true" tabindex="-1"></a>        cost_history.append(cost)</span>
<span id="cb11-27"><a href="#cb11-27" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb11-28"><a href="#cb11-28" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> theta, cost_history</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="l2-regularization-ridge" class="level3" data-number="12.2.2">
<h3 data-number="12.2.2" class="anchored" data-anchor-id="l2-regularization-ridge"><span class="header-section-number">12.2.2</span> L2 Regularization (Ridge)</h3>
<p>L2 regularization adds a penalty proportional to the square of weights, shrinking all coefficients but rarely setting them to exactly zero.</p>
<p>J_L2(Œ∏) = J(Œ∏) + Œ±‚àëŒ∏·µ¢¬≤/2</p>
<p>The gradient includes an additional term: Œ±Œ∏·µ¢ for each parameter.</p>
<div id="ce4b5e45" class="cell" data-execution_count="10">
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> ridge_gradient_descent(X, y, learning_rate<span class="op">=</span><span class="fl">0.01</span>, iterations<span class="op">=</span><span class="dv">1000</span>, alpha<span class="op">=</span><span class="fl">0.1</span>):</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>    m <span class="op">=</span> X.shape[<span class="dv">0</span>]</span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a>    n_features <span class="op">=</span> X.shape[<span class="dv">1</span>]</span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a>    theta <span class="op">=</span> np.zeros((n_features <span class="op">+</span> <span class="dv">1</span>, <span class="dv">1</span>))</span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a>    X_b <span class="op">=</span> np.c_[np.ones((m, <span class="dv">1</span>)), X]</span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a>    cost_history <span class="op">=</span> []</span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(iterations):</span>
<span id="cb12-9"><a href="#cb12-9" aria-hidden="true" tabindex="-1"></a>        predictions <span class="op">=</span> X_b.dot(theta)</span>
<span id="cb12-10"><a href="#cb12-10" aria-hidden="true" tabindex="-1"></a>        error <span class="op">=</span> predictions <span class="op">-</span> y</span>
<span id="cb12-11"><a href="#cb12-11" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb12-12"><a href="#cb12-12" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Gradient for the intercept (no regularization)</span></span>
<span id="cb12-13"><a href="#cb12-13" aria-hidden="true" tabindex="-1"></a>        gradient_intercept <span class="op">=</span> (<span class="dv">1</span><span class="op">/</span>m) <span class="op">*</span> X_b[:, <span class="dv">0</span>:<span class="dv">1</span>].T.dot(error)</span>
<span id="cb12-14"><a href="#cb12-14" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb12-15"><a href="#cb12-15" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Gradient for the weights (with L2 regularization)</span></span>
<span id="cb12-16"><a href="#cb12-16" aria-hidden="true" tabindex="-1"></a>        gradients_weights <span class="op">=</span> (<span class="dv">1</span><span class="op">/</span>m) <span class="op">*</span> X_b[:, <span class="dv">1</span>:].T.dot(error) <span class="op">+</span> (alpha<span class="op">/</span>m) <span class="op">*</span> <span class="dv">2</span> <span class="op">*</span> theta[<span class="dv">1</span>:]</span>
<span id="cb12-17"><a href="#cb12-17" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb12-18"><a href="#cb12-18" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Update parameters</span></span>
<span id="cb12-19"><a href="#cb12-19" aria-hidden="true" tabindex="-1"></a>        theta[<span class="dv">0</span>] <span class="op">=</span> theta[<span class="dv">0</span>] <span class="op">-</span> learning_rate <span class="op">*</span> gradient_intercept</span>
<span id="cb12-20"><a href="#cb12-20" aria-hidden="true" tabindex="-1"></a>        theta[<span class="dv">1</span>:] <span class="op">=</span> theta[<span class="dv">1</span>:] <span class="op">-</span> learning_rate <span class="op">*</span> gradients_weights</span>
<span id="cb12-21"><a href="#cb12-21" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb12-22"><a href="#cb12-22" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Calculate cost with L2 regularization</span></span>
<span id="cb12-23"><a href="#cb12-23" aria-hidden="true" tabindex="-1"></a>        mse <span class="op">=</span> (<span class="dv">1</span><span class="op">/</span>(<span class="dv">2</span><span class="op">*</span>m)) <span class="op">*</span> np.<span class="bu">sum</span>(np.square(error))</span>
<span id="cb12-24"><a href="#cb12-24" aria-hidden="true" tabindex="-1"></a>        l2_penalty <span class="op">=</span> (alpha<span class="op">/</span>(<span class="dv">2</span><span class="op">*</span>m)) <span class="op">*</span> np.<span class="bu">sum</span>(np.square(theta[<span class="dv">1</span>:]))</span>
<span id="cb12-25"><a href="#cb12-25" aria-hidden="true" tabindex="-1"></a>        cost <span class="op">=</span> mse <span class="op">+</span> l2_penalty</span>
<span id="cb12-26"><a href="#cb12-26" aria-hidden="true" tabindex="-1"></a>        cost_history.append(cost)</span>
<span id="cb12-27"><a href="#cb12-27" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb12-28"><a href="#cb12-28" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> theta, cost_history</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="elastic-net" class="level3" data-number="12.2.3">
<h3 data-number="12.2.3" class="anchored" data-anchor-id="elastic-net"><span class="header-section-number">12.2.3</span> Elastic Net</h3>
<p>Elastic Net combines both L1 and L2 regularization, providing a balance between the feature selection properties of L1 and the parameter shrinking of L2:</p>
<p>J_EN(Œ∏) = J(Œ∏) + Œ±œÅ‚àë|Œ∏·µ¢| + Œ±(1-œÅ)‚àëŒ∏·µ¢¬≤/2</p>
<p>Where œÅ controls the balance between L1 and L2 penalties.</p>
<div id="fba52e60" class="cell" data-execution_count="11">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> elastic_net_gradient_descent(X, y, learning_rate<span class="op">=</span><span class="fl">0.01</span>, iterations<span class="op">=</span><span class="dv">1000</span>, alpha<span class="op">=</span><span class="fl">0.1</span>, l1_ratio<span class="op">=</span><span class="fl">0.5</span>):</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>    m <span class="op">=</span> X.shape[<span class="dv">0</span>]</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a>    n_features <span class="op">=</span> X.shape[<span class="dv">1</span>]</span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a>    theta <span class="op">=</span> np.zeros((n_features <span class="op">+</span> <span class="dv">1</span>, <span class="dv">1</span>))</span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a>    X_b <span class="op">=</span> np.c_[np.ones((m, <span class="dv">1</span>)), X]</span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a>    cost_history <span class="op">=</span> []</span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(iterations):</span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a>        predictions <span class="op">=</span> X_b.dot(theta)</span>
<span id="cb13-10"><a href="#cb13-10" aria-hidden="true" tabindex="-1"></a>        error <span class="op">=</span> predictions <span class="op">-</span> y</span>
<span id="cb13-11"><a href="#cb13-11" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb13-12"><a href="#cb13-12" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Gradient for the intercept (no regularization)</span></span>
<span id="cb13-13"><a href="#cb13-13" aria-hidden="true" tabindex="-1"></a>        gradient_intercept <span class="op">=</span> (<span class="dv">1</span><span class="op">/</span>m) <span class="op">*</span> X_b[:, <span class="dv">0</span>:<span class="dv">1</span>].T.dot(error)</span>
<span id="cb13-14"><a href="#cb13-14" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb13-15"><a href="#cb13-15" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Gradient for the weights (with Elastic Net regularization)</span></span>
<span id="cb13-16"><a href="#cb13-16" aria-hidden="true" tabindex="-1"></a>        l1_grad <span class="op">=</span> (alpha<span class="op">/</span>m) <span class="op">*</span> l1_ratio <span class="op">*</span> np.sign(theta[<span class="dv">1</span>:])</span>
<span id="cb13-17"><a href="#cb13-17" aria-hidden="true" tabindex="-1"></a>        l2_grad <span class="op">=</span> (alpha<span class="op">/</span>m) <span class="op">*</span> (<span class="dv">1</span> <span class="op">-</span> l1_ratio) <span class="op">*</span> <span class="dv">2</span> <span class="op">*</span> theta[<span class="dv">1</span>:]</span>
<span id="cb13-18"><a href="#cb13-18" aria-hidden="true" tabindex="-1"></a>        gradients_weights <span class="op">=</span> (<span class="dv">1</span><span class="op">/</span>m) <span class="op">*</span> X_b[:, <span class="dv">1</span>:].T.dot(error) <span class="op">+</span> l1_grad <span class="op">+</span> l2_grad</span>
<span id="cb13-19"><a href="#cb13-19" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb13-20"><a href="#cb13-20" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Update parameters</span></span>
<span id="cb13-21"><a href="#cb13-21" aria-hidden="true" tabindex="-1"></a>        theta[<span class="dv">0</span>] <span class="op">=</span> theta[<span class="dv">0</span>] <span class="op">-</span> learning_rate <span class="op">*</span> gradient_intercept</span>
<span id="cb13-22"><a href="#cb13-22" aria-hidden="true" tabindex="-1"></a>        theta[<span class="dv">1</span>:] <span class="op">=</span> theta[<span class="dv">1</span>:] <span class="op">-</span> learning_rate <span class="op">*</span> gradients_weights</span>
<span id="cb13-23"><a href="#cb13-23" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb13-24"><a href="#cb13-24" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Calculate cost with Elastic Net regularization</span></span>
<span id="cb13-25"><a href="#cb13-25" aria-hidden="true" tabindex="-1"></a>        mse <span class="op">=</span> (<span class="dv">1</span><span class="op">/</span>(<span class="dv">2</span><span class="op">*</span>m)) <span class="op">*</span> np.<span class="bu">sum</span>(np.square(error))</span>
<span id="cb13-26"><a href="#cb13-26" aria-hidden="true" tabindex="-1"></a>        l1_penalty <span class="op">=</span> (alpha <span class="op">*</span> l1_ratio<span class="op">/</span>m) <span class="op">*</span> np.<span class="bu">sum</span>(np.<span class="bu">abs</span>(theta[<span class="dv">1</span>:]))</span>
<span id="cb13-27"><a href="#cb13-27" aria-hidden="true" tabindex="-1"></a>        l2_penalty <span class="op">=</span> (alpha <span class="op">*</span> (<span class="dv">1</span> <span class="op">-</span> l1_ratio)<span class="op">/</span>(<span class="dv">2</span><span class="op">*</span>m)) <span class="op">*</span> np.<span class="bu">sum</span>(np.square(theta[<span class="dv">1</span>:]))</span>
<span id="cb13-28"><a href="#cb13-28" aria-hidden="true" tabindex="-1"></a>        cost <span class="op">=</span> mse <span class="op">+</span> l1_penalty <span class="op">+</span> l2_penalty</span>
<span id="cb13-29"><a href="#cb13-29" aria-hidden="true" tabindex="-1"></a>        cost_history.append(cost)</span>
<span id="cb13-30"><a href="#cb13-30" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb13-31"><a href="#cb13-31" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> theta, cost_history</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
</section>
<section id="early-stopping" class="level2" data-number="12.3">
<h2 data-number="12.3" class="anchored" data-anchor-id="early-stopping"><span class="header-section-number">12.3</span> Early Stopping</h2>
<p>Early stopping prevents overfitting by monitoring validation error and stopping training when it starts to increase. This technique leverages the observation that models typically fit the training data progressively better over time while their performance on unseen data eventually starts to degrade.</p>
<section id="conceptual-understanding-3" class="level3" data-number="12.3.1">
<h3 data-number="12.3.1" class="anchored" data-anchor-id="conceptual-understanding-3"><span class="header-section-number">12.3.1</span> Conceptual Understanding</h3>
<p>Early stopping can be viewed as a form of implicit regularization: - During initial training, the model learns general patterns that apply to both training and validation data - Later in training, the model starts to memorize training examples (overfit), harming generalization</p>
<p>By stopping ‚Äúearly‚Äù when validation performance starts to deteriorate, we capture the model at its optimal generalization point.</p>
</section>
</section>
<section id="early-stopping-1" class="level2" data-number="12.4">
<h2 data-number="12.4" class="anchored" data-anchor-id="early-stopping-1"><span class="header-section-number">12.4</span> Early Stopping</h2>
<p>Early stopping prevents overfitting by monitoring validation error and stopping training when it starts to increase.</p>
<div id="c06d282e" class="cell" data-execution_count="12">
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> gradient_descent_with_early_stopping(X_train, y_train, X_val, y_val, learning_rate<span class="op">=</span><span class="fl">0.01</span>, max_iterations<span class="op">=</span><span class="dv">1000</span>, patience<span class="op">=</span><span class="dv">5</span>):</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>    m_train <span class="op">=</span> X_train.shape[<span class="dv">0</span>]</span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a>    n_features <span class="op">=</span> X_train.shape[<span class="dv">1</span>]</span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a>    theta <span class="op">=</span> np.zeros((n_features <span class="op">+</span> <span class="dv">1</span>, <span class="dv">1</span>))</span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a>    X_train_b <span class="op">=</span> np.c_[np.ones((m_train, <span class="dv">1</span>)), X_train]</span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a>    X_val_b <span class="op">=</span> np.c_[np.ones((X_val.shape[<span class="dv">0</span>], <span class="dv">1</span>)), X_val]</span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a>    train_cost_history <span class="op">=</span> []</span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a>    val_cost_history <span class="op">=</span> []</span>
<span id="cb14-10"><a href="#cb14-10" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb14-11"><a href="#cb14-11" aria-hidden="true" tabindex="-1"></a>    best_val_cost <span class="op">=</span> <span class="bu">float</span>(<span class="st">'inf'</span>)</span>
<span id="cb14-12"><a href="#cb14-12" aria-hidden="true" tabindex="-1"></a>    best_theta <span class="op">=</span> <span class="va">None</span></span>
<span id="cb14-13"><a href="#cb14-13" aria-hidden="true" tabindex="-1"></a>    patience_counter <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb14-14"><a href="#cb14-14" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb14-15"><a href="#cb14-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(max_iterations):</span>
<span id="cb14-16"><a href="#cb14-16" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Train step</span></span>
<span id="cb14-17"><a href="#cb14-17" aria-hidden="true" tabindex="-1"></a>        predictions <span class="op">=</span> X_train_b.dot(theta)</span>
<span id="cb14-18"><a href="#cb14-18" aria-hidden="true" tabindex="-1"></a>        error <span class="op">=</span> predictions <span class="op">-</span> y_train</span>
<span id="cb14-19"><a href="#cb14-19" aria-hidden="true" tabindex="-1"></a>        gradients <span class="op">=</span> (<span class="dv">1</span><span class="op">/</span>m_train) <span class="op">*</span> X_train_b.T.dot(error)</span>
<span id="cb14-20"><a href="#cb14-20" aria-hidden="true" tabindex="-1"></a>        theta <span class="op">=</span> theta <span class="op">-</span> learning_rate <span class="op">*</span> gradients</span>
<span id="cb14-21"><a href="#cb14-21" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb14-22"><a href="#cb14-22" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Calculate training cost</span></span>
<span id="cb14-23"><a href="#cb14-23" aria-hidden="true" tabindex="-1"></a>        train_cost <span class="op">=</span> (<span class="dv">1</span><span class="op">/</span>(<span class="dv">2</span><span class="op">*</span>m_train)) <span class="op">*</span> np.<span class="bu">sum</span>(np.square(error))</span>
<span id="cb14-24"><a href="#cb14-24" aria-hidden="true" tabindex="-1"></a>        train_cost_history.append(train_cost)</span>
<span id="cb14-25"><a href="#cb14-25" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb14-26"><a href="#cb14-26" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Calculate validation cost</span></span>
<span id="cb14-27"><a href="#cb14-27" aria-hidden="true" tabindex="-1"></a>        val_predictions <span class="op">=</span> X_val_b.dot(theta)</span>
<span id="cb14-28"><a href="#cb14-28" aria-hidden="true" tabindex="-1"></a>        val_error <span class="op">=</span> val_predictions <span class="op">-</span> y_val</span>
<span id="cb14-29"><a href="#cb14-29" aria-hidden="true" tabindex="-1"></a>        val_cost <span class="op">=</span> (<span class="dv">1</span><span class="op">/</span>(<span class="dv">2</span><span class="op">*</span>X_val.shape[<span class="dv">0</span>])) <span class="op">*</span> np.<span class="bu">sum</span>(np.square(val_error))</span>
<span id="cb14-30"><a href="#cb14-30" aria-hidden="true" tabindex="-1"></a>        val_cost_history.append(val_cost)</span>
<span id="cb14-31"><a href="#cb14-31" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb14-32"><a href="#cb14-32" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Early stopping logic</span></span>
<span id="cb14-33"><a href="#cb14-33" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> val_cost <span class="op">&lt;</span> best_val_cost:</span>
<span id="cb14-34"><a href="#cb14-34" aria-hidden="true" tabindex="-1"></a>            best_val_cost <span class="op">=</span> val_cost</span>
<span id="cb14-35"><a href="#cb14-35" aria-hidden="true" tabindex="-1"></a>            best_theta <span class="op">=</span> theta.copy()</span>
<span id="cb14-36"><a href="#cb14-36" aria-hidden="true" tabindex="-1"></a>            patience_counter <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb14-37"><a href="#cb14-37" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb14-38"><a href="#cb14-38" aria-hidden="true" tabindex="-1"></a>            patience_counter <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb14-39"><a href="#cb14-39" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb14-40"><a href="#cb14-40" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> patience_counter <span class="op">&gt;=</span> patience:</span>
<span id="cb14-41"><a href="#cb14-41" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="ss">f"Early stopping at iteration </span><span class="sc">{</span>i<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb14-42"><a href="#cb14-42" aria-hidden="true" tabindex="-1"></a>            <span class="cf">break</span></span>
<span id="cb14-43"><a href="#cb14-43" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb14-44"><a href="#cb14-44" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Visualize training and validation costs</span></span>
<span id="cb14-45"><a href="#cb14-45" aria-hidden="true" tabindex="-1"></a>    plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">6</span>))</span>
<span id="cb14-46"><a href="#cb14-46" aria-hidden="true" tabindex="-1"></a>    plt.plot(train_cost_history, label<span class="op">=</span><span class="st">'Training Cost'</span>)</span>
<span id="cb14-47"><a href="#cb14-47" aria-hidden="true" tabindex="-1"></a>    plt.plot(val_cost_history, label<span class="op">=</span><span class="st">'Validation Cost'</span>)</span>
<span id="cb14-48"><a href="#cb14-48" aria-hidden="true" tabindex="-1"></a>    plt.xlabel(<span class="st">'Iterations'</span>)</span>
<span id="cb14-49"><a href="#cb14-49" aria-hidden="true" tabindex="-1"></a>    plt.ylabel(<span class="st">'Cost'</span>)</span>
<span id="cb14-50"><a href="#cb14-50" aria-hidden="true" tabindex="-1"></a>    plt.title(<span class="st">'Training and Validation Costs with Early Stopping'</span>)</span>
<span id="cb14-51"><a href="#cb14-51" aria-hidden="true" tabindex="-1"></a>    plt.legend()</span>
<span id="cb14-52"><a href="#cb14-52" aria-hidden="true" tabindex="-1"></a>    plt.grid(<span class="va">True</span>)</span>
<span id="cb14-53"><a href="#cb14-53" aria-hidden="true" tabindex="-1"></a>    plt.show()</span>
<span id="cb14-54"><a href="#cb14-54" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb14-55"><a href="#cb14-55" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> best_theta, train_cost_history, val_cost_history</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="batch-normalization" class="level2" data-number="12.5">
<h2 data-number="12.5" class="anchored" data-anchor-id="batch-normalization"><span class="header-section-number">12.5</span> Batch Normalization</h2>
<p>Batch Normalization improves training by normalizing layer inputs, addressing issues like internal covariate shift and vanishing gradients.</p>
<div id="fb0513b8" class="cell" data-execution_count="13">
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> batch_normalize(X, epsilon<span class="op">=</span><span class="fl">1e-8</span>):</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Simple implementation of batch normalization for a mini-batch"""</span></span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Calculate mean and variance along the batch dimension</span></span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a>    batch_mean <span class="op">=</span> np.mean(X, axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a>    batch_var <span class="op">=</span> np.var(X, axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Normalize</span></span>
<span id="cb15-8"><a href="#cb15-8" aria-hidden="true" tabindex="-1"></a>    X_normalized <span class="op">=</span> (X <span class="op">-</span> batch_mean) <span class="op">/</span> np.sqrt(batch_var <span class="op">+</span> epsilon)</span>
<span id="cb15-9"><a href="#cb15-9" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb15-10"><a href="#cb15-10" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Parameters for scale and shift (in practice, these would be learned)</span></span>
<span id="cb15-11"><a href="#cb15-11" aria-hidden="true" tabindex="-1"></a>    gamma <span class="op">=</span> np.ones(X.shape[<span class="dv">1</span>])</span>
<span id="cb15-12"><a href="#cb15-12" aria-hidden="true" tabindex="-1"></a>    beta <span class="op">=</span> np.zeros(X.shape[<span class="dv">1</span>])</span>
<span id="cb15-13"><a href="#cb15-13" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb15-14"><a href="#cb15-14" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Scale and shift</span></span>
<span id="cb15-15"><a href="#cb15-15" aria-hidden="true" tabindex="-1"></a>    X_bn <span class="op">=</span> gamma <span class="op">*</span> X_normalized <span class="op">+</span> beta</span>
<span id="cb15-16"><a href="#cb15-16" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb15-17"><a href="#cb15-17" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> X_bn</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
</section>
<section id="implementing-gradient-descent-in-popular-libraries" class="level1" data-number="13">
<h1 data-number="13"><span class="header-section-number">13</span> Implementing Gradient Descent in Popular Libraries</h1>
<section id="using-scikit-learn" class="level2" data-number="13.1">
<h2 data-number="13.1" class="anchored" data-anchor-id="using-scikit-learn"><span class="header-section-number">13.1</span> Using Scikit-Learn</h2>
<div id="688a295f" class="cell" data-execution_count="14">
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> SGDRegressor, SGDClassifier</span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a><span class="co"># For regression</span></span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a>sgd_reg <span class="op">=</span> SGDRegressor(max_iter<span class="op">=</span><span class="dv">1000</span>, tol<span class="op">=</span><span class="fl">1e-3</span>, penalty<span class="op">=</span><span class="st">'l2'</span>, alpha<span class="op">=</span><span class="fl">0.01</span>, learning_rate<span class="op">=</span><span class="st">'optimal'</span>)</span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a>sgd_reg.fit(X_train, y_train.ravel())</span>
<span id="cb16-6"><a href="#cb16-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-7"><a href="#cb16-7" aria-hidden="true" tabindex="-1"></a><span class="co"># For classification</span></span>
<span id="cb16-8"><a href="#cb16-8" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.datasets <span class="im">import</span> make_classification</span>
<span id="cb16-9"><a href="#cb16-9" aria-hidden="true" tabindex="-1"></a>X_class, y_class <span class="op">=</span> make_classification(n_samples<span class="op">=</span><span class="dv">1000</span>, n_features<span class="op">=</span><span class="dv">20</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb16-10"><a href="#cb16-10" aria-hidden="true" tabindex="-1"></a>X_train_class, X_test_class, y_train_class, y_test_class <span class="op">=</span> train_test_split(X_class, y_class, test_size<span class="op">=</span><span class="fl">0.2</span>)</span>
<span id="cb16-11"><a href="#cb16-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-12"><a href="#cb16-12" aria-hidden="true" tabindex="-1"></a>sgd_clf <span class="op">=</span> SGDClassifier(max_iter<span class="op">=</span><span class="dv">1000</span>, tol<span class="op">=</span><span class="fl">1e-3</span>, penalty<span class="op">=</span><span class="st">'l2'</span>, alpha<span class="op">=</span><span class="fl">0.01</span>, learning_rate<span class="op">=</span><span class="st">'optimal'</span>)</span>
<span id="cb16-13"><a href="#cb16-13" aria-hidden="true" tabindex="-1"></a>sgd_clf.fit(X_train_class, y_train_class)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="14">
<style>#sk-container-id-1 {
  /* Definition of color scheme common for light and dark mode */
  --sklearn-color-text: #000;
  --sklearn-color-text-muted: #666;
  --sklearn-color-line: gray;
  /* Definition of color scheme for unfitted estimators */
  --sklearn-color-unfitted-level-0: #fff5e6;
  --sklearn-color-unfitted-level-1: #f6e4d2;
  --sklearn-color-unfitted-level-2: #ffe0b3;
  --sklearn-color-unfitted-level-3: chocolate;
  /* Definition of color scheme for fitted estimators */
  --sklearn-color-fitted-level-0: #f0f8ff;
  --sklearn-color-fitted-level-1: #d4ebff;
  --sklearn-color-fitted-level-2: #b3dbfd;
  --sklearn-color-fitted-level-3: cornflowerblue;

  /* Specific color for light theme */
  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));
  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));
  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));
  --sklearn-color-icon: #696969;

  @media (prefers-color-scheme: dark) {
    /* Redefinition of color scheme for dark theme */
    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));
    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));
    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));
    --sklearn-color-icon: #878787;
  }
}

#sk-container-id-1 {
  color: var(--sklearn-color-text);
}

#sk-container-id-1 pre {
  padding: 0;
}

#sk-container-id-1 input.sk-hidden--visually {
  border: 0;
  clip: rect(1px 1px 1px 1px);
  clip: rect(1px, 1px, 1px, 1px);
  height: 1px;
  margin: -1px;
  overflow: hidden;
  padding: 0;
  position: absolute;
  width: 1px;
}

#sk-container-id-1 div.sk-dashed-wrapped {
  border: 1px dashed var(--sklearn-color-line);
  margin: 0 0.4em 0.5em 0.4em;
  box-sizing: border-box;
  padding-bottom: 0.4em;
  background-color: var(--sklearn-color-background);
}

#sk-container-id-1 div.sk-container {
  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`
     but bootstrap.min.css set `[hidden] { display: none !important; }`
     so we also need the `!important` here to be able to override the
     default hidden behavior on the sphinx rendered scikit-learn.org.
     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */
  display: inline-block !important;
  position: relative;
}

#sk-container-id-1 div.sk-text-repr-fallback {
  display: none;
}

div.sk-parallel-item,
div.sk-serial,
div.sk-item {
  /* draw centered vertical line to link estimators */
  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));
  background-size: 2px 100%;
  background-repeat: no-repeat;
  background-position: center center;
}

/* Parallel-specific style estimator block */

#sk-container-id-1 div.sk-parallel-item::after {
  content: "";
  width: 100%;
  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);
  flex-grow: 1;
}

#sk-container-id-1 div.sk-parallel {
  display: flex;
  align-items: stretch;
  justify-content: center;
  background-color: var(--sklearn-color-background);
  position: relative;
}

#sk-container-id-1 div.sk-parallel-item {
  display: flex;
  flex-direction: column;
}

#sk-container-id-1 div.sk-parallel-item:first-child::after {
  align-self: flex-end;
  width: 50%;
}

#sk-container-id-1 div.sk-parallel-item:last-child::after {
  align-self: flex-start;
  width: 50%;
}

#sk-container-id-1 div.sk-parallel-item:only-child::after {
  width: 0;
}

/* Serial-specific style estimator block */

#sk-container-id-1 div.sk-serial {
  display: flex;
  flex-direction: column;
  align-items: center;
  background-color: var(--sklearn-color-background);
  padding-right: 1em;
  padding-left: 1em;
}


/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is
clickable and can be expanded/collapsed.
- Pipeline and ColumnTransformer use this feature and define the default style
- Estimators will overwrite some part of the style using the `sk-estimator` class
*/

/* Pipeline and ColumnTransformer style (default) */

#sk-container-id-1 div.sk-toggleable {
  /* Default theme specific background. It is overwritten whether we have a
  specific estimator or a Pipeline/ColumnTransformer */
  background-color: var(--sklearn-color-background);
}

/* Toggleable label */
#sk-container-id-1 label.sk-toggleable__label {
  cursor: pointer;
  display: flex;
  width: 100%;
  margin-bottom: 0;
  padding: 0.5em;
  box-sizing: border-box;
  text-align: center;
  align-items: start;
  justify-content: space-between;
  gap: 0.5em;
}

#sk-container-id-1 label.sk-toggleable__label .caption {
  font-size: 0.6rem;
  font-weight: lighter;
  color: var(--sklearn-color-text-muted);
}

#sk-container-id-1 label.sk-toggleable__label-arrow:before {
  /* Arrow on the left of the label */
  content: "‚ñ∏";
  float: left;
  margin-right: 0.25em;
  color: var(--sklearn-color-icon);
}

#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {
  color: var(--sklearn-color-text);
}

/* Toggleable content - dropdown */

#sk-container-id-1 div.sk-toggleable__content {
  max-height: 0;
  max-width: 0;
  overflow: hidden;
  text-align: left;
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-0);
}

#sk-container-id-1 div.sk-toggleable__content.fitted {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-0);
}

#sk-container-id-1 div.sk-toggleable__content pre {
  margin: 0.2em;
  border-radius: 0.25em;
  color: var(--sklearn-color-text);
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-0);
}

#sk-container-id-1 div.sk-toggleable__content.fitted pre {
  /* unfitted */
  background-color: var(--sklearn-color-fitted-level-0);
}

#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {
  /* Expand drop-down */
  max-height: 200px;
  max-width: 100%;
  overflow: auto;
}

#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {
  content: "‚ñæ";
}

/* Pipeline/ColumnTransformer-specific style */

#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {
  color: var(--sklearn-color-text);
  background-color: var(--sklearn-color-unfitted-level-2);
}

#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {
  background-color: var(--sklearn-color-fitted-level-2);
}

/* Estimator-specific style */

/* Colorize estimator box */
#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-2);
}

#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-2);
}

#sk-container-id-1 div.sk-label label.sk-toggleable__label,
#sk-container-id-1 div.sk-label label {
  /* The background is the default theme color */
  color: var(--sklearn-color-text-on-default-background);
}

/* On hover, darken the color of the background */
#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {
  color: var(--sklearn-color-text);
  background-color: var(--sklearn-color-unfitted-level-2);
}

/* Label box, darken color on hover, fitted */
#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {
  color: var(--sklearn-color-text);
  background-color: var(--sklearn-color-fitted-level-2);
}

/* Estimator label */

#sk-container-id-1 div.sk-label label {
  font-family: monospace;
  font-weight: bold;
  display: inline-block;
  line-height: 1.2em;
}

#sk-container-id-1 div.sk-label-container {
  text-align: center;
}

/* Estimator-specific */
#sk-container-id-1 div.sk-estimator {
  font-family: monospace;
  border: 1px dotted var(--sklearn-color-border-box);
  border-radius: 0.25em;
  box-sizing: border-box;
  margin-bottom: 0.5em;
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-0);
}

#sk-container-id-1 div.sk-estimator.fitted {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-0);
}

/* on hover */
#sk-container-id-1 div.sk-estimator:hover {
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-2);
}

#sk-container-id-1 div.sk-estimator.fitted:hover {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-2);
}

/* Specification for estimator info (e.g. "i" and "?") */

/* Common style for "i" and "?" */

.sk-estimator-doc-link,
a:link.sk-estimator-doc-link,
a:visited.sk-estimator-doc-link {
  float: right;
  font-size: smaller;
  line-height: 1em;
  font-family: monospace;
  background-color: var(--sklearn-color-background);
  border-radius: 1em;
  height: 1em;
  width: 1em;
  text-decoration: none !important;
  margin-left: 0.5em;
  text-align: center;
  /* unfitted */
  border: var(--sklearn-color-unfitted-level-1) 1pt solid;
  color: var(--sklearn-color-unfitted-level-1);
}

.sk-estimator-doc-link.fitted,
a:link.sk-estimator-doc-link.fitted,
a:visited.sk-estimator-doc-link.fitted {
  /* fitted */
  border: var(--sklearn-color-fitted-level-1) 1pt solid;
  color: var(--sklearn-color-fitted-level-1);
}

/* On hover */
div.sk-estimator:hover .sk-estimator-doc-link:hover,
.sk-estimator-doc-link:hover,
div.sk-label-container:hover .sk-estimator-doc-link:hover,
.sk-estimator-doc-link:hover {
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-3);
  color: var(--sklearn-color-background);
  text-decoration: none;
}

div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,
.sk-estimator-doc-link.fitted:hover,
div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,
.sk-estimator-doc-link.fitted:hover {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-3);
  color: var(--sklearn-color-background);
  text-decoration: none;
}

/* Span, style for the box shown on hovering the info icon */
.sk-estimator-doc-link span {
  display: none;
  z-index: 9999;
  position: relative;
  font-weight: normal;
  right: .2ex;
  padding: .5ex;
  margin: .5ex;
  width: min-content;
  min-width: 20ex;
  max-width: 50ex;
  color: var(--sklearn-color-text);
  box-shadow: 2pt 2pt 4pt #999;
  /* unfitted */
  background: var(--sklearn-color-unfitted-level-0);
  border: .5pt solid var(--sklearn-color-unfitted-level-3);
}

.sk-estimator-doc-link.fitted span {
  /* fitted */
  background: var(--sklearn-color-fitted-level-0);
  border: var(--sklearn-color-fitted-level-3);
}

.sk-estimator-doc-link:hover span {
  display: block;
}

/* "?"-specific style due to the `<a>` HTML tag */

#sk-container-id-1 a.estimator_doc_link {
  float: right;
  font-size: 1rem;
  line-height: 1em;
  font-family: monospace;
  background-color: var(--sklearn-color-background);
  border-radius: 1rem;
  height: 1rem;
  width: 1rem;
  text-decoration: none;
  /* unfitted */
  color: var(--sklearn-color-unfitted-level-1);
  border: var(--sklearn-color-unfitted-level-1) 1pt solid;
}

#sk-container-id-1 a.estimator_doc_link.fitted {
  /* fitted */
  border: var(--sklearn-color-fitted-level-1) 1pt solid;
  color: var(--sklearn-color-fitted-level-1);
}

/* On hover */
#sk-container-id-1 a.estimator_doc_link:hover {
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-3);
  color: var(--sklearn-color-background);
  text-decoration: none;
}

#sk-container-id-1 a.estimator_doc_link.fitted:hover {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-3);
}
</style><div id="sk-container-id-1" class="sk-top-container"><div class="sk-text-repr-fallback"><pre>SGDClassifier(alpha=0.01)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br>On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class="sk-container" hidden=""><div class="sk-item"><div class="sk-estimator fitted sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-1" type="checkbox" checked=""><label for="sk-estimator-id-1" class="sk-toggleable__label fitted sk-toggleable__label-arrow"><div><div>SGDClassifier</div></div><div><a class="sk-estimator-doc-link fitted" rel="noreferrer" target="_blank" href="https://scikit-learn.org/1.6/modules/generated/sklearn.linear_model.SGDClassifier.html">?<span>Documentation for SGDClassifier</span></a><span class="sk-estimator-doc-link fitted">i<span>Fitted</span></span></div></label><div class="sk-toggleable__content fitted"><pre>SGDClassifier(alpha=0.01)</pre></div> </div></div></div></div>
</div>
</div>
</section>
<section id="using-tensorflowkeras" class="level2" data-number="13.2">
<h2 data-number="13.2" class="anchored" data-anchor-id="using-tensorflowkeras"><span class="header-section-number">13.2</span> Using TensorFlow/Keras</h2>
<div class="sourceCode" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> tensorflow <span class="im">as</span> tf</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tensorflow.keras.models <span class="im">import</span> Sequential</span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tensorflow.keras.layers <span class="im">import</span> Dense</span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tensorflow.keras.optimizers <span class="im">import</span> SGD</span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a simple model</span></span>
<span id="cb17-7"><a href="#cb17-7" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> Sequential([</span>
<span id="cb17-8"><a href="#cb17-8" aria-hidden="true" tabindex="-1"></a>    Dense(<span class="dv">10</span>, activation<span class="op">=</span><span class="st">'relu'</span>, input_shape<span class="op">=</span>(X_train.shape[<span class="dv">1</span>],)),</span>
<span id="cb17-9"><a href="#cb17-9" aria-hidden="true" tabindex="-1"></a>    Dense(<span class="dv">1</span>)</span>
<span id="cb17-10"><a href="#cb17-10" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb17-11"><a href="#cb17-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-12"><a href="#cb17-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Configure the optimizer</span></span>
<span id="cb17-13"><a href="#cb17-13" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> SGD(learning_rate<span class="op">=</span><span class="fl">0.01</span>)</span>
<span id="cb17-14"><a href="#cb17-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-15"><a href="#cb17-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Compile the model</span></span>
<span id="cb17-16"><a href="#cb17-16" aria-hidden="true" tabindex="-1"></a>model.<span class="bu">compile</span>(optimizer<span class="op">=</span>optimizer, loss<span class="op">=</span><span class="st">'mse'</span>)</span>
<span id="cb17-17"><a href="#cb17-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-18"><a href="#cb17-18" aria-hidden="true" tabindex="-1"></a><span class="co"># Train the model</span></span>
<span id="cb17-19"><a href="#cb17-19" aria-hidden="true" tabindex="-1"></a>history <span class="op">=</span> model.fit(</span>
<span id="cb17-20"><a href="#cb17-20" aria-hidden="true" tabindex="-1"></a>    X_train, y_train, </span>
<span id="cb17-21"><a href="#cb17-21" aria-hidden="true" tabindex="-1"></a>    epochs<span class="op">=</span><span class="dv">100</span>, </span>
<span id="cb17-22"><a href="#cb17-22" aria-hidden="true" tabindex="-1"></a>    batch_size<span class="op">=</span><span class="dv">32</span>, </span>
<span id="cb17-23"><a href="#cb17-23" aria-hidden="true" tabindex="-1"></a>    validation_split<span class="op">=</span><span class="fl">0.2</span>,</span>
<span id="cb17-24"><a href="#cb17-24" aria-hidden="true" tabindex="-1"></a>    verbose<span class="op">=</span><span class="dv">0</span></span>
<span id="cb17-25"><a href="#cb17-25" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb17-26"><a href="#cb17-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-27"><a href="#cb17-27" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot the learning curves</span></span>
<span id="cb17-28"><a href="#cb17-28" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">6</span>))</span>
<span id="cb17-29"><a href="#cb17-29" aria-hidden="true" tabindex="-1"></a>plt.plot(history.history[<span class="st">'loss'</span>], label<span class="op">=</span><span class="st">'Training Loss'</span>)</span>
<span id="cb17-30"><a href="#cb17-30" aria-hidden="true" tabindex="-1"></a>plt.plot(history.history[<span class="st">'val_loss'</span>], label<span class="op">=</span><span class="st">'Validation Loss'</span>)</span>
<span id="cb17-31"><a href="#cb17-31" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Epochs'</span>)</span>
<span id="cb17-32"><a href="#cb17-32" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Loss'</span>)</span>
<span id="cb17-33"><a href="#cb17-33" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Training Progress with SGD in TensorFlow/Keras'</span>)</span>
<span id="cb17-34"><a href="#cb17-34" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb17-35"><a href="#cb17-35" aria-hidden="true" tabindex="-1"></a>plt.grid(<span class="va">True</span>)</span>
<span id="cb17-36"><a href="#cb17-36" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
</section>
<section id="conclusion" class="level1" data-number="14">
<h1 data-number="14"><span class="header-section-number">14</span> Conclusion</h1>
<p>Gradient Descent is a cornerstone optimization algorithm in machine learning with several variants optimized for different scenarios. The choice between Batch GD, SGD, and Mini-batch GD depends on your specific needs regarding speed, memory efficiency, and stability.</p>
<p>Advanced techniques like regularization, early stopping, and batch normalization can significantly enhance the performance of gradient-based optimization, leading to better models with improved generalization capabilities.</p>
<p>When implementing Gradient Descent, remember these key considerations: - Choose an appropriate learning rate - Scale your features - Select the right variant for your dataset size - Consider applying regularization to prevent overfitting - Monitor validation performance for early stopping</p>
<p>With a solid understanding of these concepts and techniques, you can effectively leverage Gradient Descent to train high-performing machine learning models across a wide range of applications.</p>
<section id="references" class="level2" data-number="14.1">
<h2 data-number="14.1" class="anchored" data-anchor-id="references"><span class="header-section-number">14.1</span> References</h2>
<ol type="1">
<li>Goodfellow, I., Bengio, Y., &amp; Courville, A. (2016). Deep Learning. MIT Press.</li>
<li>Ruder, S. (2016). An overview of gradient descent optimization algorithms. arXiv preprint arXiv:1609.04747.</li>
<li>Ioffe, S., &amp; Szegedy, C. (2015). Batch normalization: Accelerating deep network training by reducing internal covariate shift. ICML.</li>
</ol>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "Óßã";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./chapter7.html" class="pagination-link" aria-label="Outlier Detection and Recommendation Systems">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Outlier Detection and Recommendation Systems</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./chapter9.html" class="pagination-link" aria-label="Neural Networks and Deep Learning Foundations">
        <span class="nav-page-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Neural Networks and Deep Learning Foundations</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->




</body></html>