<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.42">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>12&nbsp; Batch Normalization, RNN, Distributed Deep Learning and Tensorflow – Machine Learning and Deep Learning Notes</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./summary.html" rel="next">
<link href="./chapter9.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-2f5df379a58b258e96c21c0638c20c03.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-8da5b4427184b79ecddefad3d342027e.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./chapter10.html"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Batch Normalization, RNN, Distributed Deep Learning and Tensorflow</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Machine Learning and Deep Learning Notes</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">🚀 Introduction</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Fundamentals of Machine Learning</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Building an End-to-End Machine Learning Pipeline</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter3.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Unsupervised Learning: Clustering Techniques</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter4.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Supervised Learning: Regression and Classification</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter5.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Dimensionality Reduction Methods</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter6.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Tree-Based Models and Ensemble Learning</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter6v2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Support Vector Machines and Model Evaluation</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter7.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Outlier Detection and Recommendation Systems</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter8.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Gradient Descent: Optimization in Machine Learning</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter9.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Neural Networks and Deep Learning Foundations</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter10.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Batch Normalization, RNN, Distributed Deep Learning and Tensorflow</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./summary.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Summary</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#batch-normalization-a-comprehensive-guide" id="toc-batch-normalization-a-comprehensive-guide" class="nav-link active" data-scroll-target="#batch-normalization-a-comprehensive-guide"><span class="header-section-number">12.1</span> Batch Normalization: A Comprehensive Guide</a>
  <ul class="collapse">
  <li><a href="#introduction" id="toc-introduction" class="nav-link" data-scroll-target="#introduction"><span class="header-section-number">12.1.1</span> Introduction</a></li>
  <li><a href="#the-problem-internal-covariate-shift" id="toc-the-problem-internal-covariate-shift" class="nav-link" data-scroll-target="#the-problem-internal-covariate-shift"><span class="header-section-number">12.1.2</span> The Problem: Internal Covariate Shift</a></li>
  <li><a href="#how-batch-normalization-works" id="toc-how-batch-normalization-works" class="nav-link" data-scroll-target="#how-batch-normalization-works"><span class="header-section-number">12.1.3</span> How Batch Normalization Works</a></li>
  <li><a href="#implementing-batch-normalization-in-python" id="toc-implementing-batch-normalization-in-python" class="nav-link" data-scroll-target="#implementing-batch-normalization-in-python"><span class="header-section-number">12.1.4</span> Implementing Batch Normalization in Python</a></li>
  <li><a href="#key-benefits-of-batch-normalization" id="toc-key-benefits-of-batch-normalization" class="nav-link" data-scroll-target="#key-benefits-of-batch-normalization"><span class="header-section-number">12.1.5</span> Key Benefits of Batch Normalization</a></li>
  <li><a href="#practical-considerations-and-limitations" id="toc-practical-considerations-and-limitations" class="nav-link" data-scroll-target="#practical-considerations-and-limitations"><span class="header-section-number">12.1.6</span> Practical Considerations and Limitations</a></li>
  <li><a href="#practical-examples" id="toc-practical-examples" class="nav-link" data-scroll-target="#practical-examples"><span class="header-section-number">12.1.7</span> Practical Examples</a></li>
  <li><a href="#example-2-visual-comparison-of-training-stability" id="toc-example-2-visual-comparison-of-training-stability" class="nav-link" data-scroll-target="#example-2-visual-comparison-of-training-stability"><span class="header-section-number">12.1.8</span> Example 2: Visual Comparison of Training Stability</a></li>
  <li><a href="#understanding-batchnorm-intuitively" id="toc-understanding-batchnorm-intuitively" class="nav-link" data-scroll-target="#understanding-batchnorm-intuitively"><span class="header-section-number">12.1.9</span> Understanding BatchNorm Intuitively</a></li>
  <li><a href="#conclusion" id="toc-conclusion" class="nav-link" data-scroll-target="#conclusion"><span class="header-section-number">12.1.10</span> Conclusion</a></li>
  </ul></li>
  <li><a href="#recurrent-neural-networks-a-comprehensive-guide" id="toc-recurrent-neural-networks-a-comprehensive-guide" class="nav-link" data-scroll-target="#recurrent-neural-networks-a-comprehensive-guide"><span class="header-section-number">12.2</span> Recurrent Neural Networks: A Comprehensive Guide</a>
  <ul class="collapse">
  <li><a href="#introduction-1" id="toc-introduction-1" class="nav-link" data-scroll-target="#introduction-1"><span class="header-section-number">12.2.1</span> Introduction</a></li>
  <li><a href="#the-fundamental-concept-of-rnns" id="toc-the-fundamental-concept-of-rnns" class="nav-link" data-scroll-target="#the-fundamental-concept-of-rnns"><span class="header-section-number">12.2.2</span> The Fundamental Concept of RNNs</a></li>
  <li><a href="#rnn-architecture" id="toc-rnn-architecture" class="nav-link" data-scroll-target="#rnn-architecture"><span class="header-section-number">12.2.3</span> RNN Architecture</a></li>
  <li><a href="#recurrent-connection-schemes" id="toc-recurrent-connection-schemes" class="nav-link" data-scroll-target="#recurrent-connection-schemes"><span class="header-section-number">12.2.4</span> Recurrent Connection Schemes</a></li>
  <li><a href="#rnn-applications-for-sequence-problems" id="toc-rnn-applications-for-sequence-problems" class="nav-link" data-scroll-target="#rnn-applications-for-sequence-problems"><span class="header-section-number">12.2.5</span> RNN Applications for Sequence Problems</a></li>
  <li><a href="#training-recurrent-neural-networks" id="toc-training-recurrent-neural-networks" class="nav-link" data-scroll-target="#training-recurrent-neural-networks"><span class="header-section-number">12.2.6</span> Training Recurrent Neural Networks</a></li>
  <li><a href="#implementing-rnns-in-python" id="toc-implementing-rnns-in-python" class="nav-link" data-scroll-target="#implementing-rnns-in-python"><span class="header-section-number">12.2.7</span> Implementing RNNs in Python</a></li>
  <li><a href="#understanding-rnns-visually" id="toc-understanding-rnns-visually" class="nav-link" data-scroll-target="#understanding-rnns-visually"><span class="header-section-number">12.2.8</span> Understanding RNNs Visually</a></li>
  <li><a href="#practical-considerations" id="toc-practical-considerations" class="nav-link" data-scroll-target="#practical-considerations"><span class="header-section-number">12.2.9</span> Practical Considerations</a></li>
  <li><a href="#conclusion-1" id="toc-conclusion-1" class="nav-link" data-scroll-target="#conclusion-1"><span class="header-section-number">12.2.10</span> Conclusion</a></li>
  </ul></li>
  <li><a href="#deep-learning-frameworks" id="toc-deep-learning-frameworks" class="nav-link" data-scroll-target="#deep-learning-frameworks"><span class="header-section-number">12.3</span> Deep Learning Frameworks</a>
  <ul class="collapse">
  <li><a href="#popular-deep-learning-frameworks" id="toc-popular-deep-learning-frameworks" class="nav-link" data-scroll-target="#popular-deep-learning-frameworks"><span class="header-section-number">12.3.1</span> Popular Deep Learning Frameworks</a></li>
  <li><a href="#evolution-of-neural-network-architectures" id="toc-evolution-of-neural-network-architectures" class="nav-link" data-scroll-target="#evolution-of-neural-network-architectures"><span class="header-section-number">12.3.2</span> Evolution of Neural Network Architectures</a></li>
  </ul></li>
  <li><a href="#model-training-approaches" id="toc-model-training-approaches" class="nav-link" data-scroll-target="#model-training-approaches"><span class="header-section-number">12.4</span> Model Training Approaches</a>
  <ul class="collapse">
  <li><a href="#model-training-steps" id="toc-model-training-steps" class="nav-link" data-scroll-target="#model-training-steps"><span class="header-section-number">12.4.1</span> Model Training Steps</a></li>
  <li><a href="#training-paradigms" id="toc-training-paradigms" class="nav-link" data-scroll-target="#training-paradigms"><span class="header-section-number">12.4.2</span> Training Paradigms</a></li>
  <li><a href="#onnx-open-neural-network-exchange" id="toc-onnx-open-neural-network-exchange" class="nav-link" data-scroll-target="#onnx-open-neural-network-exchange"><span class="header-section-number">12.4.3</span> ONNX: Open Neural Network eXchange</a></li>
  </ul></li>
  <li><a href="#distributed-machine-learning" id="toc-distributed-machine-learning" class="nav-link" data-scroll-target="#distributed-machine-learning"><span class="header-section-number">12.5</span> Distributed Machine Learning</a>
  <ul class="collapse">
  <li><a href="#non-distributed-vs.-distributed-approach" id="toc-non-distributed-vs.-distributed-approach" class="nav-link" data-scroll-target="#non-distributed-vs.-distributed-approach"><span class="header-section-number">12.5.1</span> Non-Distributed vs.&nbsp;Distributed Approach</a></li>
  <li><a href="#parallelization-methods-in-distributed-deep-learning" id="toc-parallelization-methods-in-distributed-deep-learning" class="nav-link" data-scroll-target="#parallelization-methods-in-distributed-deep-learning"><span class="header-section-number">12.5.2</span> Parallelization Methods in Distributed Deep Learning</a></li>
  <li><a href="#synchronization-strategies" id="toc-synchronization-strategies" class="nav-link" data-scroll-target="#synchronization-strategies"><span class="header-section-number">12.5.3</span> Synchronization Strategies</a></li>
  <li><a href="#conclusion-2" id="toc-conclusion-2" class="nav-link" data-scroll-target="#conclusion-2"><span class="header-section-number">12.5.4</span> Conclusion</a></li>
  </ul></li>
  <li><a href="#introduction-to-tensorflow" id="toc-introduction-to-tensorflow" class="nav-link" data-scroll-target="#introduction-to-tensorflow"><span class="header-section-number">12.6</span> Introduction to TensorFlow</a>
  <ul class="collapse">
  <li><a href="#what-is-tensorflow" id="toc-what-is-tensorflow" class="nav-link" data-scroll-target="#what-is-tensorflow"><span class="header-section-number">12.6.1</span> What is TensorFlow?</a></li>
  <li><a href="#core-concept-tensors" id="toc-core-concept-tensors" class="nav-link" data-scroll-target="#core-concept-tensors"><span class="header-section-number">12.6.2</span> Core Concept: Tensors</a></li>
  <li><a href="#tensorflows-architecture" id="toc-tensorflows-architecture" class="nav-link" data-scroll-target="#tensorflows-architecture"><span class="header-section-number">12.6.3</span> TensorFlow’s Architecture</a></li>
  <li><a href="#tensorflows-python-api" id="toc-tensorflows-python-api" class="nav-link" data-scroll-target="#tensorflows-python-api"><span class="header-section-number">12.6.4</span> TensorFlow’s Python API</a></li>
  <li><a href="#tensorflow-vs.-numpy" id="toc-tensorflow-vs.-numpy" class="nav-link" data-scroll-target="#tensorflow-vs.-numpy"><span class="header-section-number">12.6.5</span> TensorFlow vs.&nbsp;NumPy</a></li>
  <li><a href="#computational-graphs-in-tensorflow" id="toc-computational-graphs-in-tensorflow" class="nav-link" data-scroll-target="#computational-graphs-in-tensorflow"><span class="header-section-number">12.6.6</span> Computational Graphs in TensorFlow</a></li>
  <li><a href="#how-tensorflow-works" id="toc-how-tensorflow-works" class="nav-link" data-scroll-target="#how-tensorflow-works"><span class="header-section-number">12.6.7</span> How TensorFlow Works</a></li>
  <li><a href="#tensorflows-execution-framework" id="toc-tensorflows-execution-framework" class="nav-link" data-scroll-target="#tensorflows-execution-framework"><span class="header-section-number">12.6.8</span> TensorFlow’s Execution Framework</a></li>
  <li><a href="#installing-and-setting-up-tensorflow" id="toc-installing-and-setting-up-tensorflow" class="nav-link" data-scroll-target="#installing-and-setting-up-tensorflow"><span class="header-section-number">12.6.9</span> Installing and Setting Up TensorFlow</a></li>
  <li><a href="#tensorflow-ecosystem" id="toc-tensorflow-ecosystem" class="nav-link" data-scroll-target="#tensorflow-ecosystem"><span class="header-section-number">12.6.10</span> TensorFlow Ecosystem</a></li>
  <li><a href="#tensorflow-vs.-other-frameworks" id="toc-tensorflow-vs.-other-frameworks" class="nav-link" data-scroll-target="#tensorflow-vs.-other-frameworks"><span class="header-section-number">12.6.11</span> TensorFlow vs.&nbsp;Other Frameworks</a></li>
  <li><a href="#conclusion-3" id="toc-conclusion-3" class="nav-link" data-scroll-target="#conclusion-3"><span class="header-section-number">12.6.12</span> Conclusion</a></li>
  <li><a href="#additional-resources" id="toc-additional-resources" class="nav-link" data-scroll-target="#additional-resources"><span class="header-section-number">12.6.13</span> Additional Resources</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Batch Normalization, RNN, Distributed Deep Learning and Tensorflow</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<section id="batch-normalization-a-comprehensive-guide" class="level2" data-number="12.1">
<h2 data-number="12.1" class="anchored" data-anchor-id="batch-normalization-a-comprehensive-guide"><span class="header-section-number">12.1</span> Batch Normalization: A Comprehensive Guide</h2>
<section id="introduction" class="level3" data-number="12.1.1">
<h3 data-number="12.1.1" class="anchored" data-anchor-id="introduction"><span class="header-section-number">12.1.1</span> Introduction</h3>
<p>Batch Normalization (BatchNorm or BN) is considered one of the most successful architectural innovations in deep learning history. Introduced by Sergey Ioffe and Christian Szegedy in their 2015 paper “Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift,” this technique has become a standard component in modern neural network architectures.</p>
<p>This document provides a comprehensive explanation of Batch Normalization, its mechanisms, benefits, and practical implementation in Python.</p>
</section>
<section id="the-problem-internal-covariate-shift" class="level3" data-number="12.1.2">
<h3 data-number="12.1.2" class="anchored" data-anchor-id="the-problem-internal-covariate-shift"><span class="header-section-number">12.1.2</span> The Problem: Internal Covariate Shift</h3>
<p>To understand why BatchNorm is valuable, we first need to understand the problem it addresses: <strong>internal covariate shift</strong>.</p>
<p>In deep neural networks, as data flows through many layers, the distribution of inputs to each layer continuously changes during training. This happens because the parameters of previous layers are constantly being updated. This phenomenon, called internal covariate shift, creates several challenges:</p>
<ol type="1">
<li>It makes training unstable</li>
<li>It requires lower learning rates</li>
<li>It makes deeper networks harder to train</li>
<li>It contributes to the vanishing/exploding gradient problem</li>
</ol>
<section id="an-analogy" class="level4" data-number="12.1.2.1">
<h4 data-number="12.1.2.1" class="anchored" data-anchor-id="an-analogy"><span class="header-section-number">12.1.2.1</span> An Analogy</h4>
<p>Imagine you’re learning to catch a ball, but every time you try, someone changes the weight of the ball without telling you. Sometimes it’s as light as a tennis ball, sometimes as heavy as a bowling ball. This would make learning extremely difficult!</p>
<p>This is similar to what happens in neural networks: each layer keeps receiving inputs with different statistical distributions, making it hard for the layer to “learn” properly.</p>
</section>
</section>
<section id="how-batch-normalization-works" class="level3" data-number="12.1.3">
<h3 data-number="12.1.3" class="anchored" data-anchor-id="how-batch-normalization-works"><span class="header-section-number">12.1.3</span> How Batch Normalization Works</h3>
<p>Batch Normalization applies a transformation that maintains the mean output close to 0 and the output standard deviation close to 1. Here’s the step-by-step process:</p>
<ol type="1">
<li>For each feature in a mini-batch, calculate the mean and variance</li>
<li>Normalize the features by subtracting the mean and dividing by the standard deviation</li>
<li>Scale and shift the normalized values using two learnable parameters (gamma and beta)</li>
</ol>
<section id="the-mathematical-formulation" class="level4" data-number="12.1.3.1">
<h4 data-number="12.1.3.1" class="anchored" data-anchor-id="the-mathematical-formulation"><span class="header-section-number">12.1.3.1</span> The Mathematical Formulation</h4>
<p>For a mini-batch <span class="math inline">\(B = \{x_1, x_2, ..., x_m\}\)</span>, the BatchNorm transformation works as follows:</p>
<ol type="1">
<li>Calculate mini-batch mean: <span class="math inline">\(\mu_B = \frac{1}{m}\sum_{i=1}^{m}x_i\)</span></li>
<li>Calculate mini-batch variance: <span class="math inline">\(\sigma_B^2 = \frac{1}{m}\sum_{i=1}^{m}(x_i - \mu_B)^2\)</span></li>
<li>Normalize: <span class="math inline">\(\hat{x_i} = \frac{x_i - \mu_B}{\sqrt{\sigma_B^2 + \epsilon}}\)</span> (where <span class="math inline">\(\epsilon\)</span> is a small constant for numerical stability)</li>
<li>Scale and shift: <span class="math inline">\(y_i = \gamma\hat{x_i} + \beta\)</span> (where <span class="math inline">\(\gamma\)</span> and <span class="math inline">\(\beta\)</span> are learnable parameters)</li>
</ol>
<p>The parameters <span class="math inline">\(\gamma\)</span> and <span class="math inline">\(\beta\)</span> allow the network to learn the optimal scale and shift for each feature, which means it can even learn to “undo” the normalization if that’s optimal.</p>
</section>
<section id="an-analogy-for-how-it-works" class="level4" data-number="12.1.3.2">
<h4 data-number="12.1.3.2" class="anchored" data-anchor-id="an-analogy-for-how-it-works"><span class="header-section-number">12.1.3.2</span> An Analogy for How it Works</h4>
<p>Think of BatchNorm like a thermostat for each layer of your neural network. Just as a thermostat keeps room temperature stable despite changing outdoor conditions, BatchNorm keeps the statistical distribution of inputs to each layer stable despite changes in earlier layers.</p>
</section>
</section>
<section id="implementing-batch-normalization-in-python" class="level3" data-number="12.1.4">
<h3 data-number="12.1.4" class="anchored" data-anchor-id="implementing-batch-normalization-in-python"><span class="header-section-number">12.1.4</span> Implementing Batch Normalization in Python</h3>
<p>Let’s see how to implement BatchNorm in Python, both manually and using deep learning frameworks.</p>
<section id="manual-implementation" class="level4" data-number="12.1.4.1">
<h4 data-number="12.1.4.1" class="anchored" data-anchor-id="manual-implementation"><span class="header-section-number">12.1.4.1</span> Manual Implementation</h4>
<div id="4c1816bf" class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> batch_norm(x, gamma, beta, eps<span class="op">=</span><span class="fl">1e-5</span>):</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>    <span class="co"># x: input features for a mini-batch</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>    <span class="co"># gamma, beta: scale and shift parameters</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>    <span class="co"># eps: small constant for numerical stability</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Calculate batch mean and variance</span></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>    batch_mean <span class="op">=</span> np.mean(x, axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>    batch_var <span class="op">=</span> np.var(x, axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Normalize</span></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>    x_norm <span class="op">=</span> (x <span class="op">-</span> batch_mean) <span class="op">/</span> np.sqrt(batch_var <span class="op">+</span> eps)</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Scale and shift</span></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>    out <span class="op">=</span> gamma <span class="op">*</span> x_norm <span class="op">+</span> beta</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Store cache for backward pass</span></span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>    cache <span class="op">=</span> (x, x_norm, batch_mean, batch_var, gamma, beta, eps)</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> out, cache</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="using-pytorch" class="level4" data-number="12.1.4.2">
<h4 data-number="12.1.4.2" class="anchored" data-anchor-id="using-pytorch"><span class="header-section-number">12.1.4.2</span> Using PyTorch</h4>
<div id="713314a7" class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Creating a model with BatchNorm</span></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> NetWithBatchNorm(nn.Module):</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>):</span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>(NetWithBatchNorm, <span class="va">self</span>).<span class="fu">__init__</span>()</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fc1 <span class="op">=</span> nn.Linear(<span class="dv">784</span>, <span class="dv">100</span>)</span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.bn1 <span class="op">=</span> nn.BatchNorm1d(<span class="dv">100</span>)</span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fc2 <span class="op">=</span> nn.Linear(<span class="dv">100</span>, <span class="dv">10</span>)</span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.fc1(x)</span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.bn1(x)</span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> torch.relu(x)</span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.fc2(x)</span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="using-tensorflowkeras" class="level4" data-number="12.1.4.3">
<h4 data-number="12.1.4.3" class="anchored" data-anchor-id="using-tensorflowkeras"><span class="header-section-number">12.1.4.3</span> Using TensorFlow/Keras</h4>
<div id="d266342f" class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> tensorflow <span class="im">as</span> tf</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tensorflow.keras <span class="im">import</span> layers, models</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Creating a model with BatchNorm</span></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> models.Sequential([</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>    layers.Dense(<span class="dv">100</span>, input_shape<span class="op">=</span>(<span class="dv">784</span>,)),</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>    layers.BatchNormalization(),</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>    layers.Activation(<span class="st">'relu'</span>),</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>    layers.Dense(<span class="dv">10</span>)</span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
</section>
<section id="key-benefits-of-batch-normalization" class="level3" data-number="12.1.5">
<h3 data-number="12.1.5" class="anchored" data-anchor-id="key-benefits-of-batch-normalization"><span class="header-section-number">12.1.5</span> Key Benefits of Batch Normalization</h3>
<section id="faster-training" class="level4" data-number="12.1.5.1">
<h4 data-number="12.1.5.1" class="anchored" data-anchor-id="faster-training"><span class="header-section-number">12.1.5.1</span> 1. Faster Training</h4>
<p>BatchNorm allows us to use higher learning rates without the risk of divergence, leading to faster convergence. In some cases, training time can be reduced by a factor of 14x!</p>
<div id="c083b009" class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Example showing faster convergence with BatchNorm</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Simulated training curves</span></span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>epochs <span class="op">=</span> np.arange(<span class="dv">1</span>, <span class="dv">21</span>)</span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>loss_without_bn <span class="op">=</span> <span class="dv">1</span> <span class="op">/</span> (<span class="dv">1</span> <span class="op">+</span> np.exp(epochs <span class="op">-</span> <span class="dv">10</span>)) <span class="op">+</span> <span class="fl">0.1</span></span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>loss_with_bn <span class="op">=</span> <span class="dv">1</span> <span class="op">/</span> (<span class="dv">1</span> <span class="op">+</span> np.exp(<span class="dv">3</span> <span class="op">*</span> (epochs <span class="op">-</span> <span class="dv">5</span>))) <span class="op">+</span> <span class="fl">0.05</span></span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">5</span>))</span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a>plt.plot(epochs, loss_without_bn, <span class="st">'b-'</span>, label<span class="op">=</span><span class="st">'Without BatchNorm'</span>)</span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a>plt.plot(epochs, loss_with_bn, <span class="st">'r-'</span>, label<span class="op">=</span><span class="st">'With BatchNorm'</span>)</span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Epochs'</span>)</span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Loss'</span>)</span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Training Convergence With and Without BatchNorm'</span>)</span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a>plt.grid(<span class="va">True</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="chapter10_files/figure-html/cell-5-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="reduces-the-vanishing-gradient-problem" class="level4" data-number="12.1.5.2">
<h4 data-number="12.1.5.2" class="anchored" data-anchor-id="reduces-the-vanishing-gradient-problem"><span class="header-section-number">12.1.5.2</span> 2. Reduces the Vanishing Gradient Problem</h4>
<p>By ensuring that the distribution of activations doesn’t shift too much, BatchNorm helps prevent the vanishing gradient problem, especially in deep networks.</p>
</section>
<section id="acts-as-a-regularizer" class="level4" data-number="12.1.5.3">
<h4 data-number="12.1.5.3" class="anchored" data-anchor-id="acts-as-a-regularizer"><span class="header-section-number">12.1.5.3</span> 3. Acts as a Regularizer</h4>
<p>BatchNorm has a regularizing effect, reducing the need for other regularization techniques like Dropout.</p>
<div id="74f45b1b" class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Example comparing dropout vs BatchNorm</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> ModelWithDropout(nn.Module):</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>):</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>(ModelWithDropout, <span class="va">self</span>).<span class="fu">__init__</span>()</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fc1 <span class="op">=</span> nn.Linear(<span class="dv">784</span>, <span class="dv">100</span>)</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.dropout <span class="op">=</span> nn.Dropout(<span class="fl">0.3</span>)</span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fc2 <span class="op">=</span> nn.Linear(<span class="dv">100</span>, <span class="dv">10</span>)</span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.fc1(x)</span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> torch.relu(x)</span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.dropout(x)</span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.fc2(x)</span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x</span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-16"><a href="#cb5-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Sometimes using both can be beneficial</span></span>
<span id="cb5-17"><a href="#cb5-17" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> ModelWithBothRegularizers(nn.Module):</span>
<span id="cb5-18"><a href="#cb5-18" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>):</span>
<span id="cb5-19"><a href="#cb5-19" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>(ModelWithBothRegularizers, <span class="va">self</span>).<span class="fu">__init__</span>()</span>
<span id="cb5-20"><a href="#cb5-20" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fc1 <span class="op">=</span> nn.Linear(<span class="dv">784</span>, <span class="dv">100</span>)</span>
<span id="cb5-21"><a href="#cb5-21" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.bn1 <span class="op">=</span> nn.BatchNorm1d(<span class="dv">100</span>)</span>
<span id="cb5-22"><a href="#cb5-22" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.dropout <span class="op">=</span> nn.Dropout(<span class="fl">0.1</span>)  <span class="co"># Lower dropout rate when used with BN</span></span>
<span id="cb5-23"><a href="#cb5-23" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fc2 <span class="op">=</span> nn.Linear(<span class="dv">100</span>, <span class="dv">10</span>)</span>
<span id="cb5-24"><a href="#cb5-24" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb5-25"><a href="#cb5-25" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb5-26"><a href="#cb5-26" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.fc1(x)</span>
<span id="cb5-27"><a href="#cb5-27" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.bn1(x)</span>
<span id="cb5-28"><a href="#cb5-28" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> torch.relu(x)</span>
<span id="cb5-29"><a href="#cb5-29" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.dropout(x)</span>
<span id="cb5-30"><a href="#cb5-30" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.fc2(x)</span>
<span id="cb5-31"><a href="#cb5-31" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="reduces-sensitivity-to-initialization" class="level4" data-number="12.1.5.4">
<h4 data-number="12.1.5.4" class="anchored" data-anchor-id="reduces-sensitivity-to-initialization"><span class="header-section-number">12.1.5.4</span> 4. Reduces Sensitivity to Initialization</h4>
<p>With BatchNorm, the network becomes less sensitive to the weight initialization scheme, making training more robust.</p>
</section>
<section id="eliminates-need-for-input-standardization" class="level4" data-number="12.1.5.5">
<h4 data-number="12.1.5.5" class="anchored" data-anchor-id="eliminates-need-for-input-standardization"><span class="header-section-number">12.1.5.5</span> 5. Eliminates Need for Input Standardization</h4>
<p>If you add a BatchNorm layer as the first layer of your neural network, you don’t need to standardize your training set; the BatchNorm layer will do it for you.</p>
</section>
</section>
<section id="practical-considerations-and-limitations" class="level3" data-number="12.1.6">
<h3 data-number="12.1.6" class="anchored" data-anchor-id="practical-considerations-and-limitations"><span class="header-section-number">12.1.6</span> Practical Considerations and Limitations</h3>
<section id="batch-size-matters" class="level4" data-number="12.1.6.1">
<h4 data-number="12.1.6.1" class="anchored" data-anchor-id="batch-size-matters"><span class="header-section-number">12.1.6.1</span> 1. Batch Size Matters</h4>
<p>BatchNorm calculates statistics over mini-batches, so it performs less effectively with very small batch sizes (e.g., batch size of 1 or 2).</p>
<div id="07868467" class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.optim <span class="im">as</span> optim</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.utils.data <span class="im">import</span> DataLoader, TensorDataset</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Define SimpleNet with BatchNorm</span></span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> SimpleNet(nn.Module):</span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>):</span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>(SimpleNet, <span class="va">self</span>).<span class="fu">__init__</span>()</span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fc1 <span class="op">=</span> nn.Linear(<span class="dv">10</span>, <span class="dv">10</span>)  <span class="co"># Changed from 5 to 10 input features</span></span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.bn <span class="op">=</span> nn.BatchNorm1d(<span class="dv">10</span>)  <span class="co"># BatchNorm on 10D output</span></span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fc2 <span class="op">=</span> nn.Linear(<span class="dv">10</span>, <span class="dv">2</span>)  <span class="co"># 10 -&gt; 2 output classes</span></span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.relu <span class="op">=</span> nn.ReLU()</span>
<span id="cb6-14"><a href="#cb6-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-15"><a href="#cb6-15" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb6-16"><a href="#cb6-16" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.fc1(x)</span>
<span id="cb6-17"><a href="#cb6-17" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.bn(x)  <span class="co"># Apply BatchNorm</span></span>
<span id="cb6-18"><a href="#cb6-18" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.relu(x)</span>
<span id="cb6-19"><a href="#cb6-19" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.fc2(x)</span>
<span id="cb6-20"><a href="#cb6-20" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x</span>
<span id="cb6-21"><a href="#cb6-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-22"><a href="#cb6-22" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> train_with_different_batch_sizes(train_data, batch_sizes<span class="op">=</span>[<span class="dv">16</span>, <span class="dv">32</span>, <span class="dv">64</span>, <span class="dv">128</span>], epochs<span class="op">=</span><span class="dv">5</span>):</span>
<span id="cb6-23"><a href="#cb6-23" aria-hidden="true" tabindex="-1"></a>    results <span class="op">=</span> []</span>
<span id="cb6-24"><a href="#cb6-24" aria-hidden="true" tabindex="-1"></a>    device <span class="op">=</span> torch.device(<span class="st">"cuda"</span> <span class="cf">if</span> torch.cuda.is_available() <span class="cf">else</span> <span class="st">"cpu"</span>)  </span>
<span id="cb6-25"><a href="#cb6-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-26"><a href="#cb6-26" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> batch_size <span class="kw">in</span> batch_sizes:</span>
<span id="cb6-27"><a href="#cb6-27" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Create data loader with current batch size</span></span>
<span id="cb6-28"><a href="#cb6-28" aria-hidden="true" tabindex="-1"></a>        train_loader <span class="op">=</span> DataLoader(train_data, batch_size<span class="op">=</span>batch_size, shuffle<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb6-29"><a href="#cb6-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-30"><a href="#cb6-30" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Initialize model, loss, and optimizer</span></span>
<span id="cb6-31"><a href="#cb6-31" aria-hidden="true" tabindex="-1"></a>        model <span class="op">=</span> SimpleNet().to(device)</span>
<span id="cb6-32"><a href="#cb6-32" aria-hidden="true" tabindex="-1"></a>        criterion <span class="op">=</span> nn.CrossEntropyLoss()</span>
<span id="cb6-33"><a href="#cb6-33" aria-hidden="true" tabindex="-1"></a>        optimizer <span class="op">=</span> optim.SGD(model.parameters(), lr<span class="op">=</span><span class="fl">0.01</span>)</span>
<span id="cb6-34"><a href="#cb6-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-35"><a href="#cb6-35" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Training loop</span></span>
<span id="cb6-36"><a href="#cb6-36" aria-hidden="true" tabindex="-1"></a>        model.train()</span>
<span id="cb6-37"><a href="#cb6-37" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(epochs):</span>
<span id="cb6-38"><a href="#cb6-38" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> data, target <span class="kw">in</span> train_loader:</span>
<span id="cb6-39"><a href="#cb6-39" aria-hidden="true" tabindex="-1"></a>                data, target <span class="op">=</span> data.to(device), target.to(device)</span>
<span id="cb6-40"><a href="#cb6-40" aria-hidden="true" tabindex="-1"></a>                optimizer.zero_grad()</span>
<span id="cb6-41"><a href="#cb6-41" aria-hidden="true" tabindex="-1"></a>                output <span class="op">=</span> model(data)</span>
<span id="cb6-42"><a href="#cb6-42" aria-hidden="true" tabindex="-1"></a>                loss <span class="op">=</span> criterion(output, target)</span>
<span id="cb6-43"><a href="#cb6-43" aria-hidden="true" tabindex="-1"></a>                loss.backward()</span>
<span id="cb6-44"><a href="#cb6-44" aria-hidden="true" tabindex="-1"></a>                optimizer.step()</span>
<span id="cb6-45"><a href="#cb6-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-46"><a href="#cb6-46" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Simulate validation accuracy (here, final training loss as proxy)</span></span>
<span id="cb6-47"><a href="#cb6-47" aria-hidden="true" tabindex="-1"></a>        total_loss <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb6-48"><a href="#cb6-48" aria-hidden="true" tabindex="-1"></a>        <span class="cf">with</span> torch.no_grad():</span>
<span id="cb6-49"><a href="#cb6-49" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> data, target <span class="kw">in</span> train_loader:</span>
<span id="cb6-50"><a href="#cb6-50" aria-hidden="true" tabindex="-1"></a>                data, target <span class="op">=</span> data.to(device), target.to(device)</span>
<span id="cb6-51"><a href="#cb6-51" aria-hidden="true" tabindex="-1"></a>                output <span class="op">=</span> model(data)</span>
<span id="cb6-52"><a href="#cb6-52" aria-hidden="true" tabindex="-1"></a>                total_loss <span class="op">+=</span> criterion(output, target).item()</span>
<span id="cb6-53"><a href="#cb6-53" aria-hidden="true" tabindex="-1"></a>        avg_loss <span class="op">=</span> total_loss <span class="op">/</span> <span class="bu">len</span>(train_loader)</span>
<span id="cb6-54"><a href="#cb6-54" aria-hidden="true" tabindex="-1"></a>        results.append((batch_size, avg_loss))</span>
<span id="cb6-55"><a href="#cb6-55" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"Batch Size: </span><span class="sc">{</span>batch_size<span class="sc">}</span><span class="ss">, Avg Loss: </span><span class="sc">{</span>avg_loss<span class="sc">:.4f}</span><span class="ss">"</span>)       </span>
<span id="cb6-56"><a href="#cb6-56" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-57"><a href="#cb6-57" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> results</span>
<span id="cb6-58"><a href="#cb6-58" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-59"><a href="#cb6-59" aria-hidden="true" tabindex="-1"></a><span class="co"># Example usage with synthetic data</span></span>
<span id="cb6-60"><a href="#cb6-60" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> torch.randn(<span class="dv">1000</span>, <span class="dv">10</span>)  <span class="co"># 1000 samples, 10 features</span></span>
<span id="cb6-61"><a href="#cb6-61" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> torch.randint(<span class="dv">0</span>, <span class="dv">2</span>, (<span class="dv">1000</span>,))  <span class="co"># Binary labels</span></span>
<span id="cb6-62"><a href="#cb6-62" aria-hidden="true" tabindex="-1"></a>train_data <span class="op">=</span> TensorDataset(X, y)</span>
<span id="cb6-63"><a href="#cb6-63" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-64"><a href="#cb6-64" aria-hidden="true" tabindex="-1"></a>results <span class="op">=</span> train_with_different_batch_sizes(train_data)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Batch Size: 16, Avg Loss: 0.6948
Batch Size: 32, Avg Loss: 0.7007
Batch Size: 64, Avg Loss: 0.7129
Batch Size: 128, Avg Loss: 0.7162</code></pre>
</div>
</div>
</section>
<section id="inference-mode-behavior" class="level4" data-number="12.1.6.2">
<h4 data-number="12.1.6.2" class="anchored" data-anchor-id="inference-mode-behavior"><span class="header-section-number">12.1.6.2</span> 2. Inference Mode Behavior</h4>
<p>During training, BatchNorm uses mini-batch statistics. During inference, it uses running estimates of the mean and variance computed during training.</p>
<div id="1db7d6fa" class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.utils.data <span class="im">import</span> DataLoader, TensorDataset</span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Define SimpleNet with BatchNorm</span></span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> SimpleNet(nn.Module):</span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>):</span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>(SimpleNet, <span class="va">self</span>).<span class="fu">__init__</span>()</span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fc1 <span class="op">=</span> nn.Linear(<span class="dv">5</span>, <span class="dv">10</span>)  <span class="co"># 5 input features -&gt; 10</span></span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.bn <span class="op">=</span> nn.BatchNorm1d(<span class="dv">10</span>)  <span class="co"># BatchNorm on 10D output</span></span>
<span id="cb8-11"><a href="#cb8-11" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fc2 <span class="op">=</span> nn.Linear(<span class="dv">10</span>, <span class="dv">2</span>)  <span class="co"># 10 -&gt; 2 output classes</span></span>
<span id="cb8-12"><a href="#cb8-12" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.relu <span class="op">=</span> nn.ReLU()</span>
<span id="cb8-13"><a href="#cb8-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-14"><a href="#cb8-14" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb8-15"><a href="#cb8-15" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.fc1(x)</span>
<span id="cb8-16"><a href="#cb8-16" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.bn(x)  <span class="co"># Apply BatchNorm</span></span>
<span id="cb8-17"><a href="#cb8-17" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.relu(x)</span>
<span id="cb8-18"><a href="#cb8-18" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.fc2(x)</span>
<span id="cb8-19"><a href="#cb8-19" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x</span>
<span id="cb8-20"><a href="#cb8-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-21"><a href="#cb8-21" aria-hidden="true" tabindex="-1"></a><span class="co"># Synthetic data</span></span>
<span id="cb8-22"><a href="#cb8-22" aria-hidden="true" tabindex="-1"></a>X_train <span class="op">=</span> torch.randn(<span class="dv">100</span>, <span class="dv">5</span>)  <span class="co"># 100 samples, 5 features</span></span>
<span id="cb8-23"><a href="#cb8-23" aria-hidden="true" tabindex="-1"></a>y_train <span class="op">=</span> torch.randint(<span class="dv">0</span>, <span class="dv">2</span>, (<span class="dv">100</span>,))  <span class="co"># Binary labels</span></span>
<span id="cb8-24"><a href="#cb8-24" aria-hidden="true" tabindex="-1"></a>train_data <span class="op">=</span> TensorDataset(X_train, y_train)</span>
<span id="cb8-25"><a href="#cb8-25" aria-hidden="true" tabindex="-1"></a>train_loader <span class="op">=</span> DataLoader(train_data, batch_size<span class="op">=</span><span class="dv">32</span>, shuffle<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb8-26"><a href="#cb8-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-27"><a href="#cb8-27" aria-hidden="true" tabindex="-1"></a><span class="co"># Initialize model and optimizer</span></span>
<span id="cb8-28"><a href="#cb8-28" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> SimpleNet()</span>
<span id="cb8-29"><a href="#cb8-29" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> torch.optim.SGD(model.parameters(), lr<span class="op">=</span><span class="fl">0.01</span>)</span>
<span id="cb8-30"><a href="#cb8-30" aria-hidden="true" tabindex="-1"></a>criterion <span class="op">=</span> nn.CrossEntropyLoss()</span>
<span id="cb8-31"><a href="#cb8-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-32"><a href="#cb8-32" aria-hidden="true" tabindex="-1"></a><span class="co"># Training mode - uses batch statistics</span></span>
<span id="cb8-33"><a href="#cb8-33" aria-hidden="true" tabindex="-1"></a>model.train()</span>
<span id="cb8-34"><a href="#cb8-34" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Training mode - BatchNorm uses batch statistics:"</span>)</span>
<span id="cb8-35"><a href="#cb8-35" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">2</span>):  <span class="co"># Short training for demo</span></span>
<span id="cb8-36"><a href="#cb8-36" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> data, target <span class="kw">in</span> train_loader:</span>
<span id="cb8-37"><a href="#cb8-37" aria-hidden="true" tabindex="-1"></a>        optimizer.zero_grad()</span>
<span id="cb8-38"><a href="#cb8-38" aria-hidden="true" tabindex="-1"></a>        output <span class="op">=</span> model(data)</span>
<span id="cb8-39"><a href="#cb8-39" aria-hidden="true" tabindex="-1"></a>        loss <span class="op">=</span> criterion(output, target)</span>
<span id="cb8-40"><a href="#cb8-40" aria-hidden="true" tabindex="-1"></a>        loss.backward()</span>
<span id="cb8-41"><a href="#cb8-41" aria-hidden="true" tabindex="-1"></a>        optimizer.step()</span>
<span id="cb8-42"><a href="#cb8-42" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Print running stats after training</span></span>
<span id="cb8-43"><a href="#cb8-43" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Epoch </span><span class="sc">{</span>epoch<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ss">, Running Mean: </span><span class="sc">{</span>model<span class="sc">.</span>bn<span class="sc">.</span>running_mean[:<span class="dv">3</span>]<span class="sc">}</span><span class="ss">..."</span>)</span>
<span id="cb8-44"><a href="#cb8-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-45"><a href="#cb8-45" aria-hidden="true" tabindex="-1"></a><span class="co"># Inference mode - uses running statistics</span></span>
<span id="cb8-46"><a href="#cb8-46" aria-hidden="true" tabindex="-1"></a>model.<span class="bu">eval</span>()</span>
<span id="cb8-47"><a href="#cb8-47" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Inference mode - BatchNorm uses running statistics:"</span>)</span>
<span id="cb8-48"><a href="#cb8-48" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> torch.no_grad():</span>
<span id="cb8-49"><a href="#cb8-49" aria-hidden="true" tabindex="-1"></a>    X_test <span class="op">=</span> torch.randn(<span class="dv">10</span>, <span class="dv">5</span>)  <span class="co"># Test data</span></span>
<span id="cb8-50"><a href="#cb8-50" aria-hidden="true" tabindex="-1"></a>    output <span class="op">=</span> model(X_test)</span>
<span id="cb8-51"><a href="#cb8-51" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Output for test data (first 3): </span><span class="sc">{</span>output[:<span class="dv">3</span>]<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb8-52"><a href="#cb8-52" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Running Mean (fixed): </span><span class="sc">{</span>model<span class="sc">.</span>bn<span class="sc">.</span>running_mean[:<span class="dv">3</span>]<span class="sc">}</span><span class="ss">..."</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Training mode - BatchNorm uses batch statistics:
Epoch 1, Running Mean: tensor([-0.0338, -0.0677, -0.1086])...
Epoch 2, Running Mean: tensor([ 0.0290, -0.1263, -0.1469])...

Inference mode - BatchNorm uses running statistics:
Output for test data (first 3): tensor([[ 0.5041, -0.6767],
        [ 0.3027, -0.4218],
        [ 0.3367, -0.3680]])
Running Mean (fixed): tensor([ 0.0290, -0.1263, -0.1469])...</code></pre>
</div>
</div>
</section>
<section id="challenges-with-rnns" class="level4" data-number="12.1.6.3">
<h4 data-number="12.1.6.3" class="anchored" data-anchor-id="challenges-with-rnns"><span class="header-section-number">12.1.6.3</span> 3. Challenges with RNNs</h4>
<p>BatchNorm is tricky to use in Recurrent Neural Networks (RNNs) because of the recurrent structure and variable sequence lengths.</p>
<div id="0c6207bc" class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Alternative to BatchNorm for RNNs: Layer Normalization</span></span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> RNNWithLayerNorm(nn.Module):</span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, input_size, hidden_size):</span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>(RNNWithLayerNorm, <span class="va">self</span>).<span class="fu">__init__</span>()</span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.rnn_cell <span class="op">=</span> nn.RNNCell(input_size, hidden_size)</span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.layer_norm <span class="op">=</span> nn.LayerNorm(hidden_size)</span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x, hidden):</span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a>        new_hidden <span class="op">=</span> <span class="va">self</span>.rnn_cell(x, hidden)</span>
<span id="cb10-10"><a href="#cb10-10" aria-hidden="true" tabindex="-1"></a>        normalized_hidden <span class="op">=</span> <span class="va">self</span>.layer_norm(new_hidden)</span>
<span id="cb10-11"><a href="#cb10-11" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> normalized_hidden</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="gradient-clipping-for-rnns" class="level4" data-number="12.1.6.4">
<h4 data-number="12.1.6.4" class="anchored" data-anchor-id="gradient-clipping-for-rnns"><span class="header-section-number">12.1.6.4</span> 4. Gradient Clipping for RNNs</h4>
<p>As mentioned in the input, Gradient Clipping is often used with RNNs to mitigate the exploding gradients problem. BatchNorm doesn’t help much with this.</p>
<div id="d3fd5246" class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Gradient clipping example</span></span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> SimpleNet()</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> torch.optim.Adam(model.parameters(), lr<span class="op">=</span><span class="fl">0.001</span>)</span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a>loss_function <span class="op">=</span> nn.CrossEntropyLoss()  <span class="co"># Define the loss function</span></span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Training loop</span></span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> inputs, targets <span class="kw">in</span> train_loader:</span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Forward pass</span></span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"></a>    outputs <span class="op">=</span> model(inputs)</span>
<span id="cb11-10"><a href="#cb11-10" aria-hidden="true" tabindex="-1"></a>    loss <span class="op">=</span> loss_function(outputs, targets)</span>
<span id="cb11-11"><a href="#cb11-11" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb11-12"><a href="#cb11-12" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Backward pass</span></span>
<span id="cb11-13"><a href="#cb11-13" aria-hidden="true" tabindex="-1"></a>    optimizer.zero_grad()</span>
<span id="cb11-14"><a href="#cb11-14" aria-hidden="true" tabindex="-1"></a>    loss.backward()</span>
<span id="cb11-15"><a href="#cb11-15" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb11-16"><a href="#cb11-16" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Gradient clipping (prevents exploding gradients)</span></span>
<span id="cb11-17"><a href="#cb11-17" aria-hidden="true" tabindex="-1"></a>    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm<span class="op">=</span><span class="fl">1.0</span>)</span>
<span id="cb11-18"><a href="#cb11-18" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb11-19"><a href="#cb11-19" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Update weights</span></span>
<span id="cb11-20"><a href="#cb11-20" aria-hidden="true" tabindex="-1"></a>    optimizer.step()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="computational-overhead" class="level4" data-number="12.1.6.5">
<h4 data-number="12.1.6.5" class="anchored" data-anchor-id="computational-overhead"><span class="header-section-number">12.1.6.5</span> 5. Computational Overhead</h4>
<p>BatchNorm adds computational overhead to your neural network, making each epoch take longer to process.</p>
</section>
</section>
<section id="practical-examples" class="level3" data-number="12.1.7">
<h3 data-number="12.1.7" class="anchored" data-anchor-id="practical-examples"><span class="header-section-number">12.1.7</span> Practical Examples</h3>
<section id="example-1-mnist-classification-with-and-without-batchnorm" class="level4" data-number="12.1.7.1">
<h4 data-number="12.1.7.1" class="anchored" data-anchor-id="example-1-mnist-classification-with-and-without-batchnorm"><span class="header-section-number">12.1.7.1</span> Example 1: MNIST Classification with and without BatchNorm</h4>
<div id="2700fb95" class="cell" data-execution_count="10">
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.optim <span class="im">as</span> optim</span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torchvision</span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torchvision.transforms <span class="im">as</span> transforms</span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.utils.data <span class="im">import</span> DataLoader</span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Load MNIST dataset</span></span>
<span id="cb12-9"><a href="#cb12-9" aria-hidden="true" tabindex="-1"></a>transform <span class="op">=</span> transforms.Compose([transforms.ToTensor(), transforms.Normalize((<span class="fl">0.5</span>,), (<span class="fl">0.5</span>,))])</span>
<span id="cb12-10"><a href="#cb12-10" aria-hidden="true" tabindex="-1"></a>trainset <span class="op">=</span> torchvision.datasets.MNIST(root<span class="op">=</span><span class="st">'./data'</span>, train<span class="op">=</span><span class="va">True</span>, download<span class="op">=</span><span class="va">True</span>, transform<span class="op">=</span>transform)</span>
<span id="cb12-11"><a href="#cb12-11" aria-hidden="true" tabindex="-1"></a>testset <span class="op">=</span> torchvision.datasets.MNIST(root<span class="op">=</span><span class="st">'./data'</span>, train<span class="op">=</span><span class="va">False</span>, download<span class="op">=</span><span class="va">True</span>, transform<span class="op">=</span>transform)</span>
<span id="cb12-12"><a href="#cb12-12" aria-hidden="true" tabindex="-1"></a>trainloader <span class="op">=</span> DataLoader(trainset, batch_size<span class="op">=</span><span class="dv">64</span>, shuffle<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb12-13"><a href="#cb12-13" aria-hidden="true" tabindex="-1"></a>testloader <span class="op">=</span> DataLoader(testset, batch_size<span class="op">=</span><span class="dv">64</span>, shuffle<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb12-14"><a href="#cb12-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-15"><a href="#cb12-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Model without BatchNorm</span></span>
<span id="cb12-16"><a href="#cb12-16" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> NetWithoutBN(nn.Module):</span>
<span id="cb12-17"><a href="#cb12-17" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>):</span>
<span id="cb12-18"><a href="#cb12-18" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>(NetWithoutBN, <span class="va">self</span>).<span class="fu">__init__</span>()</span>
<span id="cb12-19"><a href="#cb12-19" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fc1 <span class="op">=</span> nn.Linear(<span class="dv">28</span><span class="op">*</span><span class="dv">28</span>, <span class="dv">256</span>)</span>
<span id="cb12-20"><a href="#cb12-20" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fc2 <span class="op">=</span> nn.Linear(<span class="dv">256</span>, <span class="dv">128</span>)</span>
<span id="cb12-21"><a href="#cb12-21" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fc3 <span class="op">=</span> nn.Linear(<span class="dv">128</span>, <span class="dv">10</span>)</span>
<span id="cb12-22"><a href="#cb12-22" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb12-23"><a href="#cb12-23" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb12-24"><a href="#cb12-24" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> x.view(<span class="op">-</span><span class="dv">1</span>, <span class="dv">28</span><span class="op">*</span><span class="dv">28</span>)</span>
<span id="cb12-25"><a href="#cb12-25" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> torch.relu(<span class="va">self</span>.fc1(x))</span>
<span id="cb12-26"><a href="#cb12-26" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> torch.relu(<span class="va">self</span>.fc2(x))</span>
<span id="cb12-27"><a href="#cb12-27" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.fc3(x)</span>
<span id="cb12-28"><a href="#cb12-28" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x</span>
<span id="cb12-29"><a href="#cb12-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-30"><a href="#cb12-30" aria-hidden="true" tabindex="-1"></a><span class="co"># Model with BatchNorm</span></span>
<span id="cb12-31"><a href="#cb12-31" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> NetWithBN(nn.Module):</span>
<span id="cb12-32"><a href="#cb12-32" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>):</span>
<span id="cb12-33"><a href="#cb12-33" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>(NetWithBN, <span class="va">self</span>).<span class="fu">__init__</span>()</span>
<span id="cb12-34"><a href="#cb12-34" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fc1 <span class="op">=</span> nn.Linear(<span class="dv">28</span><span class="op">*</span><span class="dv">28</span>, <span class="dv">256</span>)</span>
<span id="cb12-35"><a href="#cb12-35" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.bn1 <span class="op">=</span> nn.BatchNorm1d(<span class="dv">256</span>)</span>
<span id="cb12-36"><a href="#cb12-36" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fc2 <span class="op">=</span> nn.Linear(<span class="dv">256</span>, <span class="dv">128</span>)</span>
<span id="cb12-37"><a href="#cb12-37" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.bn2 <span class="op">=</span> nn.BatchNorm1d(<span class="dv">128</span>)</span>
<span id="cb12-38"><a href="#cb12-38" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fc3 <span class="op">=</span> nn.Linear(<span class="dv">128</span>, <span class="dv">10</span>)</span>
<span id="cb12-39"><a href="#cb12-39" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb12-40"><a href="#cb12-40" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb12-41"><a href="#cb12-41" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> x.view(<span class="op">-</span><span class="dv">1</span>, <span class="dv">28</span><span class="op">*</span><span class="dv">28</span>)</span>
<span id="cb12-42"><a href="#cb12-42" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.fc1(x)</span>
<span id="cb12-43"><a href="#cb12-43" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.bn1(x)</span>
<span id="cb12-44"><a href="#cb12-44" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> torch.relu(x)</span>
<span id="cb12-45"><a href="#cb12-45" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.fc2(x)</span>
<span id="cb12-46"><a href="#cb12-46" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.bn2(x)</span>
<span id="cb12-47"><a href="#cb12-47" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> torch.relu(x)</span>
<span id="cb12-48"><a href="#cb12-48" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.fc3(x)</span>
<span id="cb12-49"><a href="#cb12-49" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x</span>
<span id="cb12-50"><a href="#cb12-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-51"><a href="#cb12-51" aria-hidden="true" tabindex="-1"></a><span class="co"># Training function</span></span>
<span id="cb12-52"><a href="#cb12-52" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> train_model(model, trainloader, epochs<span class="op">=</span><span class="dv">5</span>, lr<span class="op">=</span><span class="fl">0.01</span>):</span>
<span id="cb12-53"><a href="#cb12-53" aria-hidden="true" tabindex="-1"></a>    criterion <span class="op">=</span> nn.CrossEntropyLoss()</span>
<span id="cb12-54"><a href="#cb12-54" aria-hidden="true" tabindex="-1"></a>    optimizer <span class="op">=</span> optim.SGD(model.parameters(), lr<span class="op">=</span>lr, momentum<span class="op">=</span><span class="fl">0.9</span>)</span>
<span id="cb12-55"><a href="#cb12-55" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb12-56"><a href="#cb12-56" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(epochs):</span>
<span id="cb12-57"><a href="#cb12-57" aria-hidden="true" tabindex="-1"></a>        running_loss <span class="op">=</span> <span class="fl">0.0</span></span>
<span id="cb12-58"><a href="#cb12-58" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i, data <span class="kw">in</span> <span class="bu">enumerate</span>(trainloader, <span class="dv">0</span>):</span>
<span id="cb12-59"><a href="#cb12-59" aria-hidden="true" tabindex="-1"></a>            inputs, labels <span class="op">=</span> data</span>
<span id="cb12-60"><a href="#cb12-60" aria-hidden="true" tabindex="-1"></a>            optimizer.zero_grad()</span>
<span id="cb12-61"><a href="#cb12-61" aria-hidden="true" tabindex="-1"></a>            outputs <span class="op">=</span> model(inputs)</span>
<span id="cb12-62"><a href="#cb12-62" aria-hidden="true" tabindex="-1"></a>            loss <span class="op">=</span> criterion(outputs, labels)</span>
<span id="cb12-63"><a href="#cb12-63" aria-hidden="true" tabindex="-1"></a>            loss.backward()</span>
<span id="cb12-64"><a href="#cb12-64" aria-hidden="true" tabindex="-1"></a>            optimizer.step()</span>
<span id="cb12-65"><a href="#cb12-65" aria-hidden="true" tabindex="-1"></a>            running_loss <span class="op">+=</span> loss.item()</span>
<span id="cb12-66"><a href="#cb12-66" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb12-67"><a href="#cb12-67" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f'Epoch </span><span class="sc">{</span>epoch<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ss">, Loss: </span><span class="sc">{</span>running_loss<span class="op">/</span><span class="bu">len</span>(trainloader)<span class="sc">:.3f}</span><span class="ss">'</span>)</span>
<span id="cb12-68"><a href="#cb12-68" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb12-69"><a href="#cb12-69" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">'Finished Training'</span>)</span>
<span id="cb12-70"><a href="#cb12-70" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-71"><a href="#cb12-71" aria-hidden="true" tabindex="-1"></a><span class="co"># Training both models</span></span>
<span id="cb12-72"><a href="#cb12-72" aria-hidden="true" tabindex="-1"></a>model_without_bn <span class="op">=</span> NetWithoutBN()</span>
<span id="cb12-73"><a href="#cb12-73" aria-hidden="true" tabindex="-1"></a>model_with_bn <span class="op">=</span> NetWithBN()</span>
<span id="cb12-74"><a href="#cb12-74" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-75"><a href="#cb12-75" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Training model without BatchNorm:"</span>)</span>
<span id="cb12-76"><a href="#cb12-76" aria-hidden="true" tabindex="-1"></a>train_model(model_without_bn, trainloader)</span>
<span id="cb12-77"><a href="#cb12-77" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-78"><a href="#cb12-78" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Training model with BatchNorm:"</span>)</span>
<span id="cb12-79"><a href="#cb12-79" aria-hidden="true" tabindex="-1"></a>train_model(model_with_bn, trainloader)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Training model without BatchNorm:
Epoch 1, Loss: 0.413
Epoch 2, Loss: 0.169
Epoch 3, Loss: 0.121
Epoch 4, Loss: 0.094
Epoch 5, Loss: 0.080
Finished Training

Training model with BatchNorm:
Epoch 1, Loss: 0.211
Epoch 2, Loss: 0.087
Epoch 3, Loss: 0.061
Epoch 4, Loss: 0.043
Epoch 5, Loss: 0.037
Finished Training</code></pre>
</div>
</div>
</section>
</section>
<section id="example-2-visual-comparison-of-training-stability" class="level3" data-number="12.1.8">
<h3 data-number="12.1.8" class="anchored" data-anchor-id="example-2-visual-comparison-of-training-stability"><span class="header-section-number">12.1.8</span> Example 2: Visual Comparison of Training Stability</h3>
<div id="0f99fa72" class="cell" data-execution_count="11">
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn.functional <span class="im">as</span> F</span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Function to train and record losses</span></span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> train_and_record(model, trainloader, epochs<span class="op">=</span><span class="dv">10</span>, lr<span class="op">=</span><span class="fl">0.01</span>):</span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a>    criterion <span class="op">=</span> nn.CrossEntropyLoss()</span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a>    optimizer <span class="op">=</span> optim.SGD(model.parameters(), lr<span class="op">=</span>lr, momentum<span class="op">=</span><span class="fl">0.9</span>)</span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a>    losses <span class="op">=</span> []</span>
<span id="cb14-10"><a href="#cb14-10" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb14-11"><a href="#cb14-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(epochs):</span>
<span id="cb14-12"><a href="#cb14-12" aria-hidden="true" tabindex="-1"></a>        epoch_losses <span class="op">=</span> []</span>
<span id="cb14-13"><a href="#cb14-13" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> data <span class="kw">in</span> trainloader:</span>
<span id="cb14-14"><a href="#cb14-14" aria-hidden="true" tabindex="-1"></a>            inputs, labels <span class="op">=</span> data</span>
<span id="cb14-15"><a href="#cb14-15" aria-hidden="true" tabindex="-1"></a>            optimizer.zero_grad()</span>
<span id="cb14-16"><a href="#cb14-16" aria-hidden="true" tabindex="-1"></a>            outputs <span class="op">=</span> model(inputs)</span>
<span id="cb14-17"><a href="#cb14-17" aria-hidden="true" tabindex="-1"></a>            loss <span class="op">=</span> criterion(outputs, labels)</span>
<span id="cb14-18"><a href="#cb14-18" aria-hidden="true" tabindex="-1"></a>            loss.backward()</span>
<span id="cb14-19"><a href="#cb14-19" aria-hidden="true" tabindex="-1"></a>            optimizer.step()</span>
<span id="cb14-20"><a href="#cb14-20" aria-hidden="true" tabindex="-1"></a>            epoch_losses.append(loss.item())</span>
<span id="cb14-21"><a href="#cb14-21" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb14-22"><a href="#cb14-22" aria-hidden="true" tabindex="-1"></a>        losses.append(np.mean(epoch_losses))</span>
<span id="cb14-23"><a href="#cb14-23" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb14-24"><a href="#cb14-24" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> losses</span>
<span id="cb14-25"><a href="#cb14-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-26"><a href="#cb14-26" aria-hidden="true" tabindex="-1"></a><span class="co"># Record losses for both models</span></span>
<span id="cb14-27"><a href="#cb14-27" aria-hidden="true" tabindex="-1"></a>losses_without_bn <span class="op">=</span> train_and_record(NetWithoutBN(), trainloader)</span>
<span id="cb14-28"><a href="#cb14-28" aria-hidden="true" tabindex="-1"></a>losses_with_bn <span class="op">=</span> train_and_record(NetWithBN(), trainloader)</span>
<span id="cb14-29"><a href="#cb14-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-30"><a href="#cb14-30" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot results</span></span>
<span id="cb14-31"><a href="#cb14-31" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">5</span>))</span>
<span id="cb14-32"><a href="#cb14-32" aria-hidden="true" tabindex="-1"></a>plt.plot(losses_without_bn, <span class="st">'b-'</span>, label<span class="op">=</span><span class="st">'Without BatchNorm'</span>)</span>
<span id="cb14-33"><a href="#cb14-33" aria-hidden="true" tabindex="-1"></a>plt.plot(losses_with_bn, <span class="st">'r-'</span>, label<span class="op">=</span><span class="st">'With BatchNorm'</span>)</span>
<span id="cb14-34"><a href="#cb14-34" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Epochs'</span>)</span>
<span id="cb14-35"><a href="#cb14-35" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Loss'</span>)</span>
<span id="cb14-36"><a href="#cb14-36" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Training Loss With and Without BatchNorm'</span>)</span>
<span id="cb14-37"><a href="#cb14-37" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb14-38"><a href="#cb14-38" aria-hidden="true" tabindex="-1"></a>plt.grid(<span class="va">True</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="chapter10_files/figure-html/cell-12-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="understanding-batchnorm-intuitively" class="level3" data-number="12.1.9">
<h3 data-number="12.1.9" class="anchored" data-anchor-id="understanding-batchnorm-intuitively"><span class="header-section-number">12.1.9</span> Understanding BatchNorm Intuitively</h3>
<section id="the-highway-analogy" class="level4" data-number="12.1.9.1">
<h4 data-number="12.1.9.1" class="anchored" data-anchor-id="the-highway-analogy"><span class="header-section-number">12.1.9.1</span> The Highway Analogy</h4>
<p>Think of neural network training as a long highway with many cars (gradients) traveling from the output layer back to the input layer. Without BatchNorm, the traffic can get congested (vanishing gradients) or cars can speed out of control (exploding gradients).</p>
<p>BatchNorm acts like a series of traffic regulators placed at regular intervals along this highway. They ensure cars maintain a reasonable, consistent speed, preventing both traffic jams and dangerous racing. This allows for smoother, faster, and more stable journeys.</p>
</section>
<section id="the-cooking-analogy" class="level4" data-number="12.1.9.2">
<h4 data-number="12.1.9.2" class="anchored" data-anchor-id="the-cooking-analogy"><span class="header-section-number">12.1.9.2</span> The Cooking Analogy</h4>
<p>Another way to think about BatchNorm is in terms of cooking. Imagine you’re following a recipe that calls for “3 cups of flour.” But what if your cups are much larger or smaller than the ones the recipe author used?</p>
<p>BatchNorm is like having a standardized measuring system in your kitchen. It ensures that regardless of the “cup” size used in previous steps, each ingredient is measured and adjusted to a standard scale before being used in the next step of the recipe. This standardization makes the cooking process more robust to variations.</p>
</section>
</section>
<section id="conclusion" class="level3" data-number="12.1.10">
<h3 data-number="12.1.10" class="anchored" data-anchor-id="conclusion"><span class="header-section-number">12.1.10</span> Conclusion</h3>
<p>Batch Normalization has become an essential component in modern deep learning architectures due to its ability to accelerate training, improve stability, and enhance generalization. While it adds some computational overhead, the benefits typically far outweigh the costs.</p>
<p>Key takeaways: - BatchNorm stabilizes the distribution of layer inputs during training - It enables higher learning rates and faster convergence - It reduces the vanishing gradient problem - It has a regularizing effect - It works best with moderately-sized mini-batches - It requires special handling for RNNs</p>
<p>Understanding and properly implementing BatchNorm can significantly improve the performance of your deep learning models.</p>
</section>
</section>
<section id="recurrent-neural-networks-a-comprehensive-guide" class="level2" data-number="12.2">
<h2 data-number="12.2" class="anchored" data-anchor-id="recurrent-neural-networks-a-comprehensive-guide"><span class="header-section-number">12.2</span> Recurrent Neural Networks: A Comprehensive Guide</h2>
<section id="introduction-1" class="level3" data-number="12.2.1">
<h3 data-number="12.2.1" class="anchored" data-anchor-id="introduction-1"><span class="header-section-number">12.2.1</span> Introduction</h3>
<p>Recurrent Neural Networks (RNNs) are a class of neural networks specifically designed to handle sequential data and time series problems. Unlike traditional feedforward neural networks, RNNs have connections that form directed cycles, allowing them to maintain a “memory” of previous inputs. This makes them particularly well-suited for tasks where context and order matter, such as natural language processing, speech recognition, and time series forecasting.</p>
<p>This document provides a comprehensive explanation of RNNs, their architecture, training methodologies, and implementations in Python.</p>
</section>
<section id="the-fundamental-concept-of-rnns" class="level3" data-number="12.2.2">
<h3 data-number="12.2.2" class="anchored" data-anchor-id="the-fundamental-concept-of-rnns"><span class="header-section-number">12.2.2</span> The Fundamental Concept of RNNs</h3>
<section id="the-problem-with-traditional-neural-networks" class="level4" data-number="12.2.2.1">
<h4 data-number="12.2.2.1" class="anchored" data-anchor-id="the-problem-with-traditional-neural-networks"><span class="header-section-number">12.2.2.1</span> The Problem with Traditional Neural Networks</h4>
<p>Traditional feedforward neural networks have a significant limitation: they assume that all inputs (and outputs) are independent of each other. This assumption doesn’t hold for many real-world problems:</p>
<ul>
<li>In language, the meaning of a word depends on preceding words</li>
<li>In time series data, past values influence future ones</li>
<li>In videos, understanding a frame requires context from previous frames</li>
</ul>
</section>
<section id="the-solution-recurrent-connections" class="level4" data-number="12.2.2.2">
<h4 data-number="12.2.2.2" class="anchored" data-anchor-id="the-solution-recurrent-connections"><span class="header-section-number">12.2.2.2</span> The Solution: Recurrent Connections</h4>
<p>RNNs address this limitation by introducing recurrent connections, allowing information to persist. A recurrent neuron:</p>
<ul>
<li>Maintains a memory or state from past computations</li>
<li>Takes input from the current time step along with output from the previous time step</li>
<li>Loops data back into the same neuron at every new time instant</li>
</ul>
</section>
<section id="an-analogy-1" class="level4" data-number="12.2.2.3">
<h4 data-number="12.2.2.3" class="anchored" data-anchor-id="an-analogy-1"><span class="header-section-number">12.2.2.3</span> An Analogy</h4>
<p>Think of an RNN like a person reading a book. As you read each word, you don’t start from scratch - you carry forward your understanding of previous words and sentences. This accumulated context helps you understand the current word better. Similarly, an RNN carries forward information from previous time steps to better process the current input.</p>
</section>
</section>
<section id="rnn-architecture" class="level3" data-number="12.2.3">
<h3 data-number="12.2.3" class="anchored" data-anchor-id="rnn-architecture"><span class="header-section-number">12.2.3</span> RNN Architecture</h3>
<section id="the-recurrent-neuron" class="level4" data-number="12.2.3.1">
<h4 data-number="12.2.3.1" class="anchored" data-anchor-id="the-recurrent-neuron"><span class="header-section-number">12.2.3.1</span> The Recurrent Neuron</h4>
<p>The core building block of an RNN is the recurrent neuron. Unlike a standard neuron, a recurrent neuron has two sets of weights:</p>
<ol type="1">
<li><code>W_x</code>: Weights applied to the current input <code>x_t</code></li>
<li><code>W_h</code>: Weights applied to the previous hidden state (or output) <code>h_(t-1)</code></li>
</ol>
<p>The recurrent neuron computes its current hidden state as:</p>
<p><span class="math display">\[h_t = \tanh(W_x \cdot x_t + W_h \cdot h_{t-1} + b)\]</span></p>
<p>Where: - <code>h_t</code> is the hidden state at time step t - <code>x_t</code> is the input at time step t - <code>h_(t-1)</code> is the hidden state from the previous time step - <code>W_x</code>, <code>W_h</code> are weight matrices - <code>b</code> is the bias term - <code>tanh</code> is a non-linear activation function (commonly used in RNNs)</p>
</section>
<section id="unfolded-computational-graph" class="level4" data-number="12.2.3.2">
<h4 data-number="12.2.3.2" class="anchored" data-anchor-id="unfolded-computational-graph"><span class="header-section-number">12.2.3.2</span> Unfolded Computational Graph</h4>
<p>To better understand how RNNs process sequential data, we can “unfold” the recurrent connections across time steps. This unfolded computational graph shows the flow of information through a recurrent layer at every time instant in the sequence.</p>
<p>For example, for a sequence of five time steps, we would unfold the recurrent neuron five times across the number of instants:</p>
<pre><code>x_1 → RNN → h_1 → y_1
      ↓
x_2 → RNN → h_2 → y_2
      ↓
x_3 → RNN → h_3 → y_3
      ↓
x_4 → RNN → h_4 → y_4
      ↓
x_5 → RNN → h_5 → y_5</code></pre>
<p>This unfolding reveals that an RNN can be viewed as multiple copies of the same network, each passing information to its successor.</p>
</section>
<section id="computations-within-a-recurrent-layer" class="level4" data-number="12.2.3.3">
<h4 data-number="12.2.3.3" class="anchored" data-anchor-id="computations-within-a-recurrent-layer"><span class="header-section-number">12.2.3.3</span> Computations within a Recurrent Layer</h4>
<ol type="1">
<li>Each neuron in a recurrent layer receives the output of the previous layer and its current input</li>
<li>Neurons perform an affine transformation of inputs (matrix multiplication plus bias)</li>
<li>This result is passed through a non-linear activation function (typically tanh)</li>
<li>The output is then typically passed to a dense or fully connected layer with a softmax activation function to generate class probabilities</li>
</ol>
</section>
</section>
<section id="recurrent-connection-schemes" class="level3" data-number="12.2.4">
<h3 data-number="12.2.4" class="anchored" data-anchor-id="recurrent-connection-schemes"><span class="header-section-number">12.2.4</span> Recurrent Connection Schemes</h3>
<p>There are two main schemes for forming recurrent connections from one recurrent layer to another:</p>
<section id="recurrent-connections-between-hidden-units" class="level4" data-number="12.2.4.1">
<h4 data-number="12.2.4.1" class="anchored" data-anchor-id="recurrent-connections-between-hidden-units"><span class="header-section-number">12.2.4.1</span> 1. Recurrent Connections Between Hidden Units</h4>
<p>In this scheme, the hidden state from the previous time step is connected to the hidden state of the current time step. This approach: - Better captures high-dimensional features about the past - Allows the network to maintain a more complex state representation</p>
</section>
<section id="recurrent-connections-between-previous-output-and-hidden-unit" class="level4" data-number="12.2.4.2">
<h4 data-number="12.2.4.2" class="anchored" data-anchor-id="recurrent-connections-between-previous-output-and-hidden-unit"><span class="header-section-number">12.2.4.2</span> 2. Recurrent Connections Between Previous Output and Hidden Unit</h4>
<p>Here, the output from the previous time step is connected to the hidden unit of the current time step. This approach: - Is easier to compute and more parallelizable - May capture less complex dependencies in the data</p>
</section>
</section>
<section id="rnn-applications-for-sequence-problems" class="level3" data-number="12.2.5">
<h3 data-number="12.2.5" class="anchored" data-anchor-id="rnn-applications-for-sequence-problems"><span class="header-section-number">12.2.5</span> RNN Applications for Sequence Problems</h3>
<p>RNNs are versatile and can be applied to various sequence-related problems:</p>
<section id="one-to-many-an-input-to-a-sequence-of-outputs" class="level4" data-number="12.2.5.1">
<h4 data-number="12.2.5.1" class="anchored" data-anchor-id="one-to-many-an-input-to-a-sequence-of-outputs"><span class="header-section-number">12.2.5.1</span> One-to-Many: An Input to a Sequence of Outputs</h4>
<p>Example: Image Captioning - Input: A single image - Output: A sequence of words describing the image - The network must generate an entire sentence based on a single static input</p>
<div id="4870257e" class="cell" data-execution_count="12">
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Pseudocode for image captioning RNN</span></span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> image_captioning_rnn(image):</span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Extract features from image using CNN</span></span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a>    image_features <span class="op">=</span> cnn_encoder(image)</span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb16-6"><a href="#cb16-6" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Initialize RNN hidden state with image features</span></span>
<span id="cb16-7"><a href="#cb16-7" aria-hidden="true" tabindex="-1"></a>    hidden_state <span class="op">=</span> initial_transform(image_features)</span>
<span id="cb16-8"><a href="#cb16-8" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb16-9"><a href="#cb16-9" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Generate caption word by word</span></span>
<span id="cb16-10"><a href="#cb16-10" aria-hidden="true" tabindex="-1"></a>    caption <span class="op">=</span> []</span>
<span id="cb16-11"><a href="#cb16-11" aria-hidden="true" tabindex="-1"></a>    current_word <span class="op">=</span> <span class="st">'&lt;START&gt;'</span></span>
<span id="cb16-12"><a href="#cb16-12" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb16-13"><a href="#cb16-13" aria-hidden="true" tabindex="-1"></a>    <span class="cf">while</span> current_word <span class="op">!=</span> <span class="st">'&lt;END&gt;'</span> <span class="kw">and</span> <span class="bu">len</span>(caption) <span class="op">&lt;</span> max_length:</span>
<span id="cb16-14"><a href="#cb16-14" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Predict next word</span></span>
<span id="cb16-15"><a href="#cb16-15" aria-hidden="true" tabindex="-1"></a>        output, hidden_state <span class="op">=</span> rnn_cell(current_word, hidden_state)</span>
<span id="cb16-16"><a href="#cb16-16" aria-hidden="true" tabindex="-1"></a>        current_word <span class="op">=</span> get_most_probable_word(output)</span>
<span id="cb16-17"><a href="#cb16-17" aria-hidden="true" tabindex="-1"></a>        caption.append(current_word)</span>
<span id="cb16-18"><a href="#cb16-18" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb16-19"><a href="#cb16-19" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> caption</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="many-to-one-a-sequence-of-inputs-to-an-output" class="level4" data-number="12.2.5.2">
<h4 data-number="12.2.5.2" class="anchored" data-anchor-id="many-to-one-a-sequence-of-inputs-to-an-output"><span class="header-section-number">12.2.5.2</span> Many-to-One: A Sequence of Inputs to an Output</h4>
<p>Example: Sentiment Analysis - Input: A sequence of words (a review or comment) - Output: A single classification (positive/negative sentiment) - The network must process the entire sequence before making a decision</p>
<div id="f0a54e35" class="cell" data-execution_count="13">
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Pseudocode for sentiment analysis RNN</span></span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> sentiment_analysis_rnn(text_sequence):</span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Initialize hidden state</span></span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a>    hidden_state <span class="op">=</span> initial_zero_state()</span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Process each word in the sequence</span></span>
<span id="cb17-7"><a href="#cb17-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> word <span class="kw">in</span> text_sequence:</span>
<span id="cb17-8"><a href="#cb17-8" aria-hidden="true" tabindex="-1"></a>        word_embedding <span class="op">=</span> embed(word)</span>
<span id="cb17-9"><a href="#cb17-9" aria-hidden="true" tabindex="-1"></a>        hidden_state <span class="op">=</span> rnn_cell(word_embedding, hidden_state)</span>
<span id="cb17-10"><a href="#cb17-10" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb17-11"><a href="#cb17-11" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Final classification based on the last hidden state</span></span>
<span id="cb17-12"><a href="#cb17-12" aria-hidden="true" tabindex="-1"></a>    sentiment <span class="op">=</span> classifier(hidden_state)</span>
<span id="cb17-13"><a href="#cb17-13" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> sentiment</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="many-to-many-synced-synchronized-sequence-input-to-output" class="level4" data-number="12.2.5.3">
<h4 data-number="12.2.5.3" class="anchored" data-anchor-id="many-to-many-synced-synchronized-sequence-input-to-output"><span class="header-section-number">12.2.5.3</span> Many-to-Many (Synced): Synchronized Sequence Input to Output</h4>
<p>Example: Video Classification (frame by frame) - Input: A sequence of video frames - Output: A label for each corresponding frame - The network processes each input and immediately produces the corresponding output</p>
<div id="155955b0" class="cell" data-execution_count="14">
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Pseudocode for video frame classification</span></span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> video_frame_classification(video_frames):</span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Initialize hidden state</span></span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a>    hidden_state <span class="op">=</span> initial_zero_state()</span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true" tabindex="-1"></a>    frame_labels <span class="op">=</span> []</span>
<span id="cb18-6"><a href="#cb18-6" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb18-7"><a href="#cb18-7" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Process each frame and generate corresponding label</span></span>
<span id="cb18-8"><a href="#cb18-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> frame <span class="kw">in</span> video_frames:</span>
<span id="cb18-9"><a href="#cb18-9" aria-hidden="true" tabindex="-1"></a>        frame_features <span class="op">=</span> extract_features(frame)</span>
<span id="cb18-10"><a href="#cb18-10" aria-hidden="true" tabindex="-1"></a>        hidden_state <span class="op">=</span> rnn_cell(frame_features, hidden_state)</span>
<span id="cb18-11"><a href="#cb18-11" aria-hidden="true" tabindex="-1"></a>        label <span class="op">=</span> classifier(hidden_state)</span>
<span id="cb18-12"><a href="#cb18-12" aria-hidden="true" tabindex="-1"></a>        frame_labels.append(label)</span>
<span id="cb18-13"><a href="#cb18-13" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb18-14"><a href="#cb18-14" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> frame_labels</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="many-to-many-encoder-decoder-sequence-to-sequence-architecture" class="level4" data-number="12.2.5.4">
<h4 data-number="12.2.5.4" class="anchored" data-anchor-id="many-to-many-encoder-decoder-sequence-to-sequence-architecture"><span class="header-section-number">12.2.5.4</span> Many-to-Many (Encoder-Decoder): Sequence-to-Sequence Architecture</h4>
<p>Example: Machine Translation - Input: A sequence of words in one language - Output: A sequence of words in another language - The network first encodes the entire input sequence, then generates the output sequence</p>
<div id="3e75f278" class="cell" data-execution_count="15">
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Pseudocode for sequence-to-sequence translation</span></span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> translate_sequence_to_sequence(source_sentence):</span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Encoder phase</span></span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a>    encoder_hidden_state <span class="op">=</span> initial_zero_state()</span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> word <span class="kw">in</span> source_sentence:</span>
<span id="cb19-6"><a href="#cb19-6" aria-hidden="true" tabindex="-1"></a>        word_embedding <span class="op">=</span> embed_source(word)</span>
<span id="cb19-7"><a href="#cb19-7" aria-hidden="true" tabindex="-1"></a>        encoder_hidden_state <span class="op">=</span> encoder_rnn(word_embedding, encoder_hidden_state)</span>
<span id="cb19-8"><a href="#cb19-8" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb19-9"><a href="#cb19-9" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Decoder phase</span></span>
<span id="cb19-10"><a href="#cb19-10" aria-hidden="true" tabindex="-1"></a>    decoder_hidden_state <span class="op">=</span> encoder_hidden_state</span>
<span id="cb19-11"><a href="#cb19-11" aria-hidden="true" tabindex="-1"></a>    output_sentence <span class="op">=</span> []</span>
<span id="cb19-12"><a href="#cb19-12" aria-hidden="true" tabindex="-1"></a>    current_word <span class="op">=</span> <span class="st">'&lt;START&gt;'</span></span>
<span id="cb19-13"><a href="#cb19-13" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb19-14"><a href="#cb19-14" aria-hidden="true" tabindex="-1"></a>    <span class="cf">while</span> current_word <span class="op">!=</span> <span class="st">'&lt;END&gt;'</span> <span class="kw">and</span> <span class="bu">len</span>(output_sentence) <span class="op">&lt;</span> max_length:</span>
<span id="cb19-15"><a href="#cb19-15" aria-hidden="true" tabindex="-1"></a>        word_embedding <span class="op">=</span> embed_target(current_word)</span>
<span id="cb19-16"><a href="#cb19-16" aria-hidden="true" tabindex="-1"></a>        output, decoder_hidden_state <span class="op">=</span> decoder_rnn(word_embedding, decoder_hidden_state)</span>
<span id="cb19-17"><a href="#cb19-17" aria-hidden="true" tabindex="-1"></a>        current_word <span class="op">=</span> get_most_probable_word(output)</span>
<span id="cb19-18"><a href="#cb19-18" aria-hidden="true" tabindex="-1"></a>        output_sentence.append(current_word)</span>
<span id="cb19-19"><a href="#cb19-19" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb19-20"><a href="#cb19-20" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> output_sentence</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
</section>
<section id="training-recurrent-neural-networks" class="level3" data-number="12.2.6">
<h3 data-number="12.2.6" class="anchored" data-anchor-id="training-recurrent-neural-networks"><span class="header-section-number">12.2.6</span> Training Recurrent Neural Networks</h3>
<section id="backpropagation-through-time-bptt" class="level4" data-number="12.2.6.1">
<h4 data-number="12.2.6.1" class="anchored" data-anchor-id="backpropagation-through-time-bptt"><span class="header-section-number">12.2.6.1</span> Backpropagation Through Time (BPTT)</h4>
<p>Standard backpropagation cannot work directly with recurrent structures due to their cyclic nature. Instead, RNNs are trained using Backpropagation Through Time (BPTT).</p>
<p>BPTT works by: 1. Unrolling the recurrent neuron across time instants 2. Applying backpropagation to the unrolled neurons at each time step, as if it were a very deep feedforward network 3. Accumulating gradients across time steps 4. Updating the weights based on these accumulated gradients</p>
<div id="85fe222a" class="cell" data-execution_count="16">
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Pseudocode for BPTT</span></span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> backpropagation_through_time(sequence, true_outputs, model):</span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Forward pass</span></span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a>    hidden_states <span class="op">=</span> []</span>
<span id="cb20-5"><a href="#cb20-5" aria-hidden="true" tabindex="-1"></a>    outputs <span class="op">=</span> []</span>
<span id="cb20-6"><a href="#cb20-6" aria-hidden="true" tabindex="-1"></a>    hidden_state <span class="op">=</span> initial_zero_state()</span>
<span id="cb20-7"><a href="#cb20-7" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb20-8"><a href="#cb20-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> x_t <span class="kw">in</span> sequence:</span>
<span id="cb20-9"><a href="#cb20-9" aria-hidden="true" tabindex="-1"></a>        hidden_state <span class="op">=</span> rnn_cell_forward(x_t, hidden_state)</span>
<span id="cb20-10"><a href="#cb20-10" aria-hidden="true" tabindex="-1"></a>        output <span class="op">=</span> output_layer(hidden_state)</span>
<span id="cb20-11"><a href="#cb20-11" aria-hidden="true" tabindex="-1"></a>        hidden_states.append(hidden_state)</span>
<span id="cb20-12"><a href="#cb20-12" aria-hidden="true" tabindex="-1"></a>        outputs.append(output)</span>
<span id="cb20-13"><a href="#cb20-13" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb20-14"><a href="#cb20-14" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Calculate loss</span></span>
<span id="cb20-15"><a href="#cb20-15" aria-hidden="true" tabindex="-1"></a>    loss <span class="op">=</span> calculate_loss(outputs, true_outputs)</span>
<span id="cb20-16"><a href="#cb20-16" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb20-17"><a href="#cb20-17" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Backward pass (BPTT)</span></span>
<span id="cb20-18"><a href="#cb20-18" aria-hidden="true" tabindex="-1"></a>    d_hidden <span class="op">=</span> zero_gradient()</span>
<span id="cb20-19"><a href="#cb20-19" aria-hidden="true" tabindex="-1"></a>    gradients <span class="op">=</span> initialize_gradients()</span>
<span id="cb20-20"><a href="#cb20-20" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb20-21"><a href="#cb20-21" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> t <span class="kw">in</span> <span class="bu">reversed</span>(<span class="bu">range</span>(<span class="bu">len</span>(sequence))):</span>
<span id="cb20-22"><a href="#cb20-22" aria-hidden="true" tabindex="-1"></a>        d_output <span class="op">=</span> loss_gradient(outputs[t], true_outputs[t])</span>
<span id="cb20-23"><a href="#cb20-23" aria-hidden="true" tabindex="-1"></a>        d_hidden <span class="op">+=</span> output_layer_backward(d_output)</span>
<span id="cb20-24"><a href="#cb20-24" aria-hidden="true" tabindex="-1"></a>        dx, dh_prev, dW_gradients <span class="op">=</span> rnn_cell_backward(d_hidden, hidden_states[t])</span>
<span id="cb20-25"><a href="#cb20-25" aria-hidden="true" tabindex="-1"></a>        d_hidden <span class="op">=</span> dh_prev</span>
<span id="cb20-26"><a href="#cb20-26" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb20-27"><a href="#cb20-27" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Accumulate gradients</span></span>
<span id="cb20-28"><a href="#cb20-28" aria-hidden="true" tabindex="-1"></a>        gradients <span class="op">=</span> update_gradients(gradients, dW_gradients)</span>
<span id="cb20-29"><a href="#cb20-29" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb20-30"><a href="#cb20-30" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> loss, gradients</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="challenges-in-training-rnns" class="level4" data-number="12.2.6.2">
<h4 data-number="12.2.6.2" class="anchored" data-anchor-id="challenges-in-training-rnns"><span class="header-section-number">12.2.6.2</span> Challenges in Training RNNs</h4>
<section id="the-vanishing-and-exploding-gradient-problem" class="level5" data-number="12.2.6.2.1">
<h5 data-number="12.2.6.2.1" class="anchored" data-anchor-id="the-vanishing-and-exploding-gradient-problem"><span class="header-section-number">12.2.6.2.1</span> The Vanishing and Exploding Gradient Problem</h5>
<p>One of the major challenges in training RNNs is the vanishing and exploding gradient problem. This occurs because:</p>
<ol type="1">
<li>During BPTT, gradients are multiplied by the same weight matrix repeatedly</li>
<li>If the largest eigenvalue of this matrix is &gt; 1, gradients explode</li>
<li>If the largest eigenvalue is &lt; 1, gradients vanish</li>
</ol>
<p>This problem is particularly severe for long sequences, as the gradient either becomes extremely small (vanishing) or extremely large (exploding) after many time steps.</p>
</section>
<section id="solutions-to-gradient-problems" class="level5" data-number="12.2.6.2.2">
<h5 data-number="12.2.6.2.2" class="anchored" data-anchor-id="solutions-to-gradient-problems"><span class="header-section-number">12.2.6.2.2</span> Solutions to Gradient Problems</h5>
<ol type="1">
<li><p><strong>Gradient Clipping</strong>: Limits the magnitude of gradients during training to prevent explosion</p>
<div id="26693793" class="cell" data-execution_count="17">
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a> <span class="co"># Define a simple model</span></span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a> model <span class="op">=</span> tf.keras.Sequential([</span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a>     tf.keras.layers.Input(shape<span class="op">=</span>(<span class="dv">5</span>,)),  <span class="co"># 5 features, preferred over input_shape</span></span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"></a>     tf.keras.layers.Dense(<span class="dv">10</span>, activation<span class="op">=</span><span class="st">'relu'</span>),</span>
<span id="cb21-5"><a href="#cb21-5" aria-hidden="true" tabindex="-1"></a>     tf.keras.layers.Dense(<span class="dv">2</span>)  <span class="co"># 2 output classes</span></span>
<span id="cb21-6"><a href="#cb21-6" aria-hidden="true" tabindex="-1"></a> ])</span>
<span id="cb21-7"><a href="#cb21-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-8"><a href="#cb21-8" aria-hidden="true" tabindex="-1"></a> <span class="co"># Synthetic data</span></span>
<span id="cb21-9"><a href="#cb21-9" aria-hidden="true" tabindex="-1"></a> X_train <span class="op">=</span> np.random.randn(<span class="dv">100</span>, <span class="dv">5</span>)  <span class="co"># 100 samples, 5 features</span></span>
<span id="cb21-10"><a href="#cb21-10" aria-hidden="true" tabindex="-1"></a> y_train <span class="op">=</span> np.random.randint(<span class="dv">0</span>, <span class="dv">2</span>, (<span class="dv">100</span>,))  <span class="co"># Binary labels</span></span>
<span id="cb21-11"><a href="#cb21-11" aria-hidden="true" tabindex="-1"></a> train_data <span class="op">=</span> tf.data.Dataset.from_tensor_slices((X_train, y_train)).batch(<span class="dv">32</span>)</span>
<span id="cb21-12"><a href="#cb21-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-13"><a href="#cb21-13" aria-hidden="true" tabindex="-1"></a> <span class="co"># Optimizer</span></span>
<span id="cb21-14"><a href="#cb21-14" aria-hidden="true" tabindex="-1"></a> optimizer <span class="op">=</span> tf.keras.optimizers.Adam(learning_rate<span class="op">=</span><span class="fl">0.001</span>)</span>
<span id="cb21-15"><a href="#cb21-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-16"><a href="#cb21-16" aria-hidden="true" tabindex="-1"></a> <span class="co"># Training loop</span></span>
<span id="cb21-17"><a href="#cb21-17" aria-hidden="true" tabindex="-1"></a> <span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">2</span>):  <span class="co"># Short demo</span></span>
<span id="cb21-18"><a href="#cb21-18" aria-hidden="true" tabindex="-1"></a>     <span class="cf">for</span> inputs, targets <span class="kw">in</span> train_data:</span>
<span id="cb21-19"><a href="#cb21-19" aria-hidden="true" tabindex="-1"></a>         <span class="cf">with</span> tf.GradientTape() <span class="im">as</span> tape:</span>
<span id="cb21-20"><a href="#cb21-20" aria-hidden="true" tabindex="-1"></a>             outputs <span class="op">=</span> model(inputs, training<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb21-21"><a href="#cb21-21" aria-hidden="true" tabindex="-1"></a>             <span class="co"># Compute mean loss across batch</span></span>
<span id="cb21-22"><a href="#cb21-22" aria-hidden="true" tabindex="-1"></a>             loss <span class="op">=</span> tf.reduce_mean(</span>
<span id="cb21-23"><a href="#cb21-23" aria-hidden="true" tabindex="-1"></a>                 tf.keras.losses.sparse_categorical_crossentropy(targets, outputs, from_logits<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb21-24"><a href="#cb21-24" aria-hidden="true" tabindex="-1"></a>             )</span>
<span id="cb21-25"><a href="#cb21-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-26"><a href="#cb21-26" aria-hidden="true" tabindex="-1"></a>         <span class="co"># Compute gradients</span></span>
<span id="cb21-27"><a href="#cb21-27" aria-hidden="true" tabindex="-1"></a>         gradients <span class="op">=</span> tape.gradient(loss, model.trainable_variables)</span>
<span id="cb21-28"><a href="#cb21-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-29"><a href="#cb21-29" aria-hidden="true" tabindex="-1"></a>         <span class="co"># Gradient clipping</span></span>
<span id="cb21-30"><a href="#cb21-30" aria-hidden="true" tabindex="-1"></a>         clipped_gradients, _ <span class="op">=</span> tf.clip_by_global_norm(gradients, clip_norm<span class="op">=</span><span class="fl">5.0</span>)</span>
<span id="cb21-31"><a href="#cb21-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-32"><a href="#cb21-32" aria-hidden="true" tabindex="-1"></a>         <span class="co"># Apply clipped gradients</span></span>
<span id="cb21-33"><a href="#cb21-33" aria-hidden="true" tabindex="-1"></a>         optimizer.apply_gradients(<span class="bu">zip</span>(clipped_gradients, model.trainable_variables))</span>
<span id="cb21-34"><a href="#cb21-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-35"><a href="#cb21-35" aria-hidden="true" tabindex="-1"></a>     <span class="bu">print</span>(<span class="ss">f"Epoch </span><span class="sc">{</span>epoch<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ss">, Loss: </span><span class="sc">{</span>loss<span class="sc">.</span>numpy()<span class="sc">:.4f}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 1, Loss: 1.6372
Epoch 2, Loss: 1.5903</code></pre>
</div>
</div></li>
<li><p><strong>Batch Normalization</strong>: Stabilizes the distribution of inputs to each layer</p>
<div class="sourceCode" id="cb23"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Adding batch normalization to RNN</span></span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> BatchNormalization()(x)</span>
<span id="cb23-3"><a href="#cb23-3" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> SimpleRNN(units<span class="op">=</span><span class="dv">64</span>)(x)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div></li>
<li><p><strong>ReLU Activation</strong>: Can help with the vanishing gradient problem</p>
<div class="sourceCode" id="cb24"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Using ReLU instead of tanh in a custom RNN cell</span></span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a>next_h <span class="op">=</span> keras.activations.relu(np.dot(x_t, Wx) <span class="op">+</span> np.dot(prev_h, Wh) <span class="op">+</span> b)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div></li>
<li><p><strong>Long Short-Term Memory (LSTM)</strong>: A special type of RNN designed to handle long-term dependencies</p></li>
</ol>
<p>Despite these solutions, the limitations of basic RNNs with long-term dependencies led to the development of more sophisticated architectures, particularly the Long Short-Term Memory (LSTM) cell, which we’ll discuss in another document.</p>
</section>
</section>
</section>
<section id="implementing-rnns-in-python" class="level3" data-number="12.2.7">
<h3 data-number="12.2.7" class="anchored" data-anchor-id="implementing-rnns-in-python"><span class="header-section-number">12.2.7</span> Implementing RNNs in Python</h3>
<section id="simple-rnn-implementation-in-tensorflowkeras" class="level4" data-number="12.2.7.1">
<h4 data-number="12.2.7.1" class="anchored" data-anchor-id="simple-rnn-implementation-in-tensorflowkeras"><span class="header-section-number">12.2.7.1</span> Simple RNN Implementation in TensorFlow/Keras</h4>
<p>Here’s how to implement a basic RNN using TensorFlow’s built-in <code>SimpleRNN</code> layer:</p>
<div id="2cb2914a" class="cell" data-execution_count="18">
<div class="sourceCode cell-code" id="cb25"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> tensorflow <span class="im">as</span> tf</span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb25-3"><a href="#cb25-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tensorflow.keras.models <span class="im">import</span> Sequential</span>
<span id="cb25-4"><a href="#cb25-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tensorflow.keras.layers <span class="im">import</span> SimpleRNN, Dense, Embedding</span>
<span id="cb25-5"><a href="#cb25-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-6"><a href="#cb25-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Example: Sentiment Analysis with SimpleRNN</span></span>
<span id="cb25-7"><a href="#cb25-7" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> build_sentiment_model(vocab_size, embedding_dim, max_length):</span>
<span id="cb25-8"><a href="#cb25-8" aria-hidden="true" tabindex="-1"></a>    model <span class="op">=</span> Sequential([</span>
<span id="cb25-9"><a href="#cb25-9" aria-hidden="true" tabindex="-1"></a>        Embedding(vocab_size, embedding_dim, input_length<span class="op">=</span>max_length),</span>
<span id="cb25-10"><a href="#cb25-10" aria-hidden="true" tabindex="-1"></a>        SimpleRNN(<span class="dv">64</span>),  <span class="co"># 64 units in the RNN layer</span></span>
<span id="cb25-11"><a href="#cb25-11" aria-hidden="true" tabindex="-1"></a>        Dense(<span class="dv">1</span>, activation<span class="op">=</span><span class="st">'sigmoid'</span>)  <span class="co"># Binary classification (positive/negative)</span></span>
<span id="cb25-12"><a href="#cb25-12" aria-hidden="true" tabindex="-1"></a>    ])</span>
<span id="cb25-13"><a href="#cb25-13" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb25-14"><a href="#cb25-14" aria-hidden="true" tabindex="-1"></a>    model.<span class="bu">compile</span>(</span>
<span id="cb25-15"><a href="#cb25-15" aria-hidden="true" tabindex="-1"></a>        optimizer<span class="op">=</span><span class="st">'adam'</span>,</span>
<span id="cb25-16"><a href="#cb25-16" aria-hidden="true" tabindex="-1"></a>        loss<span class="op">=</span><span class="st">'binary_crossentropy'</span>,</span>
<span id="cb25-17"><a href="#cb25-17" aria-hidden="true" tabindex="-1"></a>        metrics<span class="op">=</span>[<span class="st">'accuracy'</span>]</span>
<span id="cb25-18"><a href="#cb25-18" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb25-19"><a href="#cb25-19" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb25-20"><a href="#cb25-20" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> model</span>
<span id="cb25-21"><a href="#cb25-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-22"><a href="#cb25-22" aria-hidden="true" tabindex="-1"></a><span class="co"># Example: Using SimpleRNN with different configurations</span></span>
<span id="cb25-23"><a href="#cb25-23" aria-hidden="true" tabindex="-1"></a><span class="co"># By default, SimpleRNN only returns the final hidden state</span></span>
<span id="cb25-24"><a href="#cb25-24" aria-hidden="true" tabindex="-1"></a>inputs <span class="op">=</span> np.random.random([<span class="dv">32</span>, <span class="dv">10</span>, <span class="dv">8</span>]).astype(np.float32)  <span class="co"># Batch size=32, seq_length=10, input_dim=8</span></span>
<span id="cb25-25"><a href="#cb25-25" aria-hidden="true" tabindex="-1"></a>simple_rnn <span class="op">=</span> tf.keras.layers.SimpleRNN(<span class="dv">4</span>)  <span class="co"># 4 RNN units</span></span>
<span id="cb25-26"><a href="#cb25-26" aria-hidden="true" tabindex="-1"></a>output <span class="op">=</span> simple_rnn(inputs)  <span class="co"># Output shape is [32, 4]</span></span>
<span id="cb25-27"><a href="#cb25-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-28"><a href="#cb25-28" aria-hidden="true" tabindex="-1"></a><span class="co"># To get the entire sequence output and final state</span></span>
<span id="cb25-29"><a href="#cb25-29" aria-hidden="true" tabindex="-1"></a>simple_rnn_full <span class="op">=</span> tf.keras.layers.SimpleRNN(</span>
<span id="cb25-30"><a href="#cb25-30" aria-hidden="true" tabindex="-1"></a>    <span class="dv">4</span>, return_sequences<span class="op">=</span><span class="va">True</span>, return_state<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb25-31"><a href="#cb25-31" aria-hidden="true" tabindex="-1"></a>whole_sequence_output, final_state <span class="op">=</span> simple_rnn_full(inputs)</span>
<span id="cb25-32"><a href="#cb25-32" aria-hidden="true" tabindex="-1"></a><span class="co"># whole_sequence_output shape: [32, 10, 4]</span></span>
<span id="cb25-33"><a href="#cb25-33" aria-hidden="true" tabindex="-1"></a><span class="co"># final_state shape: [32, 4]</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="a-more-complex-rnn-example-text-generation" class="level4" data-number="12.2.7.2">
<h4 data-number="12.2.7.2" class="anchored" data-anchor-id="a-more-complex-rnn-example-text-generation"><span class="header-section-number">12.2.7.2</span> A More Complex RNN Example: Text Generation</h4>
<div id="5ace9478" class="cell" data-execution_count="19">
<div class="sourceCode cell-code" id="cb26"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> tensorflow <span class="im">as</span> tf</span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb26-3"><a href="#cb26-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tensorflow.keras.models <span class="im">import</span> Sequential</span>
<span id="cb26-4"><a href="#cb26-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tensorflow.keras.layers <span class="im">import</span> SimpleRNN, Dense, Embedding</span>
<span id="cb26-5"><a href="#cb26-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tensorflow.keras.preprocessing.text <span class="im">import</span> Tokenizer</span>
<span id="cb26-6"><a href="#cb26-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tensorflow.keras.preprocessing.sequence <span class="im">import</span> pad_sequences</span>
<span id="cb26-7"><a href="#cb26-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-8"><a href="#cb26-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Example text data</span></span>
<span id="cb26-9"><a href="#cb26-9" aria-hidden="true" tabindex="-1"></a>texts <span class="op">=</span> [</span>
<span id="cb26-10"><a href="#cb26-10" aria-hidden="true" tabindex="-1"></a>    <span class="st">"I love machine learning"</span>,</span>
<span id="cb26-11"><a href="#cb26-11" aria-hidden="true" tabindex="-1"></a>    <span class="st">"RNNs are great for sequential data"</span>,</span>
<span id="cb26-12"><a href="#cb26-12" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Natural language processing is fascinating"</span>,</span>
<span id="cb26-13"><a href="#cb26-13" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Deep learning models can solve complex problems"</span></span>
<span id="cb26-14"><a href="#cb26-14" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb26-15"><a href="#cb26-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-16"><a href="#cb26-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Tokenize the text</span></span>
<span id="cb26-17"><a href="#cb26-17" aria-hidden="true" tabindex="-1"></a>tokenizer <span class="op">=</span> Tokenizer()</span>
<span id="cb26-18"><a href="#cb26-18" aria-hidden="true" tabindex="-1"></a>tokenizer.fit_on_texts(texts)</span>
<span id="cb26-19"><a href="#cb26-19" aria-hidden="true" tabindex="-1"></a>vocab_size <span class="op">=</span> <span class="bu">len</span>(tokenizer.word_index) <span class="op">+</span> <span class="dv">1</span></span>
<span id="cb26-20"><a href="#cb26-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-21"><a href="#cb26-21" aria-hidden="true" tabindex="-1"></a><span class="co"># Prepare training data for text generation</span></span>
<span id="cb26-22"><a href="#cb26-22" aria-hidden="true" tabindex="-1"></a>sequences <span class="op">=</span> []</span>
<span id="cb26-23"><a href="#cb26-23" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> text <span class="kw">in</span> texts:</span>
<span id="cb26-24"><a href="#cb26-24" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Convert text to sequence of integers</span></span>
<span id="cb26-25"><a href="#cb26-25" aria-hidden="true" tabindex="-1"></a>    encoded <span class="op">=</span> tokenizer.texts_to_sequences([text])[<span class="dv">0</span>]</span>
<span id="cb26-26"><a href="#cb26-26" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Create input-output pairs for each position in the sequence</span></span>
<span id="cb26-27"><a href="#cb26-27" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>, <span class="bu">len</span>(encoded)):</span>
<span id="cb26-28"><a href="#cb26-28" aria-hidden="true" tabindex="-1"></a>        sequences.append(encoded[:i<span class="op">+</span><span class="dv">1</span>])</span>
<span id="cb26-29"><a href="#cb26-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-30"><a href="#cb26-30" aria-hidden="true" tabindex="-1"></a><span class="co"># Pad sequences to the same length</span></span>
<span id="cb26-31"><a href="#cb26-31" aria-hidden="true" tabindex="-1"></a>max_length <span class="op">=</span> <span class="bu">max</span>([<span class="bu">len</span>(seq) <span class="cf">for</span> seq <span class="kw">in</span> sequences])</span>
<span id="cb26-32"><a href="#cb26-32" aria-hidden="true" tabindex="-1"></a>padded_sequences <span class="op">=</span> pad_sequences(sequences, maxlen<span class="op">=</span>max_length, padding<span class="op">=</span><span class="st">'pre'</span>)</span>
<span id="cb26-33"><a href="#cb26-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-34"><a href="#cb26-34" aria-hidden="true" tabindex="-1"></a><span class="co"># Split into input (X) and output (y)</span></span>
<span id="cb26-35"><a href="#cb26-35" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> padded_sequences[:, :<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb26-36"><a href="#cb26-36" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> tf.keras.utils.to_categorical(padded_sequences[:, <span class="op">-</span><span class="dv">1</span>], num_classes<span class="op">=</span>vocab_size)</span>
<span id="cb26-37"><a href="#cb26-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-38"><a href="#cb26-38" aria-hidden="true" tabindex="-1"></a><span class="co"># Build the model</span></span>
<span id="cb26-39"><a href="#cb26-39" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> Sequential([</span>
<span id="cb26-40"><a href="#cb26-40" aria-hidden="true" tabindex="-1"></a>    Embedding(vocab_size, <span class="dv">10</span>, input_length<span class="op">=</span>max_length<span class="op">-</span><span class="dv">1</span>),</span>
<span id="cb26-41"><a href="#cb26-41" aria-hidden="true" tabindex="-1"></a>    SimpleRNN(<span class="dv">64</span>, return_sequences<span class="op">=</span><span class="va">False</span>),</span>
<span id="cb26-42"><a href="#cb26-42" aria-hidden="true" tabindex="-1"></a>    Dense(vocab_size, activation<span class="op">=</span><span class="st">'softmax'</span>)</span>
<span id="cb26-43"><a href="#cb26-43" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb26-44"><a href="#cb26-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-45"><a href="#cb26-45" aria-hidden="true" tabindex="-1"></a>model.<span class="bu">compile</span>(</span>
<span id="cb26-46"><a href="#cb26-46" aria-hidden="true" tabindex="-1"></a>    optimizer<span class="op">=</span><span class="st">'adam'</span>,</span>
<span id="cb26-47"><a href="#cb26-47" aria-hidden="true" tabindex="-1"></a>    loss<span class="op">=</span><span class="st">'categorical_crossentropy'</span>,</span>
<span id="cb26-48"><a href="#cb26-48" aria-hidden="true" tabindex="-1"></a>    metrics<span class="op">=</span>[<span class="st">'accuracy'</span>]</span>
<span id="cb26-49"><a href="#cb26-49" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb26-50"><a href="#cb26-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-51"><a href="#cb26-51" aria-hidden="true" tabindex="-1"></a><span class="co"># Function to generate text</span></span>
<span id="cb26-52"><a href="#cb26-52" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> generate_text(seed_text, model, tokenizer, max_length, num_words<span class="op">=</span><span class="dv">10</span>):</span>
<span id="cb26-53"><a href="#cb26-53" aria-hidden="true" tabindex="-1"></a>    result <span class="op">=</span> seed_text</span>
<span id="cb26-54"><a href="#cb26-54" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb26-55"><a href="#cb26-55" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(num_words):</span>
<span id="cb26-56"><a href="#cb26-56" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Encode the text</span></span>
<span id="cb26-57"><a href="#cb26-57" aria-hidden="true" tabindex="-1"></a>        encoded <span class="op">=</span> tokenizer.texts_to_sequences([result])[<span class="dv">0</span>]</span>
<span id="cb26-58"><a href="#cb26-58" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Pad the sequence</span></span>
<span id="cb26-59"><a href="#cb26-59" aria-hidden="true" tabindex="-1"></a>        padded <span class="op">=</span> pad_sequences([encoded], maxlen<span class="op">=</span>max_length<span class="op">-</span><span class="dv">1</span>, padding<span class="op">=</span><span class="st">'pre'</span>)</span>
<span id="cb26-60"><a href="#cb26-60" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb26-61"><a href="#cb26-61" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Predict the next word</span></span>
<span id="cb26-62"><a href="#cb26-62" aria-hidden="true" tabindex="-1"></a>        prediction <span class="op">=</span> model.predict(padded, verbose<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb26-63"><a href="#cb26-63" aria-hidden="true" tabindex="-1"></a>        index <span class="op">=</span> np.argmax(prediction)</span>
<span id="cb26-64"><a href="#cb26-64" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb26-65"><a href="#cb26-65" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Convert the index to a word</span></span>
<span id="cb26-66"><a href="#cb26-66" aria-hidden="true" tabindex="-1"></a>        word <span class="op">=</span> <span class="st">""</span></span>
<span id="cb26-67"><a href="#cb26-67" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> key, value <span class="kw">in</span> tokenizer.word_index.items():</span>
<span id="cb26-68"><a href="#cb26-68" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> value <span class="op">==</span> index:</span>
<span id="cb26-69"><a href="#cb26-69" aria-hidden="true" tabindex="-1"></a>                word <span class="op">=</span> key</span>
<span id="cb26-70"><a href="#cb26-70" aria-hidden="true" tabindex="-1"></a>                <span class="cf">break</span></span>
<span id="cb26-71"><a href="#cb26-71" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb26-72"><a href="#cb26-72" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Append the word to the result</span></span>
<span id="cb26-73"><a href="#cb26-73" aria-hidden="true" tabindex="-1"></a>        result <span class="op">+=</span> <span class="st">" "</span> <span class="op">+</span> word</span>
<span id="cb26-74"><a href="#cb26-74" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb26-75"><a href="#cb26-75" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> result</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
</section>
<section id="understanding-rnns-visually" class="level3" data-number="12.2.8">
<h3 data-number="12.2.8" class="anchored" data-anchor-id="understanding-rnns-visually"><span class="header-section-number">12.2.8</span> Understanding RNNs Visually</h3>
<section id="the-chain-analogy" class="level4" data-number="12.2.8.1">
<h4 data-number="12.2.8.1" class="anchored" data-anchor-id="the-chain-analogy"><span class="header-section-number">12.2.8.1</span> The Chain Analogy</h4>
<p>A helpful way to visualize an RNN is to think of it as a chain. Each link in the chain represents a time step, and information flows from one link to the next. The strength of the chain depends on how well information can flow through it:</p>
<ul>
<li>In traditional RNNs, the links can weaken over long distances (vanishing gradients)</li>
<li>Special RNN architectures like LSTMs strengthen these links to allow information to flow more easily across long distances</li>
</ul>
</section>
<section id="the-memory-analogy" class="level4" data-number="12.2.8.2">
<h4 data-number="12.2.8.2" class="anchored" data-anchor-id="the-memory-analogy"><span class="header-section-number">12.2.8.2</span> The Memory Analogy</h4>
<p>Another useful analogy is to think of an RNN as a person with a notepad:</p>
<ol type="1">
<li>At each time step, the person receives new information (input)</li>
<li>They update their notes (hidden state) based on what they already wrote and the new information</li>
<li>They can choose what to remember (strong weights) and what to forget (weak weights)</li>
<li>Their final understanding (output) depends on what they’ve accumulated in their notes</li>
</ol>
<p>This analogy helps explain why RNNs can struggle with very long sequences - just as a person’s notes might become cluttered or they might forget early details, an RNN’s ability to maintain relevant information degrades over long sequences.</p>
</section>
</section>
<section id="practical-considerations" class="level3" data-number="12.2.9">
<h3 data-number="12.2.9" class="anchored" data-anchor-id="practical-considerations"><span class="header-section-number">12.2.9</span> Practical Considerations</h3>
<section id="when-to-use-rnns" class="level4" data-number="12.2.9.1">
<h4 data-number="12.2.9.1" class="anchored" data-anchor-id="when-to-use-rnns"><span class="header-section-number">12.2.9.1</span> When to Use RNNs</h4>
<p>RNNs are particularly useful for:</p>
<ol type="1">
<li>Sequential or time-series data where order matters</li>
<li>Natural language processing tasks (text generation, sentiment analysis)</li>
<li>Speech recognition</li>
<li>Time series forecasting</li>
<li>Video analysis</li>
</ol>
</section>
<section id="limitations-of-simple-rnns" class="level4" data-number="12.2.9.2">
<h4 data-number="12.2.9.2" class="anchored" data-anchor-id="limitations-of-simple-rnns"><span class="header-section-number">12.2.9.2</span> Limitations of Simple RNNs</h4>
<p>Basic RNNs have several limitations:</p>
<ol type="1">
<li>They struggle with long-term dependencies due to vanishing/exploding gradients</li>
<li>Training can be slow due to the sequential nature (difficult to parallelize)</li>
<li>They may not capture complex patterns as effectively as more advanced architectures</li>
</ol>
</section>
<section id="advanced-rnn-architectures" class="level4" data-number="12.2.9.3">
<h4 data-number="12.2.9.3" class="anchored" data-anchor-id="advanced-rnn-architectures"><span class="header-section-number">12.2.9.3</span> Advanced RNN Architectures</h4>
<p>Because of these limitations, more sophisticated architectures have been developed:</p>
<ol type="1">
<li><strong>Long Short-Term Memory (LSTM)</strong>: Addresses the vanishing gradient problem with special gates</li>
<li><strong>Gated Recurrent Unit (GRU)</strong>: A simplified version of LSTM with fewer parameters</li>
<li><strong>Bidirectional RNNs</strong>: Process sequences in both forward and backward directions</li>
<li><strong>Deep RNNs</strong>: Stack multiple RNN layers for more complex representations</li>
</ol>
</section>
</section>
<section id="conclusion-1" class="level3" data-number="12.2.10">
<h3 data-number="12.2.10" class="anchored" data-anchor-id="conclusion-1"><span class="header-section-number">12.2.10</span> Conclusion</h3>
<p>Recurrent Neural Networks represent a powerful class of neural network architectures specifically designed for sequential data. By maintaining state information across time steps, they can capture temporal dependencies and patterns that traditional feedforward networks cannot.</p>
<p>While basic RNNs face challenges with long-term dependencies, techniques like gradient clipping and advanced architectures like LSTMs have helped overcome these limitations. Today, RNNs and their variants form the backbone of many state-of-the-art systems in natural language processing, speech recognition, and time series analysis.</p>
<p>Key takeaways: - RNNs maintain memory of past inputs through recurrent connections - They can be unfolded across time steps and trained with BPTT - They face challenges with vanishing and exploding gradients - They’re suitable for various sequence-to-sequence tasks - Advanced variants like LSTMs improve their ability to capture long-term dependencies</p>
<p>Understanding and implementing RNNs provides a foundation for working with sequential data across numerous application domains.</p>
</section>
</section>
<section id="deep-learning-frameworks" class="level2" data-number="12.3">
<h2 data-number="12.3" class="anchored" data-anchor-id="deep-learning-frameworks"><span class="header-section-number">12.3</span> Deep Learning Frameworks</h2>
<p>Deep Learning frameworks are designed to abstract away complex mathematical operations, allowing developers and researchers to focus on the architecture and design of neural networks rather than implementation details.</p>
<section id="popular-deep-learning-frameworks" class="level3" data-number="12.3.1">
<h3 data-number="12.3.1" class="anchored" data-anchor-id="popular-deep-learning-frameworks"><span class="header-section-number">12.3.1</span> Popular Deep Learning Frameworks</h3>
<p>Several frameworks have emerged as leaders in the deep learning ecosystem:</p>
<ul>
<li><strong>TensorFlow/Keras (Google)</strong>: One of the most widely used frameworks with strong industry adoption</li>
<li><strong>PyTorch</strong>: Developed by Facebook’s AI Research lab, known for its dynamic computation graph</li>
<li><strong>Caffe</strong>: Berkeley Vision and Learning Center’s framework focused on expressiveness, speed, and modularity</li>
<li><strong>Microsoft Cognitive Toolkit (CNTK)</strong>: Microsoft’s open-source toolkit optimized for performance</li>
</ul>
<div id="658fc47f" class="cell" data-execution_count="20">
<div class="sourceCode cell-code" id="cb27"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a><span class="co"># TensorFlow/Keras example</span></span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> tensorflow <span class="im">as</span> tf</span>
<span id="cb27-3"><a href="#cb27-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tensorflow <span class="im">import</span> keras</span>
<span id="cb27-4"><a href="#cb27-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-5"><a href="#cb27-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a simple model</span></span>
<span id="cb27-6"><a href="#cb27-6" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> keras.Sequential([</span>
<span id="cb27-7"><a href="#cb27-7" aria-hidden="true" tabindex="-1"></a>    keras.layers.Dense(<span class="dv">128</span>, activation<span class="op">=</span><span class="st">'relu'</span>, input_shape<span class="op">=</span>(<span class="dv">784</span>,)),</span>
<span id="cb27-8"><a href="#cb27-8" aria-hidden="true" tabindex="-1"></a>    keras.layers.Dropout(<span class="fl">0.2</span>),</span>
<span id="cb27-9"><a href="#cb27-9" aria-hidden="true" tabindex="-1"></a>    keras.layers.Dense(<span class="dv">10</span>, activation<span class="op">=</span><span class="st">'softmax'</span>)</span>
<span id="cb27-10"><a href="#cb27-10" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb27-11"><a href="#cb27-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-12"><a href="#cb27-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Compile the model</span></span>
<span id="cb27-13"><a href="#cb27-13" aria-hidden="true" tabindex="-1"></a>model.<span class="bu">compile</span>(optimizer<span class="op">=</span><span class="st">'adam'</span>,</span>
<span id="cb27-14"><a href="#cb27-14" aria-hidden="true" tabindex="-1"></a>              loss<span class="op">=</span><span class="st">'sparse_categorical_crossentropy'</span>,</span>
<span id="cb27-15"><a href="#cb27-15" aria-hidden="true" tabindex="-1"></a>              metrics<span class="op">=</span>[<span class="st">'accuracy'</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="040562e6" class="cell" data-execution_count="21">
<div class="sourceCode cell-code" id="cb28"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a><span class="co"># PyTorch example</span></span>
<span id="cb28-2"><a href="#cb28-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb28-3"><a href="#cb28-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb28-4"><a href="#cb28-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn.functional <span class="im">as</span> F</span>
<span id="cb28-5"><a href="#cb28-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-6"><a href="#cb28-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Define a simple neural network</span></span>
<span id="cb28-7"><a href="#cb28-7" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> SimpleNet(nn.Module):</span>
<span id="cb28-8"><a href="#cb28-8" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>):</span>
<span id="cb28-9"><a href="#cb28-9" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>(SimpleNet, <span class="va">self</span>).<span class="fu">__init__</span>()</span>
<span id="cb28-10"><a href="#cb28-10" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fc1 <span class="op">=</span> nn.Linear(<span class="dv">784</span>, <span class="dv">128</span>)</span>
<span id="cb28-11"><a href="#cb28-11" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.dropout <span class="op">=</span> nn.Dropout(<span class="fl">0.2</span>)</span>
<span id="cb28-12"><a href="#cb28-12" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fc2 <span class="op">=</span> nn.Linear(<span class="dv">128</span>, <span class="dv">10</span>)</span>
<span id="cb28-13"><a href="#cb28-13" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb28-14"><a href="#cb28-14" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb28-15"><a href="#cb28-15" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> F.relu(<span class="va">self</span>.fc1(x))</span>
<span id="cb28-16"><a href="#cb28-16" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.dropout(x)</span>
<span id="cb28-17"><a href="#cb28-17" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.fc2(x)</span>
<span id="cb28-18"><a href="#cb28-18" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> F.log_softmax(x, dim<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb28-19"><a href="#cb28-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-20"><a href="#cb28-20" aria-hidden="true" tabindex="-1"></a><span class="co"># Create an instance of the network</span></span>
<span id="cb28-21"><a href="#cb28-21" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> SimpleNet()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="evolution-of-neural-network-architectures" class="level3" data-number="12.3.2">
<h3 data-number="12.3.2" class="anchored" data-anchor-id="evolution-of-neural-network-architectures"><span class="header-section-number">12.3.2</span> Evolution of Neural Network Architectures</h3>
<p>The field has seen significant evolution in neural network architectures over the years:</p>
<ul>
<li><strong>LeNet (1998)</strong>: One of the earliest convolutional neural networks, designed for handwritten digit recognition</li>
<li><strong>AlexNet (2012)</strong>: A deeper CNN that won the ImageNet competition and laid the groundwork for VGG and ResNet</li>
<li><strong>ResNet-50 (2015)</strong>: Introduced residual connections to solve the vanishing gradient problem in very deep networks</li>
<li><strong>Transformer (2017)</strong>: Revolutionized NLP with its attention mechanism, forming the basis for models like BERT and GPT</li>
</ul>
<div class="sourceCode" id="cb29"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Example of ResNet-50 implementation</span></span>
<span id="cb29-2"><a href="#cb29-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> tensorflow <span class="im">as</span> tf</span>
<span id="cb29-3"><a href="#cb29-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tensorflow.keras.applications <span class="im">import</span> ResNet50</span>
<span id="cb29-4"><a href="#cb29-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-5"><a href="#cb29-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Load pre-trained ResNet-50</span></span>
<span id="cb29-6"><a href="#cb29-6" aria-hidden="true" tabindex="-1"></a>resnet_model <span class="op">=</span> ResNet50(weights<span class="op">=</span><span class="st">'imagenet'</span>, include_top<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb29-7"><a href="#cb29-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-8"><a href="#cb29-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Example prediction</span></span>
<span id="cb29-9"><a href="#cb29-9" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tensorflow.keras.preprocessing <span class="im">import</span> image</span>
<span id="cb29-10"><a href="#cb29-10" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tensorflow.keras.applications.resnet50 <span class="im">import</span> preprocess_input, decode_predictions</span>
<span id="cb29-11"><a href="#cb29-11" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb29-12"><a href="#cb29-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-13"><a href="#cb29-13" aria-hidden="true" tabindex="-1"></a>img_path <span class="op">=</span> <span class="st">'elephant.jpg'</span></span>
<span id="cb29-14"><a href="#cb29-14" aria-hidden="true" tabindex="-1"></a>img <span class="op">=</span> image.load_img(img_path, target_size<span class="op">=</span>(<span class="dv">224</span>, <span class="dv">224</span>))</span>
<span id="cb29-15"><a href="#cb29-15" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> image.img_to_array(img)</span>
<span id="cb29-16"><a href="#cb29-16" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> np.expand_dims(x, axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb29-17"><a href="#cb29-17" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> preprocess_input(x)</span>
<span id="cb29-18"><a href="#cb29-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-19"><a href="#cb29-19" aria-hidden="true" tabindex="-1"></a>preds <span class="op">=</span> resnet_model.predict(x)</span>
<span id="cb29-20"><a href="#cb29-20" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Predicted:'</span>, decode_predictions(preds, top<span class="op">=</span><span class="dv">3</span>)[<span class="dv">0</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
</section>
<section id="model-training-approaches" class="level2" data-number="12.4">
<h2 data-number="12.4" class="anchored" data-anchor-id="model-training-approaches"><span class="header-section-number">12.4</span> Model Training Approaches</h2>
<section id="model-training-steps" class="level3" data-number="12.4.1">
<h3 data-number="12.4.1" class="anchored" data-anchor-id="model-training-steps"><span class="header-section-number">12.4.1</span> Model Training Steps</h3>
<p>The training process for deep learning models typically involves three main steps:</p>
<ol type="1">
<li>Building a computational graph from the network definition</li>
<li>Inputting training data and computing a loss function</li>
<li>Updating parameters based on gradients</li>
</ol>
</section>
<section id="training-paradigms" class="level3" data-number="12.4.2">
<h3 data-number="12.4.2" class="anchored" data-anchor-id="training-paradigms"><span class="header-section-number">12.4.2</span> Training Paradigms</h3>
<p>Two main paradigms exist for training deep learning models:</p>
<section id="define-and-run" class="level4" data-number="12.4.2.1">
<h4 data-number="12.4.2.1" class="anchored" data-anchor-id="define-and-run"><span class="header-section-number">12.4.2.1</span> Define-and-Run</h4>
<p>Frameworks like TensorFlow (traditional) and Caffe complete step one (building the computational graph) in advance of step two (inputting data). This means:</p>
<ul>
<li>The entire computational graph is defined before any data flows through</li>
<li>Optimization can be performed on the graph before execution</li>
<li>Less flexibility during runtime but potentially better performance</li>
</ul>
</section>
<section id="define-by-run" class="level4" data-number="12.4.2.2">
<h4 data-number="12.4.2.2" class="anchored" data-anchor-id="define-by-run"><span class="header-section-number">12.4.2.2</span> Define-by-Run</h4>
<p>Frameworks like PyTorch combine steps one and two into a single step. This means:</p>
<ul>
<li>The computational graph is not given before training but is built dynamically during training</li>
<li>More intuitive for debugging and experimentation</li>
<li>Greater flexibility at runtime</li>
</ul>
<div class="sourceCode" id="cb30"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Define-and-run example (Traditional TensorFlow)</span></span>
<span id="cb30-2"><a href="#cb30-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> tensorflow <span class="im">as</span> tf</span>
<span id="cb30-3"><a href="#cb30-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-4"><a href="#cb30-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Define the graph</span></span>
<span id="cb30-5"><a href="#cb30-5" aria-hidden="true" tabindex="-1"></a>a <span class="op">=</span> tf.constant(<span class="fl">5.0</span>)</span>
<span id="cb30-6"><a href="#cb30-6" aria-hidden="true" tabindex="-1"></a>b <span class="op">=</span> tf.constant(<span class="fl">6.0</span>)</span>
<span id="cb30-7"><a href="#cb30-7" aria-hidden="true" tabindex="-1"></a>c <span class="op">=</span> a <span class="op">*</span> b</span>
<span id="cb30-8"><a href="#cb30-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-9"><a href="#cb30-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Execute the graph</span></span>
<span id="cb30-10"><a href="#cb30-10" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> tf.Session() <span class="im">as</span> sess:</span>
<span id="cb30-11"><a href="#cb30-11" aria-hidden="true" tabindex="-1"></a>    result <span class="op">=</span> sess.run(c)</span>
<span id="cb30-12"><a href="#cb30-12" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(result)  <span class="co"># 30.0</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="sourceCode" id="cb31"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Define-by-run example (PyTorch)</span></span>
<span id="cb31-2"><a href="#cb31-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb31-3"><a href="#cb31-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-4"><a href="#cb31-4" aria-hidden="true" tabindex="-1"></a>a <span class="op">=</span> torch.tensor(<span class="fl">5.0</span>)</span>
<span id="cb31-5"><a href="#cb31-5" aria-hidden="true" tabindex="-1"></a>b <span class="op">=</span> torch.tensor(<span class="fl">6.0</span>)</span>
<span id="cb31-6"><a href="#cb31-6" aria-hidden="true" tabindex="-1"></a>c <span class="op">=</span> a <span class="op">*</span> b</span>
<span id="cb31-7"><a href="#cb31-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-8"><a href="#cb31-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(c.item())  <span class="co"># 30.0</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>Analogy</strong>: Think of “define-and-run” like creating a blueprint for a house before building it. Everything is planned in advance, and then executed according to the plan. “Define-by-run” is more like building a house room by room, making decisions as you go.</p>
</div>
</div>
</section>
</section>
<section id="onnx-open-neural-network-exchange" class="level3" data-number="12.4.3">
<h3 data-number="12.4.3" class="anchored" data-anchor-id="onnx-open-neural-network-exchange"><span class="header-section-number">12.4.3</span> ONNX: Open Neural Network eXchange</h3>
<p>ONNX is an open-source shared model representation for framework interoperability:</p>
<ul>
<li>Provides a common file format for deep learning models</li>
<li>Defines a common set of operators and data types</li>
<li>Enables developers to use models with various frameworks, tools, runtimes, and compilers</li>
<li>Supports an extensible computation graph model (including TensorFlow support)</li>
</ul>
<div class="sourceCode" id="cb32"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Example of exporting a PyTorch model to ONNX</span></span>
<span id="cb32-2"><a href="#cb32-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb32-3"><a href="#cb32-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torchvision</span>
<span id="cb32-4"><a href="#cb32-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-5"><a href="#cb32-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Load a pre-trained model</span></span>
<span id="cb32-6"><a href="#cb32-6" aria-hidden="true" tabindex="-1"></a>dummy_input <span class="op">=</span> torch.randn(<span class="dv">1</span>, <span class="dv">3</span>, <span class="dv">224</span>, <span class="dv">224</span>)</span>
<span id="cb32-7"><a href="#cb32-7" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> torchvision.models.resnet18(pretrained<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb32-8"><a href="#cb32-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-9"><a href="#cb32-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Export to ONNX</span></span>
<span id="cb32-10"><a href="#cb32-10" aria-hidden="true" tabindex="-1"></a>torch.onnx.export(model,               <span class="co"># model being run</span></span>
<span id="cb32-11"><a href="#cb32-11" aria-hidden="true" tabindex="-1"></a>                  dummy_input,         <span class="co"># model input (or a tuple for multiple inputs)</span></span>
<span id="cb32-12"><a href="#cb32-12" aria-hidden="true" tabindex="-1"></a>                  <span class="st">"resnet18.onnx"</span>,     <span class="co"># where to save the model</span></span>
<span id="cb32-13"><a href="#cb32-13" aria-hidden="true" tabindex="-1"></a>                  export_params<span class="op">=</span><span class="va">True</span>,  <span class="co"># store the trained parameter weights inside the model file</span></span>
<span id="cb32-14"><a href="#cb32-14" aria-hidden="true" tabindex="-1"></a>                  opset_version<span class="op">=</span><span class="dv">10</span>,    <span class="co"># the ONNX version to export the model to</span></span>
<span id="cb32-15"><a href="#cb32-15" aria-hidden="true" tabindex="-1"></a>                  do_constant_folding<span class="op">=</span><span class="va">True</span>,  <span class="co"># whether to execute constant folding for optimization</span></span>
<span id="cb32-16"><a href="#cb32-16" aria-hidden="true" tabindex="-1"></a>                  input_names <span class="op">=</span> [<span class="st">'input'</span>],   <span class="co"># the model's input names</span></span>
<span id="cb32-17"><a href="#cb32-17" aria-hidden="true" tabindex="-1"></a>                  output_names <span class="op">=</span> [<span class="st">'output'</span>]) <span class="co"># the model's output names</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
</section>
<section id="distributed-machine-learning" class="level2" data-number="12.5">
<h2 data-number="12.5" class="anchored" data-anchor-id="distributed-machine-learning"><span class="header-section-number">12.5</span> Distributed Machine Learning</h2>
<section id="non-distributed-vs.-distributed-approach" class="level3" data-number="12.5.1">
<h3 data-number="12.5.1" class="anchored" data-anchor-id="non-distributed-vs.-distributed-approach"><span class="header-section-number">12.5.1</span> Non-Distributed vs.&nbsp;Distributed Approach</h3>
<section id="non-distributed-ml" class="level4" data-number="12.5.1.1">
<h4 data-number="12.5.1.1" class="anchored" data-anchor-id="non-distributed-ml"><span class="header-section-number">12.5.1.1</span> Non-Distributed ML</h4>
<p>In the standard (non-distributed) approach: - A single machine loads the entire model - All training data is processed on this single machine - Limited by the computational resources of a single node</p>
</section>
<section id="distributed-ml" class="level4" data-number="12.5.1.2">
<h4 data-number="12.5.1.2" class="anchored" data-anchor-id="distributed-ml"><span class="header-section-number">12.5.1.2</span> Distributed ML</h4>
<p>In distributed approaches: - Multiple machines work together to train a model - Can use data parallelism, model parallelism, or hybrid approaches - Scales to much larger models and datasets</p>
</section>
</section>
<section id="parallelization-methods-in-distributed-deep-learning" class="level3" data-number="12.5.2">
<h3 data-number="12.5.2" class="anchored" data-anchor-id="parallelization-methods-in-distributed-deep-learning"><span class="header-section-number">12.5.2</span> Parallelization Methods in Distributed Deep Learning</h3>
<p>There are three main parallelization strategies:</p>
<section id="data-parallelism" class="level4" data-number="12.5.2.1">
<h4 data-number="12.5.2.1" class="anchored" data-anchor-id="data-parallelism"><span class="header-section-number">12.5.2.1</span> 1. Data Parallelism</h4>
<ul>
<li>Multiple machines load identical copies of the DL model</li>
<li>Training data is split into non-overlapping chunks</li>
<li>Each worker performs training on its chunk of data</li>
<li>Model parameters need to be synchronized between workers</li>
</ul>
<div class="sourceCode" id="cb33"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a><span class="co"># PyTorch Distributed Data Parallel example</span></span>
<span id="cb33-2"><a href="#cb33-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb33-3"><a href="#cb33-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.distributed <span class="im">as</span> dist</span>
<span id="cb33-4"><a href="#cb33-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb33-5"><a href="#cb33-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.multiprocessing <span class="im">as</span> mp</span>
<span id="cb33-6"><a href="#cb33-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.nn.parallel <span class="im">import</span> DistributedDataParallel <span class="im">as</span> DDP</span>
<span id="cb33-7"><a href="#cb33-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-8"><a href="#cb33-8" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> setup(rank, world_size):</span>
<span id="cb33-9"><a href="#cb33-9" aria-hidden="true" tabindex="-1"></a>    dist.init_process_group(<span class="st">"gloo"</span>, rank<span class="op">=</span>rank, world_size<span class="op">=</span>world_size)</span>
<span id="cb33-10"><a href="#cb33-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-11"><a href="#cb33-11" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> cleanup():</span>
<span id="cb33-12"><a href="#cb33-12" aria-hidden="true" tabindex="-1"></a>    dist.destroy_process_group()</span>
<span id="cb33-13"><a href="#cb33-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-14"><a href="#cb33-14" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> ToyModel(nn.Module):</span>
<span id="cb33-15"><a href="#cb33-15" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>):</span>
<span id="cb33-16"><a href="#cb33-16" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>(ToyModel, <span class="va">self</span>).<span class="fu">__init__</span>()</span>
<span id="cb33-17"><a href="#cb33-17" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.net <span class="op">=</span> nn.Linear(<span class="dv">10</span>, <span class="dv">2</span>)</span>
<span id="cb33-18"><a href="#cb33-18" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb33-19"><a href="#cb33-19" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb33-20"><a href="#cb33-20" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.net(x)</span>
<span id="cb33-21"><a href="#cb33-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-22"><a href="#cb33-22" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> demo_basic(rank, world_size):</span>
<span id="cb33-23"><a href="#cb33-23" aria-hidden="true" tabindex="-1"></a>    setup(rank, world_size)</span>
<span id="cb33-24"><a href="#cb33-24" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb33-25"><a href="#cb33-25" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Create model and move it to GPU with id rank</span></span>
<span id="cb33-26"><a href="#cb33-26" aria-hidden="true" tabindex="-1"></a>    model <span class="op">=</span> ToyModel().to(rank)</span>
<span id="cb33-27"><a href="#cb33-27" aria-hidden="true" tabindex="-1"></a>    ddp_model <span class="op">=</span> DDP(model, device_ids<span class="op">=</span>[rank])</span>
<span id="cb33-28"><a href="#cb33-28" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb33-29"><a href="#cb33-29" aria-hidden="true" tabindex="-1"></a>    loss_fn <span class="op">=</span> nn.MSELoss()</span>
<span id="cb33-30"><a href="#cb33-30" aria-hidden="true" tabindex="-1"></a>    optimizer <span class="op">=</span> torch.optim.SGD(ddp_model.parameters(), lr<span class="op">=</span><span class="fl">0.001</span>)</span>
<span id="cb33-31"><a href="#cb33-31" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb33-32"><a href="#cb33-32" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Forward pass</span></span>
<span id="cb33-33"><a href="#cb33-33" aria-hidden="true" tabindex="-1"></a>    outputs <span class="op">=</span> ddp_model(torch.randn(<span class="dv">20</span>, <span class="dv">10</span>).to(rank))</span>
<span id="cb33-34"><a href="#cb33-34" aria-hidden="true" tabindex="-1"></a>    labels <span class="op">=</span> torch.randn(<span class="dv">20</span>, <span class="dv">2</span>).to(rank)</span>
<span id="cb33-35"><a href="#cb33-35" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Backward pass</span></span>
<span id="cb33-36"><a href="#cb33-36" aria-hidden="true" tabindex="-1"></a>    loss <span class="op">=</span> loss_fn(outputs, labels)</span>
<span id="cb33-37"><a href="#cb33-37" aria-hidden="true" tabindex="-1"></a>    loss.backward()</span>
<span id="cb33-38"><a href="#cb33-38" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Update parameters</span></span>
<span id="cb33-39"><a href="#cb33-39" aria-hidden="true" tabindex="-1"></a>    optimizer.step()</span>
<span id="cb33-40"><a href="#cb33-40" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb33-41"><a href="#cb33-41" aria-hidden="true" tabindex="-1"></a>    cleanup()</span>
<span id="cb33-42"><a href="#cb33-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-43"><a href="#cb33-43" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> run_demo(demo_fn, world_size):</span>
<span id="cb33-44"><a href="#cb33-44" aria-hidden="true" tabindex="-1"></a>    mp.spawn(demo_fn, args<span class="op">=</span>(world_size,), nprocs<span class="op">=</span>world_size, join<span class="op">=</span><span class="va">True</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>Analogy</strong>: Data parallelism is like having multiple chefs preparing the same dish using the same recipe but with different ingredients. At the end, they share their experiences to improve the recipe.</p>
</div>
</div>
</section>
<section id="model-parallelism" class="level4" data-number="12.5.2.2">
<h4 data-number="12.5.2.2" class="anchored" data-anchor-id="model-parallelism"><span class="header-section-number">12.5.2.2</span> 2. Model Parallelism</h4>
<ul>
<li>The DL model is split across workers, with each worker loading a different part</li>
<li>Workers holding the input layer receive training data</li>
<li>In the forward pass, output signals are propagated to workers holding the next layer</li>
<li>In backpropagation, gradients flow from output to input layer workers</li>
</ul>
<div class="sourceCode" id="cb34"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Simple model parallelism example (conceptual)</span></span>
<span id="cb34-2"><a href="#cb34-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb34-3"><a href="#cb34-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb34-4"><a href="#cb34-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-5"><a href="#cb34-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Define a model that will be split across two devices</span></span>
<span id="cb34-6"><a href="#cb34-6" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> SplitModel(nn.Module):</span>
<span id="cb34-7"><a href="#cb34-7" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>):</span>
<span id="cb34-8"><a href="#cb34-8" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>(SplitModel, <span class="va">self</span>).<span class="fu">__init__</span>()</span>
<span id="cb34-9"><a href="#cb34-9" aria-hidden="true" tabindex="-1"></a>        <span class="co"># First part of the model on GPU 0</span></span>
<span id="cb34-10"><a href="#cb34-10" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.part1 <span class="op">=</span> nn.Sequential(</span>
<span id="cb34-11"><a href="#cb34-11" aria-hidden="true" tabindex="-1"></a>            nn.Linear(<span class="dv">1000</span>, <span class="dv">512</span>),</span>
<span id="cb34-12"><a href="#cb34-12" aria-hidden="true" tabindex="-1"></a>            nn.ReLU(),</span>
<span id="cb34-13"><a href="#cb34-13" aria-hidden="true" tabindex="-1"></a>            nn.Linear(<span class="dv">512</span>, <span class="dv">256</span>)</span>
<span id="cb34-14"><a href="#cb34-14" aria-hidden="true" tabindex="-1"></a>        ).to(<span class="st">'cuda:0'</span>)</span>
<span id="cb34-15"><a href="#cb34-15" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb34-16"><a href="#cb34-16" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Second part of the model on GPU 1</span></span>
<span id="cb34-17"><a href="#cb34-17" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.part2 <span class="op">=</span> nn.Sequential(</span>
<span id="cb34-18"><a href="#cb34-18" aria-hidden="true" tabindex="-1"></a>            nn.Linear(<span class="dv">256</span>, <span class="dv">128</span>),</span>
<span id="cb34-19"><a href="#cb34-19" aria-hidden="true" tabindex="-1"></a>            nn.ReLU(),</span>
<span id="cb34-20"><a href="#cb34-20" aria-hidden="true" tabindex="-1"></a>            nn.Linear(<span class="dv">128</span>, <span class="dv">10</span>)</span>
<span id="cb34-21"><a href="#cb34-21" aria-hidden="true" tabindex="-1"></a>        ).to(<span class="st">'cuda:1'</span>)</span>
<span id="cb34-22"><a href="#cb34-22" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb34-23"><a href="#cb34-23" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb34-24"><a href="#cb34-24" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Input is on GPU 0</span></span>
<span id="cb34-25"><a href="#cb34-25" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> x.to(<span class="st">'cuda:0'</span>)</span>
<span id="cb34-26"><a href="#cb34-26" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.part1(x)</span>
<span id="cb34-27"><a href="#cb34-27" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Transfer intermediate output to GPU 1</span></span>
<span id="cb34-28"><a href="#cb34-28" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> x.to(<span class="st">'cuda:1'</span>)</span>
<span id="cb34-29"><a href="#cb34-29" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.part2(x)</span>
<span id="cb34-30"><a href="#cb34-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-31"><a href="#cb34-31" aria-hidden="true" tabindex="-1"></a><span class="co"># Create the model</span></span>
<span id="cb34-32"><a href="#cb34-32" aria-hidden="true" tabindex="-1"></a>split_model <span class="op">=</span> SplitModel()</span>
<span id="cb34-33"><a href="#cb34-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-34"><a href="#cb34-34" aria-hidden="true" tabindex="-1"></a><span class="co"># Example input data</span></span>
<span id="cb34-35"><a href="#cb34-35" aria-hidden="true" tabindex="-1"></a>input_data <span class="op">=</span> torch.randn(<span class="dv">64</span>, <span class="dv">1000</span>)  <span class="co"># Batch size 64, input dim 1000</span></span>
<span id="cb34-36"><a href="#cb34-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-37"><a href="#cb34-37" aria-hidden="true" tabindex="-1"></a><span class="co"># Forward pass</span></span>
<span id="cb34-38"><a href="#cb34-38" aria-hidden="true" tabindex="-1"></a>output <span class="op">=</span> split_model(input_data)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>Analogy</strong>: Model parallelism is like an assembly line in a factory, where each worker specializes in one part of the process. The partially completed product moves from worker to worker until it’s finished.</p>
</div>
</div>
</section>
<section id="pipeline-parallelism" class="level4" data-number="12.5.2.3">
<h4 data-number="12.5.2.3" class="anchored" data-anchor-id="pipeline-parallelism"><span class="header-section-number">12.5.2.3</span> 3. Pipeline Parallelism</h4>
<ul>
<li>A hybrid approach that combines aspects of both data and model parallelism</li>
<li>The model is split into stages that run on different devices</li>
<li>Multiple batches of data are processed simultaneously in different pipeline stages</li>
<li>Reduces idle time compared to pure model parallelism</li>
</ul>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>Analogy</strong>: Pipeline parallelism is like a car wash with multiple stations (soap, rinse, dry). While one car is being rinsed, another can be getting soaped, and a third can be drying, making the whole process more efficient.</p>
</div>
</div>
</section>
</section>
<section id="synchronization-strategies" class="level3" data-number="12.5.3">
<h3 data-number="12.5.3" class="anchored" data-anchor-id="synchronization-strategies"><span class="header-section-number">12.5.3</span> Synchronization Strategies</h3>
<p>When using data parallelism, parameter synchronization is crucial:</p>
<ol type="1">
<li><strong>Synchronous SGD</strong>: All workers wait for each other before updating parameters
<ul>
<li>More stable training but potentially slower</li>
</ul></li>
<li><strong>Asynchronous SGD</strong>: Workers update parameters independently
<ul>
<li>Faster but can lead to inconsistent updates</li>
</ul></li>
</ol>
<div class="sourceCode" id="cb35"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a><span class="co"># TensorFlow distributed strategy example</span></span>
<span id="cb35-2"><a href="#cb35-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> tensorflow <span class="im">as</span> tf</span>
<span id="cb35-3"><a href="#cb35-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-4"><a href="#cb35-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a strategy for data parallelism</span></span>
<span id="cb35-5"><a href="#cb35-5" aria-hidden="true" tabindex="-1"></a>strategy <span class="op">=</span> tf.distribute.MirroredStrategy()</span>
<span id="cb35-6"><a href="#cb35-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-7"><a href="#cb35-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Create the model inside the strategy scope</span></span>
<span id="cb35-8"><a href="#cb35-8" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> strategy.scope():</span>
<span id="cb35-9"><a href="#cb35-9" aria-hidden="true" tabindex="-1"></a>    model <span class="op">=</span> tf.keras.Sequential([</span>
<span id="cb35-10"><a href="#cb35-10" aria-hidden="true" tabindex="-1"></a>        tf.keras.layers.Dense(<span class="dv">256</span>, activation<span class="op">=</span><span class="st">'relu'</span>, input_shape<span class="op">=</span>(<span class="dv">784</span>,)),</span>
<span id="cb35-11"><a href="#cb35-11" aria-hidden="true" tabindex="-1"></a>        tf.keras.layers.Dropout(<span class="fl">0.2</span>),</span>
<span id="cb35-12"><a href="#cb35-12" aria-hidden="true" tabindex="-1"></a>        tf.keras.layers.Dense(<span class="dv">10</span>, activation<span class="op">=</span><span class="st">'softmax'</span>)</span>
<span id="cb35-13"><a href="#cb35-13" aria-hidden="true" tabindex="-1"></a>    ])</span>
<span id="cb35-14"><a href="#cb35-14" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb35-15"><a href="#cb35-15" aria-hidden="true" tabindex="-1"></a>    model.<span class="bu">compile</span>(optimizer<span class="op">=</span><span class="st">'adam'</span>,</span>
<span id="cb35-16"><a href="#cb35-16" aria-hidden="true" tabindex="-1"></a>                  loss<span class="op">=</span><span class="st">'sparse_categorical_crossentropy'</span>,</span>
<span id="cb35-17"><a href="#cb35-17" aria-hidden="true" tabindex="-1"></a>                  metrics<span class="op">=</span>[<span class="st">'accuracy'</span>])</span>
<span id="cb35-18"><a href="#cb35-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-19"><a href="#cb35-19" aria-hidden="true" tabindex="-1"></a><span class="co"># Train the model (automatically handled in distributed way)</span></span>
<span id="cb35-20"><a href="#cb35-20" aria-hidden="true" tabindex="-1"></a>model.fit(train_dataset, epochs<span class="op">=</span><span class="dv">10</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="conclusion-2" class="level3" data-number="12.5.4">
<h3 data-number="12.5.4" class="anchored" data-anchor-id="conclusion-2"><span class="header-section-number">12.5.4</span> Conclusion</h3>
</section>
</section>
<section id="introduction-to-tensorflow" class="level2" data-number="12.6">
<h2 data-number="12.6" class="anchored" data-anchor-id="introduction-to-tensorflow"><span class="header-section-number">12.6</span> Introduction to TensorFlow</h2>
<p>TensorFlow is a powerful, open-source machine learning framework developed by the Google Brain Team. This document provides a comprehensive overview of TensorFlow, its architecture, and how to use it effectively for machine learning and deep learning projects.</p>
<section id="what-is-tensorflow" class="level3" data-number="12.6.1">
<h3 data-number="12.6.1" class="anchored" data-anchor-id="what-is-tensorflow"><span class="header-section-number">12.6.1</span> What is TensorFlow?</h3>
<p>TensorFlow (tf) is an end-to-end open-source platform for machine learning that provides:</p>
<ul>
<li>A comprehensive ecosystem of tools, libraries, and community resources</li>
<li>Flexible architecture that allows deployment across various platforms (CPU, GPU, TPU)</li>
<li>High-level APIs like Keras for easy model building</li>
<li>Low-level APIs for advanced research and development</li>
</ul>
<p><strong>Analogy</strong>: Think of TensorFlow as a sophisticated construction kit for building AI systems. Like LEGO blocks that snap together to create complex structures, TensorFlow provides building blocks (operations) that connect together to form computational graphs that power machine learning models.</p>
<div id="b548fe37" class="cell" data-execution_count="22">
<div class="sourceCode cell-code" id="cb36"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb36-1"><a href="#cb36-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Simple TensorFlow example</span></span>
<span id="cb36-2"><a href="#cb36-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> tensorflow <span class="im">as</span> tf</span>
<span id="cb36-3"><a href="#cb36-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-4"><a href="#cb36-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Basic operation</span></span>
<span id="cb36-5"><a href="#cb36-5" aria-hidden="true" tabindex="-1"></a>result <span class="op">=</span> tf.add(<span class="dv">1</span>, <span class="dv">2</span>).numpy()</span>
<span id="cb36-6"><a href="#cb36-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"1 + 2 = </span><span class="sc">{</span>result<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>1 + 2 = 3</code></pre>
</div>
</div>
</section>
<section id="core-concept-tensors" class="level3" data-number="12.6.2">
<h3 data-number="12.6.2" class="anchored" data-anchor-id="core-concept-tensors"><span class="header-section-number">12.6.2</span> Core Concept: Tensors</h3>
<p>The name “TensorFlow” comes from the core data structure it operates on - tensors that flow through a computational graph.</p>
<p><strong>What is a Tensor?</strong> - A tensor is a generalization of vectors and matrices to potentially higher dimensions - Tensors are multi-dimensional arrays with a uniform type (called a <code>dtype</code>) - They are similar to NumPy’s <code>ndarray</code> objects but with additional capabilities, particularly GPU acceleration</p>
<p><strong>Analogy</strong>: If a scalar is a point (0D), a vector is a line (1D), and a matrix is a grid (2D), then tensors are multi-dimensional grids (3D, 4D, etc.). Think of a tensor as a container that can hold data in various shapes - like nesting Russian dolls of different dimensions.</p>
<div class="sourceCode" id="cb38"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb38-1"><a href="#cb38-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Creating tensors in TensorFlow</span></span>
<span id="cb38-2"><a href="#cb38-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> tensorflow <span class="im">as</span> tf</span>
<span id="cb38-3"><a href="#cb38-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb38-4"><a href="#cb38-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-5"><a href="#cb38-5" aria-hidden="true" tabindex="-1"></a><span class="co"># From Python objects</span></span>
<span id="cb38-6"><a href="#cb38-6" aria-hidden="true" tabindex="-1"></a>scalar <span class="op">=</span> tf.constant(<span class="dv">42</span>)</span>
<span id="cb38-7"><a href="#cb38-7" aria-hidden="true" tabindex="-1"></a>vector <span class="op">=</span> tf.constant([<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>])</span>
<span id="cb38-8"><a href="#cb38-8" aria-hidden="true" tabindex="-1"></a>matrix <span class="op">=</span> tf.constant([[<span class="dv">1</span>, <span class="dv">2</span>], [<span class="dv">3</span>, <span class="dv">4</span>]])</span>
<span id="cb38-9"><a href="#cb38-9" aria-hidden="true" tabindex="-1"></a>tensor_3d <span class="op">=</span> tf.constant([[[<span class="dv">1</span>, <span class="dv">2</span>], [<span class="dv">3</span>, <span class="dv">4</span>]], [[<span class="dv">5</span>, <span class="dv">6</span>], [<span class="dv">7</span>, <span class="dv">8</span>]]])</span>
<span id="cb38-10"><a href="#cb38-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-11"><a href="#cb38-11" aria-hidden="true" tabindex="-1"></a><span class="co"># From NumPy arrays</span></span>
<span id="cb38-12"><a href="#cb38-12" aria-hidden="true" tabindex="-1"></a>np_array <span class="op">=</span> np.array([[<span class="dv">1</span>, <span class="dv">2</span>], [<span class="dv">3</span>, <span class="dv">4</span>]])</span>
<span id="cb38-13"><a href="#cb38-13" aria-hidden="true" tabindex="-1"></a>tensor_from_np <span class="op">=</span> tf.constant(np_array)</span>
<span id="cb38-14"><a href="#cb38-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-15"><a href="#cb38-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Check shapes</span></span>
<span id="cb38-16"><a href="#cb38-16" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Scalar shape: </span><span class="sc">{</span>scalar<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb38-17"><a href="#cb38-17" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Vector shape: </span><span class="sc">{</span>vector<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb38-18"><a href="#cb38-18" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Matrix shape: </span><span class="sc">{</span>matrix<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb38-19"><a href="#cb38-19" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"3D Tensor shape: </span><span class="sc">{</span>tensor_3d<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="tensorflows-architecture" class="level3" data-number="12.6.3">
<h3 data-number="12.6.3" class="anchored" data-anchor-id="tensorflows-architecture"><span class="header-section-number">12.6.3</span> TensorFlow’s Architecture</h3>
<p>TensorFlow’s architecture is designed to facilitate both research and production deployment.</p>
<section id="key-architectural-features" class="level4" data-number="12.6.3.1">
<h4 data-number="12.6.3.1" class="anchored" data-anchor-id="key-architectural-features"><span class="header-section-number">12.6.3.1</span> Key Architectural Features:</h4>
<ol type="1">
<li><strong>Computation Graphs</strong>: TensorFlow represents computations as graphs where:
<ul>
<li>Nodes are operations (ops)</li>
<li>Edges are tensors that flow between operations</li>
</ul></li>
<li><strong>Eager Execution</strong>: Modern TensorFlow uses eager execution by default:
<ul>
<li>Operations are evaluated immediately</li>
<li>Makes debugging and development more intuitive</li>
<li>Can be disabled for graph-based execution when needed</li>
</ul></li>
<li><strong>Distribution Strategy</strong>: TensorFlow can distribute computation across:
<ul>
<li>Multiple CPUs</li>
<li>Multiple GPUs</li>
<li>TPUs (Tensor Processing Units)</li>
<li>Clusters of machines</li>
</ul></li>
</ol>
<p><strong>Analogy</strong>: TensorFlow’s architecture is like a factory assembly line. Raw materials (input data) enter the factory, go through various specialized stations (operations) that transform them, and finally emerge as finished products (output predictions). The factory layout (computational graph) determines how everything flows, and TensorFlow’s execution engine acts as the factory manager, coordinating all the stations to run efficiently.</p>
<div class="sourceCode" id="cb39"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb39-1"><a href="#cb39-1" aria-hidden="true" tabindex="-1"></a><span class="co"># TensorFlow execution modes</span></span>
<span id="cb39-2"><a href="#cb39-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> tensorflow <span class="im">as</span> tf</span>
<span id="cb39-3"><a href="#cb39-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-4"><a href="#cb39-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Eager execution (default in TF 2.x)</span></span>
<span id="cb39-5"><a href="#cb39-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Eager execution enabled:"</span>, tf.executing_eagerly())</span>
<span id="cb39-6"><a href="#cb39-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-7"><a href="#cb39-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Basic operations execute immediately</span></span>
<span id="cb39-8"><a href="#cb39-8" aria-hidden="true" tabindex="-1"></a>a <span class="op">=</span> tf.constant([[<span class="dv">1</span>, <span class="dv">2</span>], [<span class="dv">3</span>, <span class="dv">4</span>]])</span>
<span id="cb39-9"><a href="#cb39-9" aria-hidden="true" tabindex="-1"></a>b <span class="op">=</span> tf.constant([[<span class="dv">5</span>, <span class="dv">6</span>], [<span class="dv">7</span>, <span class="dv">8</span>]])</span>
<span id="cb39-10"><a href="#cb39-10" aria-hidden="true" tabindex="-1"></a>c <span class="op">=</span> tf.matmul(a, b)</span>
<span id="cb39-11"><a href="#cb39-11" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Matrix multiplication result:</span><span class="ch">\n</span><span class="st">"</span>, c.numpy())</span>
<span id="cb39-12"><a href="#cb39-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-13"><a href="#cb39-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Graph-based execution (for performance optimization)</span></span>
<span id="cb39-14"><a href="#cb39-14" aria-hidden="true" tabindex="-1"></a><span class="at">@tf.function</span></span>
<span id="cb39-15"><a href="#cb39-15" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> graph_function(x, y):</span>
<span id="cb39-16"><a href="#cb39-16" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> tf.matmul(x, y)</span>
<span id="cb39-17"><a href="#cb39-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-18"><a href="#cb39-18" aria-hidden="true" tabindex="-1"></a><span class="co"># This will be compiled into a graph for faster execution</span></span>
<span id="cb39-19"><a href="#cb39-19" aria-hidden="true" tabindex="-1"></a>result <span class="op">=</span> graph_function(a, b)</span>
<span id="cb39-20"><a href="#cb39-20" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Graph function result:</span><span class="ch">\n</span><span class="st">"</span>, result.numpy())</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
</section>
<section id="tensorflows-python-api" class="level3" data-number="12.6.4">
<h3 data-number="12.6.4" class="anchored" data-anchor-id="tensorflows-python-api"><span class="header-section-number">12.6.4</span> TensorFlow’s Python API</h3>
<p>TensorFlow provides a rich Python API that allows users to build and train models at different levels of abstraction.</p>
<section id="api-hierarchy" class="level4" data-number="12.6.4.1">
<h4 data-number="12.6.4.1" class="anchored" data-anchor-id="api-hierarchy"><span class="header-section-number">12.6.4.1</span> API Hierarchy:</h4>
<ol type="1">
<li><strong>Low-Level API</strong>: Core TensorFlow operations
<ul>
<li>Direct tensor manipulation</li>
<li>Fine-grained control over computation</li>
<li>Used for research and advanced applications</li>
</ul></li>
<li><strong>Mid-Level API</strong>: tf.Module, layers, etc.
<ul>
<li>Building blocks for creating models</li>
<li>Provides more structure than raw tensors</li>
</ul></li>
<li><strong>High-Level API</strong>: tf.keras
<ul>
<li>Complete ML toolkit for building and training models</li>
<li>Easy to use and quick to prototype with</li>
<li>Supports multiple backends (though primarily used with TensorFlow)</li>
</ul></li>
</ol>
<p><strong>Analogy</strong>: TensorFlow’s API hierarchy is like learning to drive. The high-level API (Keras) is like driving an automatic car with cruise control - it handles most details for you. The mid-level API is like driving a manual transmission - more control but requires more knowledge. The low-level API is like building your own car from parts - complete freedom but requires extensive expertise.</p>
<div class="sourceCode" id="cb40"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb40-1"><a href="#cb40-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Different API levels example</span></span>
<span id="cb40-2"><a href="#cb40-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> tensorflow <span class="im">as</span> tf</span>
<span id="cb40-3"><a href="#cb40-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-4"><a href="#cb40-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Low-level API example</span></span>
<span id="cb40-5"><a href="#cb40-5" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> low_level_model(inputs):</span>
<span id="cb40-6"><a href="#cb40-6" aria-hidden="true" tabindex="-1"></a>    weights <span class="op">=</span> tf.Variable(tf.random.normal([<span class="dv">784</span>, <span class="dv">10</span>]))</span>
<span id="cb40-7"><a href="#cb40-7" aria-hidden="true" tabindex="-1"></a>    biases <span class="op">=</span> tf.Variable(tf.zeros([<span class="dv">10</span>]))</span>
<span id="cb40-8"><a href="#cb40-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> tf.matmul(inputs, weights) <span class="op">+</span> biases</span>
<span id="cb40-9"><a href="#cb40-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-10"><a href="#cb40-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Mid-level API using layers</span></span>
<span id="cb40-11"><a href="#cb40-11" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> MidLevelModel(tf.Module):</span>
<span id="cb40-12"><a href="#cb40-12" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>):</span>
<span id="cb40-13"><a href="#cb40-13" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb40-14"><a href="#cb40-14" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.dense <span class="op">=</span> tf.keras.layers.Dense(<span class="dv">10</span>)</span>
<span id="cb40-15"><a href="#cb40-15" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb40-16"><a href="#cb40-16" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__call__</span>(<span class="va">self</span>, inputs):</span>
<span id="cb40-17"><a href="#cb40-17" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.dense(inputs)</span>
<span id="cb40-18"><a href="#cb40-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-19"><a href="#cb40-19" aria-hidden="true" tabindex="-1"></a><span class="co"># High-level API using Keras</span></span>
<span id="cb40-20"><a href="#cb40-20" aria-hidden="true" tabindex="-1"></a>high_level_model <span class="op">=</span> tf.keras.Sequential([</span>
<span id="cb40-21"><a href="#cb40-21" aria-hidden="true" tabindex="-1"></a>    tf.keras.layers.Flatten(input_shape<span class="op">=</span>(<span class="dv">28</span>, <span class="dv">28</span>)),</span>
<span id="cb40-22"><a href="#cb40-22" aria-hidden="true" tabindex="-1"></a>    tf.keras.layers.Dense(<span class="dv">128</span>, activation<span class="op">=</span><span class="st">'relu'</span>),</span>
<span id="cb40-23"><a href="#cb40-23" aria-hidden="true" tabindex="-1"></a>    tf.keras.layers.Dropout(<span class="fl">0.2</span>),</span>
<span id="cb40-24"><a href="#cb40-24" aria-hidden="true" tabindex="-1"></a>    tf.keras.layers.Dense(<span class="dv">10</span>, activation<span class="op">=</span><span class="st">'softmax'</span>)</span>
<span id="cb40-25"><a href="#cb40-25" aria-hidden="true" tabindex="-1"></a>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
</section>
<section id="tensorflow-vs.-numpy" class="level3" data-number="12.6.5">
<h3 data-number="12.6.5" class="anchored" data-anchor-id="tensorflow-vs.-numpy"><span class="header-section-number">12.6.5</span> TensorFlow vs.&nbsp;NumPy</h3>
<p>TensorFlow’s core functionality is similar to NumPy, but with several key advantages:</p>
<ul>
<li><strong>GPU/TPU Support</strong>: Accelerated computation on specialized hardware</li>
<li><strong>Automatic Differentiation</strong>: Automatic calculation of gradients for training</li>
<li><strong>Distributed Computing</strong>: Built-in support for multi-device computation</li>
<li><strong>Graph Optimization</strong>: Computation graphs can be optimized for performance</li>
<li><strong>Deployment Options</strong>: Models can be exported to various formats for deployment</li>
</ul>
<p><strong>Analogy</strong>: NumPy is like a calculator that can perform complex math, while TensorFlow is like a programmable, parallel supercomputer. Both can do similar operations, but TensorFlow is designed to scale those operations across multiple computing devices efficiently.</p>
<div class="sourceCode" id="cb41"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb41-1"><a href="#cb41-1" aria-hidden="true" tabindex="-1"></a><span class="co"># NumPy vs TensorFlow comparison</span></span>
<span id="cb41-2"><a href="#cb41-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb41-3"><a href="#cb41-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> tensorflow <span class="im">as</span> tf</span>
<span id="cb41-4"><a href="#cb41-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> time</span>
<span id="cb41-5"><a href="#cb41-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-6"><a href="#cb41-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Create large matrices</span></span>
<span id="cb41-7"><a href="#cb41-7" aria-hidden="true" tabindex="-1"></a>size <span class="op">=</span> <span class="dv">5000</span></span>
<span id="cb41-8"><a href="#cb41-8" aria-hidden="true" tabindex="-1"></a>np_matrix <span class="op">=</span> np.random.random((size, size))</span>
<span id="cb41-9"><a href="#cb41-9" aria-hidden="true" tabindex="-1"></a>tf_matrix <span class="op">=</span> tf.random.uniform((size, size))</span>
<span id="cb41-10"><a href="#cb41-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-11"><a href="#cb41-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Compare matrix multiplication performance</span></span>
<span id="cb41-12"><a href="#cb41-12" aria-hidden="true" tabindex="-1"></a>start_time <span class="op">=</span> time.time()</span>
<span id="cb41-13"><a href="#cb41-13" aria-hidden="true" tabindex="-1"></a>np_result <span class="op">=</span> np.matmul(np_matrix, np_matrix)</span>
<span id="cb41-14"><a href="#cb41-14" aria-hidden="true" tabindex="-1"></a>np_time <span class="op">=</span> time.time() <span class="op">-</span> start_time</span>
<span id="cb41-15"><a href="#cb41-15" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"NumPy matrix multiplication time: </span><span class="sc">{</span>np_time<span class="sc">:.4f}</span><span class="ss"> seconds"</span>)</span>
<span id="cb41-16"><a href="#cb41-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-17"><a href="#cb41-17" aria-hidden="true" tabindex="-1"></a>start_time <span class="op">=</span> time.time()</span>
<span id="cb41-18"><a href="#cb41-18" aria-hidden="true" tabindex="-1"></a>tf_result <span class="op">=</span> tf.matmul(tf_matrix, tf_matrix)</span>
<span id="cb41-19"><a href="#cb41-19" aria-hidden="true" tabindex="-1"></a><span class="co"># Force execution to complete</span></span>
<span id="cb41-20"><a href="#cb41-20" aria-hidden="true" tabindex="-1"></a>_ <span class="op">=</span> tf_result.numpy()</span>
<span id="cb41-21"><a href="#cb41-21" aria-hidden="true" tabindex="-1"></a>tf_time <span class="op">=</span> time.time() <span class="op">-</span> start_time</span>
<span id="cb41-22"><a href="#cb41-22" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"TensorFlow matrix multiplication time: </span><span class="sc">{</span>tf_time<span class="sc">:.4f}</span><span class="ss"> seconds"</span>)</span>
<span id="cb41-23"><a href="#cb41-23" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Speedup: </span><span class="sc">{</span>np_time<span class="op">/</span>tf_time<span class="sc">:.2f}</span><span class="ss">x"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="computational-graphs-in-tensorflow" class="level3" data-number="12.6.6">
<h3 data-number="12.6.6" class="anchored" data-anchor-id="computational-graphs-in-tensorflow"><span class="header-section-number">12.6.6</span> Computational Graphs in TensorFlow</h3>
<p>The computational graph is a core concept in TensorFlow. It represents a series of TensorFlow operations arranged as nodes in a directed graph.</p>
<section id="key-components-of-computational-graphs" class="level4" data-number="12.6.6.1">
<h4 data-number="12.6.6.1" class="anchored" data-anchor-id="key-components-of-computational-graphs"><span class="header-section-number">12.6.6.1</span> Key Components of Computational Graphs:</h4>
<ol type="1">
<li><strong>Operations (ops)</strong>: Nodes in the graph that perform computations</li>
<li><strong>Tensors</strong>: Edges in the graph that carry data between operations</li>
<li><strong>Variables</strong>: Special operations that maintain state across executions</li>
<li><strong>Placeholders</strong> (TF 1.x): Entry points for feeding data</li>
</ol>
<p><strong>Analogy</strong>: A computational graph is like a recipe. The ingredients are tensors, the preparation steps are operations, cooking tools that remain on your counter are variables, and the serving dishes are outputs. Just as a chef can optimize a recipe by changing the order of steps or using different tools, TensorFlow can optimize a computational graph for efficiency.</p>
<div class="sourceCode" id="cb42"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb42-1"><a href="#cb42-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Simple computational graph example</span></span>
<span id="cb42-2"><a href="#cb42-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> tensorflow <span class="im">as</span> tf</span>
<span id="cb42-3"><a href="#cb42-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-4"><a href="#cb42-4" aria-hidden="true" tabindex="-1"></a><span class="co"># In TF 2.x, we can use tf.function to create graphs</span></span>
<span id="cb42-5"><a href="#cb42-5" aria-hidden="true" tabindex="-1"></a><span class="at">@tf.function</span></span>
<span id="cb42-6"><a href="#cb42-6" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> simple_graph(x):</span>
<span id="cb42-7"><a href="#cb42-7" aria-hidden="true" tabindex="-1"></a>    <span class="co"># MatMul operation</span></span>
<span id="cb42-8"><a href="#cb42-8" aria-hidden="true" tabindex="-1"></a>    W <span class="op">=</span> tf.Variable(tf.random.uniform((<span class="dv">784</span>, <span class="dv">100</span>), <span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>))</span>
<span id="cb42-9"><a href="#cb42-9" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Add operation</span></span>
<span id="cb42-10"><a href="#cb42-10" aria-hidden="true" tabindex="-1"></a>    b <span class="op">=</span> tf.Variable(tf.zeros(<span class="dv">100</span>))</span>
<span id="cb42-11"><a href="#cb42-11" aria-hidden="true" tabindex="-1"></a>    <span class="co"># ReLU operation</span></span>
<span id="cb42-12"><a href="#cb42-12" aria-hidden="true" tabindex="-1"></a>    h <span class="op">=</span> tf.nn.relu(tf.matmul(x, W) <span class="op">+</span> b)</span>
<span id="cb42-13"><a href="#cb42-13" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> h</span>
<span id="cb42-14"><a href="#cb42-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-15"><a href="#cb42-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Create input</span></span>
<span id="cb42-16"><a href="#cb42-16" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> tf.random.normal((<span class="dv">1</span>, <span class="dv">784</span>))</span>
<span id="cb42-17"><a href="#cb42-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-18"><a href="#cb42-18" aria-hidden="true" tabindex="-1"></a><span class="co"># Execute the graph</span></span>
<span id="cb42-19"><a href="#cb42-19" aria-hidden="true" tabindex="-1"></a>result <span class="op">=</span> simple_graph(x)</span>
<span id="cb42-20"><a href="#cb42-20" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Result shape: </span><span class="sc">{</span>result<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb42-21"><a href="#cb42-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-22"><a href="#cb42-22" aria-hidden="true" tabindex="-1"></a><span class="co"># We can visualize the graph using TensorBoard</span></span>
<span id="cb42-23"><a href="#cb42-23" aria-hidden="true" tabindex="-1"></a><span class="co"># (Code for visualization would go here)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
</section>
<section id="how-tensorflow-works" class="level3" data-number="12.6.7">
<h3 data-number="12.6.7" class="anchored" data-anchor-id="how-tensorflow-works"><span class="header-section-number">12.6.7</span> How TensorFlow Works</h3>
<p>TensorFlow operates by building and executing computational graphs. Here’s a simplified view of how it works:</p>
<ol type="1">
<li><strong>Define the computational graph</strong>:
<ul>
<li>Create tensors, variables, and operations</li>
<li>Connect operations to form a graph</li>
</ul></li>
<li><strong>Execute the graph</strong>:
<ul>
<li>In TF 2.x, with eager execution, operations are executed immediately</li>
<li>With <code>@tf.function</code>, Python code is converted to a graph for faster execution</li>
</ul></li>
<li><strong>Optimize parameters</strong>:
<ul>
<li>Use automatic differentiation to compute gradients</li>
<li>Update variables with optimizers</li>
</ul></li>
<li><strong>Deploy the model</strong>:
<ul>
<li>Export the graph for inference</li>
<li>Serve the model in production</li>
</ul></li>
</ol>
<p><strong>Analogy</strong>: The process is like designing, building, and operating a factory. First, you design the factory layout (define the graph). Then you build the machinery (initialize variables). Next, you start production (execute the graph). Finally, you optimize the production line based on feedback (training with gradients).</p>
<div class="sourceCode" id="cb43"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb43-1"><a href="#cb43-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Complete TensorFlow workflow example</span></span>
<span id="cb43-2"><a href="#cb43-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> tensorflow <span class="im">as</span> tf</span>
<span id="cb43-3"><a href="#cb43-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-4"><a href="#cb43-4" aria-hidden="true" tabindex="-1"></a><span class="co"># 1. Create a dataset</span></span>
<span id="cb43-5"><a href="#cb43-5" aria-hidden="true" tabindex="-1"></a>mnist <span class="op">=</span> tf.keras.datasets.mnist</span>
<span id="cb43-6"><a href="#cb43-6" aria-hidden="true" tabindex="-1"></a>(x_train, y_train), (x_test, y_test) <span class="op">=</span> mnist.load_data()</span>
<span id="cb43-7"><a href="#cb43-7" aria-hidden="true" tabindex="-1"></a>x_train, x_test <span class="op">=</span> x_train <span class="op">/</span> <span class="fl">255.0</span>, x_test <span class="op">/</span> <span class="fl">255.0</span>  <span class="co"># Normalize data</span></span>
<span id="cb43-8"><a href="#cb43-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-9"><a href="#cb43-9" aria-hidden="true" tabindex="-1"></a><span class="co"># 2. Define the model</span></span>
<span id="cb43-10"><a href="#cb43-10" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> tf.keras.models.Sequential([</span>
<span id="cb43-11"><a href="#cb43-11" aria-hidden="true" tabindex="-1"></a>  tf.keras.layers.Flatten(input_shape<span class="op">=</span>(<span class="dv">28</span>, <span class="dv">28</span>)),</span>
<span id="cb43-12"><a href="#cb43-12" aria-hidden="true" tabindex="-1"></a>  tf.keras.layers.Dense(<span class="dv">128</span>, activation<span class="op">=</span><span class="st">'relu'</span>),</span>
<span id="cb43-13"><a href="#cb43-13" aria-hidden="true" tabindex="-1"></a>  tf.keras.layers.Dropout(<span class="fl">0.2</span>),</span>
<span id="cb43-14"><a href="#cb43-14" aria-hidden="true" tabindex="-1"></a>  tf.keras.layers.Dense(<span class="dv">10</span>)</span>
<span id="cb43-15"><a href="#cb43-15" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb43-16"><a href="#cb43-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-17"><a href="#cb43-17" aria-hidden="true" tabindex="-1"></a><span class="co"># 3. Define loss function</span></span>
<span id="cb43-18"><a href="#cb43-18" aria-hidden="true" tabindex="-1"></a>loss_fn <span class="op">=</span> tf.keras.losses.SparseCategoricalCrossentropy(from_logits<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb43-19"><a href="#cb43-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-20"><a href="#cb43-20" aria-hidden="true" tabindex="-1"></a><span class="co"># 4. Compile the model</span></span>
<span id="cb43-21"><a href="#cb43-21" aria-hidden="true" tabindex="-1"></a>model.<span class="bu">compile</span>(optimizer<span class="op">=</span><span class="st">'adam'</span>,</span>
<span id="cb43-22"><a href="#cb43-22" aria-hidden="true" tabindex="-1"></a>              loss<span class="op">=</span>loss_fn,</span>
<span id="cb43-23"><a href="#cb43-23" aria-hidden="true" tabindex="-1"></a>              metrics<span class="op">=</span>[<span class="st">'accuracy'</span>])</span>
<span id="cb43-24"><a href="#cb43-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-25"><a href="#cb43-25" aria-hidden="true" tabindex="-1"></a><span class="co"># 5. Train the model</span></span>
<span id="cb43-26"><a href="#cb43-26" aria-hidden="true" tabindex="-1"></a>model.fit(x_train, y_train, epochs<span class="op">=</span><span class="dv">5</span>)</span>
<span id="cb43-27"><a href="#cb43-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-28"><a href="#cb43-28" aria-hidden="true" tabindex="-1"></a><span class="co"># 6. Evaluate the model</span></span>
<span id="cb43-29"><a href="#cb43-29" aria-hidden="true" tabindex="-1"></a>model.evaluate(x_test, y_test, verbose<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb43-30"><a href="#cb43-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-31"><a href="#cb43-31" aria-hidden="true" tabindex="-1"></a><span class="co"># 7. Save the model for deployment</span></span>
<span id="cb43-32"><a href="#cb43-32" aria-hidden="true" tabindex="-1"></a>model.save(<span class="st">'my_mnist_model'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="tensorflows-execution-framework" class="level3" data-number="12.6.8">
<h3 data-number="12.6.8" class="anchored" data-anchor-id="tensorflows-execution-framework"><span class="header-section-number">12.6.8</span> TensorFlow’s Execution Framework</h3>
<p>TensorFlow’s execution framework is designed to efficiently run computations across different devices and platforms.</p>
<section id="key-components" class="level4" data-number="12.6.8.1">
<h4 data-number="12.6.8.1" class="anchored" data-anchor-id="key-components"><span class="header-section-number">12.6.8.1</span> Key Components:</h4>
<ol type="1">
<li><strong>Device Placement</strong>: Intelligently assigns operations to available devices</li>
<li><strong>Memory Management</strong>: Efficiently allocates and frees memory</li>
<li><strong>Kernel Selection</strong>: Chooses the best implementation for each operation</li>
<li><strong>Parallel Execution</strong>: Runs independent operations in parallel</li>
<li><strong>Distributed Strategy</strong>: Coordinates computation across multiple devices</li>
</ol>
<p><strong>Analogy</strong>: TensorFlow’s execution framework is like an orchestra conductor. Just as a conductor coordinates different sections of musicians to play in harmony, TensorFlow coordinates different computing devices to process data efficiently. The conductor knows which instruments (devices) are best suited for different parts of the music (operations) and ensures they all work together seamlessly.</p>
<div class="sourceCode" id="cb44"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb44-1"><a href="#cb44-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Device placement example</span></span>
<span id="cb44-2"><a href="#cb44-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> tensorflow <span class="im">as</span> tf</span>
<span id="cb44-3"><a href="#cb44-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-4"><a href="#cb44-4" aria-hidden="true" tabindex="-1"></a><span class="co"># List available devices</span></span>
<span id="cb44-5"><a href="#cb44-5" aria-hidden="true" tabindex="-1"></a>physical_devices <span class="op">=</span> tf.config.list_physical_devices()</span>
<span id="cb44-6"><a href="#cb44-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Available devices:"</span>)</span>
<span id="cb44-7"><a href="#cb44-7" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> device <span class="kw">in</span> physical_devices:</span>
<span id="cb44-8"><a href="#cb44-8" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"  </span><span class="sc">{</span>device<span class="sc">.</span>name<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb44-9"><a href="#cb44-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-10"><a href="#cb44-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Explicit device placement</span></span>
<span id="cb44-11"><a href="#cb44-11" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> tf.device(<span class="st">'/CPU:0'</span>):</span>
<span id="cb44-12"><a href="#cb44-12" aria-hidden="true" tabindex="-1"></a>    cpu_tensor <span class="op">=</span> tf.random.normal([<span class="dv">1000</span>, <span class="dv">1000</span>])</span>
<span id="cb44-13"><a href="#cb44-13" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"CPU tensor shape: </span><span class="sc">{</span>cpu_tensor<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb44-14"><a href="#cb44-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-15"><a href="#cb44-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Use GPU if available</span></span>
<span id="cb44-16"><a href="#cb44-16" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> <span class="bu">len</span>(tf.config.list_physical_devices(<span class="st">'GPU'</span>)) <span class="op">&gt;</span> <span class="dv">0</span>:</span>
<span id="cb44-17"><a href="#cb44-17" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> tf.device(<span class="st">'/GPU:0'</span>):</span>
<span id="cb44-18"><a href="#cb44-18" aria-hidden="true" tabindex="-1"></a>        gpu_tensor <span class="op">=</span> tf.random.normal([<span class="dv">1000</span>, <span class="dv">1000</span>])</span>
<span id="cb44-19"><a href="#cb44-19" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"GPU tensor shape: </span><span class="sc">{</span>gpu_tensor<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb44-20"><a href="#cb44-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-21"><a href="#cb44-21" aria-hidden="true" tabindex="-1"></a><span class="co"># Automatic device placement</span></span>
<span id="cb44-22"><a href="#cb44-22" aria-hidden="true" tabindex="-1"></a>tensor <span class="op">=</span> tf.random.normal([<span class="dv">1000</span>, <span class="dv">1000</span>])</span>
<span id="cb44-23"><a href="#cb44-23" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Automatically placed tensor device: </span><span class="sc">{</span>tensor<span class="sc">.</span>device<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
</section>
<section id="installing-and-setting-up-tensorflow" class="level3" data-number="12.6.9">
<h3 data-number="12.6.9" class="anchored" data-anchor-id="installing-and-setting-up-tensorflow"><span class="header-section-number">12.6.9</span> Installing and Setting Up TensorFlow</h3>
<p>TensorFlow can be installed using pip, conda, or Docker.</p>
<section id="basic-installation" class="level4" data-number="12.6.9.1">
<h4 data-number="12.6.9.1" class="anchored" data-anchor-id="basic-installation"><span class="header-section-number">12.6.9.1</span> Basic Installation:</h4>
<div class="sourceCode" id="cb45"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb45-1"><a href="#cb45-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Standard TensorFlow installation (includes GPU support if CUDA is available)</span></span>
<span id="cb45-2"><a href="#cb45-2" aria-hidden="true" tabindex="-1"></a><span class="ex">pip</span> install tensorflow</span>
<span id="cb45-3"><a href="#cb45-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb45-4"><a href="#cb45-4" aria-hidden="true" tabindex="-1"></a><span class="co"># CPU-only installation (smaller package size)</span></span>
<span id="cb45-5"><a href="#cb45-5" aria-hidden="true" tabindex="-1"></a><span class="ex">pip</span> install tensorflow-cpu</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="hardware-considerations" class="level4" data-number="12.6.9.2">
<h4 data-number="12.6.9.2" class="anchored" data-anchor-id="hardware-considerations"><span class="header-section-number">12.6.9.2</span> Hardware Considerations:</h4>
<ul>
<li><strong>CPU</strong>: Works on all machines but slower for training large models</li>
<li><strong>GPU</strong>: Dramatically speeds up computation (requires NVIDIA GPU with CUDA)</li>
<li><strong>TPU</strong>: Specialized hardware for deep learning (available through Google Cloud)</li>
</ul>
<div class="sourceCode" id="cb46"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb46-1"><a href="#cb46-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Checking TensorFlow installation</span></span>
<span id="cb46-2"><a href="#cb46-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> tensorflow <span class="im">as</span> tf</span>
<span id="cb46-3"><a href="#cb46-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-4"><a href="#cb46-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Print TensorFlow version</span></span>
<span id="cb46-5"><a href="#cb46-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"TensorFlow version: </span><span class="sc">{</span>tf<span class="sc">.</span>__version__<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb46-6"><a href="#cb46-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-7"><a href="#cb46-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Check if TensorFlow can access GPU</span></span>
<span id="cb46-8"><a href="#cb46-8" aria-hidden="true" tabindex="-1"></a>gpus <span class="op">=</span> tf.config.list_physical_devices(<span class="st">'GPU'</span>)</span>
<span id="cb46-9"><a href="#cb46-9" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> gpus:</span>
<span id="cb46-10"><a href="#cb46-10" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"GPU(s) available: </span><span class="sc">{</span><span class="bu">len</span>(gpus)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb46-11"><a href="#cb46-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> gpu <span class="kw">in</span> gpus:</span>
<span id="cb46-12"><a href="#cb46-12" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"  </span><span class="sc">{</span>gpu<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb46-13"><a href="#cb46-13" aria-hidden="true" tabindex="-1"></a><span class="cf">else</span>:</span>
<span id="cb46-14"><a href="#cb46-14" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"No GPU found. Running on CPU."</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
</section>
<section id="tensorflow-ecosystem" class="level3" data-number="12.6.10">
<h3 data-number="12.6.10" class="anchored" data-anchor-id="tensorflow-ecosystem"><span class="header-section-number">12.6.10</span> TensorFlow Ecosystem</h3>
<p>TensorFlow is more than just a core library - it’s an ecosystem of tools and libraries:</p>
<ul>
<li><strong>TensorFlow Extended (TFX)</strong>: End-to-end platform for deploying production ML pipelines</li>
<li><strong>TensorFlow.js</strong>: Run models in the browser or Node.js</li>
<li><strong>TensorFlow Lite</strong>: Deploy models on mobile and edge devices</li>
<li><strong>TensorFlow Hub</strong>: Repository of pre-trained models</li>
<li><strong>TensorBoard</strong>: Visualization tool for model training</li>
<li><strong>TensorFlow Probability</strong>: Statistical computation and probabilistic modeling</li>
<li><strong>TensorFlow Datasets</strong>: Collection of ready-to-use datasets</li>
</ul>
<p><strong>Analogy</strong>: The TensorFlow ecosystem is like a comprehensive workshop for builders. The core TensorFlow library is your basic toolset, but around it, you have specialized equipment (TFX, TF.js, TF Lite) for specific tasks, materials storage (TF Hub, TF Datasets), and measurement tools (TensorBoard) to ensure everything is working correctly.</p>
<div class="sourceCode" id="cb47"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb47-1"><a href="#cb47-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Example of using TensorBoard</span></span>
<span id="cb47-2"><a href="#cb47-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> tensorflow <span class="im">as</span> tf</span>
<span id="cb47-3"><a href="#cb47-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> datetime</span>
<span id="cb47-4"><a href="#cb47-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-5"><a href="#cb47-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a dataset</span></span>
<span id="cb47-6"><a href="#cb47-6" aria-hidden="true" tabindex="-1"></a>mnist <span class="op">=</span> tf.keras.datasets.mnist</span>
<span id="cb47-7"><a href="#cb47-7" aria-hidden="true" tabindex="-1"></a>(x_train, y_train), (x_test, y_test) <span class="op">=</span> mnist.load_data()</span>
<span id="cb47-8"><a href="#cb47-8" aria-hidden="true" tabindex="-1"></a>x_train, x_test <span class="op">=</span> x_train <span class="op">/</span> <span class="fl">255.0</span>, x_test <span class="op">/</span> <span class="fl">255.0</span></span>
<span id="cb47-9"><a href="#cb47-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-10"><a href="#cb47-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a simple model</span></span>
<span id="cb47-11"><a href="#cb47-11" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> tf.keras.models.Sequential([</span>
<span id="cb47-12"><a href="#cb47-12" aria-hidden="true" tabindex="-1"></a>  tf.keras.layers.Flatten(input_shape<span class="op">=</span>(<span class="dv">28</span>, <span class="dv">28</span>)),</span>
<span id="cb47-13"><a href="#cb47-13" aria-hidden="true" tabindex="-1"></a>  tf.keras.layers.Dense(<span class="dv">512</span>, activation<span class="op">=</span><span class="st">'relu'</span>),</span>
<span id="cb47-14"><a href="#cb47-14" aria-hidden="true" tabindex="-1"></a>  tf.keras.layers.Dropout(<span class="fl">0.2</span>),</span>
<span id="cb47-15"><a href="#cb47-15" aria-hidden="true" tabindex="-1"></a>  tf.keras.layers.Dense(<span class="dv">10</span>, activation<span class="op">=</span><span class="st">'softmax'</span>)</span>
<span id="cb47-16"><a href="#cb47-16" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb47-17"><a href="#cb47-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-18"><a href="#cb47-18" aria-hidden="true" tabindex="-1"></a>model.<span class="bu">compile</span>(optimizer<span class="op">=</span><span class="st">'adam'</span>,</span>
<span id="cb47-19"><a href="#cb47-19" aria-hidden="true" tabindex="-1"></a>              loss<span class="op">=</span><span class="st">'sparse_categorical_crossentropy'</span>,</span>
<span id="cb47-20"><a href="#cb47-20" aria-hidden="true" tabindex="-1"></a>              metrics<span class="op">=</span>[<span class="st">'accuracy'</span>])</span>
<span id="cb47-21"><a href="#cb47-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-22"><a href="#cb47-22" aria-hidden="true" tabindex="-1"></a><span class="co"># Set up TensorBoard logging</span></span>
<span id="cb47-23"><a href="#cb47-23" aria-hidden="true" tabindex="-1"></a>log_dir <span class="op">=</span> <span class="st">"logs/fit/"</span> <span class="op">+</span> datetime.datetime.now().strftime(<span class="st">"%Y%m</span><span class="sc">%d</span><span class="st">-%H%M%S"</span>)</span>
<span id="cb47-24"><a href="#cb47-24" aria-hidden="true" tabindex="-1"></a>tensorboard_callback <span class="op">=</span> tf.keras.callbacks.TensorBoard(log_dir<span class="op">=</span>log_dir, histogram_freq<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb47-25"><a href="#cb47-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-26"><a href="#cb47-26" aria-hidden="true" tabindex="-1"></a><span class="co"># Train with TensorBoard</span></span>
<span id="cb47-27"><a href="#cb47-27" aria-hidden="true" tabindex="-1"></a>model.fit(x<span class="op">=</span>x_train, </span>
<span id="cb47-28"><a href="#cb47-28" aria-hidden="true" tabindex="-1"></a>          y<span class="op">=</span>y_train, </span>
<span id="cb47-29"><a href="#cb47-29" aria-hidden="true" tabindex="-1"></a>          epochs<span class="op">=</span><span class="dv">5</span>, </span>
<span id="cb47-30"><a href="#cb47-30" aria-hidden="true" tabindex="-1"></a>          validation_data<span class="op">=</span>(x_test, y_test), </span>
<span id="cb47-31"><a href="#cb47-31" aria-hidden="true" tabindex="-1"></a>          callbacks<span class="op">=</span>[tensorboard_callback])</span>
<span id="cb47-32"><a href="#cb47-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-33"><a href="#cb47-33" aria-hidden="true" tabindex="-1"></a><span class="co"># To view TensorBoard, run: tensorboard --logdir logs/fit</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="tensorflow-vs.-other-frameworks" class="level3" data-number="12.6.11">
<h3 data-number="12.6.11" class="anchored" data-anchor-id="tensorflow-vs.-other-frameworks"><span class="header-section-number">12.6.11</span> TensorFlow vs.&nbsp;Other Frameworks</h3>
<p>TensorFlow is one of several popular deep learning frameworks, each with its strengths:</p>
<section id="tensorflow-vs.-pytorch" class="level4" data-number="12.6.11.1">
<h4 data-number="12.6.11.1" class="anchored" data-anchor-id="tensorflow-vs.-pytorch"><span class="header-section-number">12.6.11.1</span> TensorFlow vs.&nbsp;PyTorch:</h4>
<ul>
<li><strong>TensorFlow</strong>: Production-ready, comprehensive ecosystem, static computation graphs (with tf.function)</li>
<li><strong>PyTorch</strong>: Dynamic computation graphs, pythonic design, popular in research</li>
</ul>
</section>
<section id="tensorflow-vs.-keras" class="level4" data-number="12.6.11.2">
<h4 data-number="12.6.11.2" class="anchored" data-anchor-id="tensorflow-vs.-keras"><span class="header-section-number">12.6.11.2</span> TensorFlow vs.&nbsp;Keras:</h4>
<ul>
<li><strong>TensorFlow</strong>: Complete ML platform with low-level and high-level APIs</li>
<li><strong>Keras</strong>: High-level API (now integrated into TensorFlow as tf.keras)</li>
</ul>
</section>
<section id="tensorflow-vs.-jax" class="level4" data-number="12.6.11.3">
<h4 data-number="12.6.11.3" class="anchored" data-anchor-id="tensorflow-vs.-jax"><span class="header-section-number">12.6.11.3</span> TensorFlow vs.&nbsp;JAX:</h4>
<ul>
<li><strong>TensorFlow</strong>: Full-featured ML platform with deployment options</li>
<li><strong>JAX</strong>: Focuses on high-performance numerical computing and research</li>
</ul>
<p><strong>Analogy</strong>: Different ML frameworks are like different programming languages. TensorFlow is like Java - comprehensive, production-ready, with a large ecosystem. PyTorch is like Python - flexible, intuitive, great for experimentation. Keras (now part of TensorFlow) is like Ruby - designed for developer happiness and rapid prototyping.</p>
</section>
</section>
<section id="conclusion-3" class="level3" data-number="12.6.12">
<h3 data-number="12.6.12" class="anchored" data-anchor-id="conclusion-3"><span class="header-section-number">12.6.12</span> Conclusion</h3>
<p>TensorFlow is a powerful and flexible platform for machine learning that provides tools for every step of the machine learning workflow, from data preprocessing to model deployment. Its architecture is designed to scale from research to production, and its ecosystem provides solutions for various deployment scenarios.</p>
<p>As you continue your TensorFlow journey, remember that the key concepts of tensors, computational graphs, and automatic differentiation form the foundation of how TensorFlow works. Building on these concepts will allow you to create increasingly sophisticated machine learning models.</p>
</section>
<section id="additional-resources" class="level3" data-number="12.6.13">
<h3 data-number="12.6.13" class="anchored" data-anchor-id="additional-resources"><span class="header-section-number">12.6.13</span> Additional Resources</h3>
<ul>
<li><a href="https://www.tensorflow.org/">TensorFlow Official Website</a></li>
<li><a href="https://www.tensorflow.org/tutorials">TensorFlow Tutorials</a></li>
<li><a href="https://www.tensorflow.org/guide">TensorFlow Guide</a></li>
<li><a href="https://www.tensorflow.org/api_docs">TensorFlow API Reference</a></li>
<li><a href="https://github.com/tensorflow/tensorflow">TensorFlow GitHub Repository</a></li>
</ul>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./chapter9.html" class="pagination-link" aria-label="Neural Networks and Deep Learning Foundations">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Neural Networks and Deep Learning Foundations</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./summary.html" class="pagination-link" aria-label="Summary">
        <span class="nav-page-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Summary</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->




</body></html>