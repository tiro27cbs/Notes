<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.42">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>11&nbsp; Neural Networks and Deep Learning Foundations – Machine Learning and Deep Learning Notes</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./chapter10.html" rel="next">
<link href="./chapter8.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-2f5df379a58b258e96c21c0638c20c03.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-8da5b4427184b79ecddefad3d342027e.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./chapter9.html"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Neural Networks and Deep Learning Foundations</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Machine Learning and Deep Learning Notes</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">🚀 Introduction</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Fundamentals of Machine Learning</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Building an End-to-End Machine Learning Pipeline</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter3.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Unsupervised Learning: Clustering Techniques</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter4.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Supervised Learning: Regression and Classification</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter5.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Dimensionality Reduction Methods</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter6.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Tree-Based Models and Ensemble Learning</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter6v2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Support Vector Machines and Model Evaluation</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter7.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Outlier Detection and Recommendation Systems</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter8.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Gradient Descent: Optimization in Machine Learning</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter9.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Neural Networks and Deep Learning Foundations</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter10.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Batch Normalization, RNN, Distributed Deep Learning and Tensorflow</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter11.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Variants of Recurrent Neural Networks (RNNs): LSTM, GRU, BiLSTM</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter12.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Autoencoders</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter13.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Convolutional Neural Networks</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter14.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Adversarial Examples and Generative Models: A Deep Dive into Robustness and Synthetic Data Generation</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter15.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">Hyper-Parameter Optimization (HPO) in Machine Learning</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./summary.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">Summary</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#ml-and-deep-learning-history" id="toc-ml-and-deep-learning-history" class="nav-link active" data-scroll-target="#ml-and-deep-learning-history"><span class="header-section-number">11.1</span> ML and Deep Learning History</a>
  <ul class="collapse">
  <li><a href="#s-1960s-early-developments" id="toc-s-1960s-early-developments" class="nav-link" data-scroll-target="#s-1960s-early-developments"><span class="header-section-number">11.1.1</span> 1950s-1960s: Early Developments</a></li>
  <li><a href="#s-1980s-rise-of-connectionism" id="toc-s-1980s-rise-of-connectionism" class="nav-link" data-scroll-target="#s-1980s-rise-of-connectionism"><span class="header-section-number">11.1.2</span> 1970s-1980s: Rise of Connectionism</a></li>
  <li><a href="#s2000s-decline-rise-of-alternatives" id="toc-s2000s-decline-rise-of-alternatives" class="nav-link" data-scroll-target="#s2000s-decline-rise-of-alternatives"><span class="header-section-number">11.1.3</span> 1990s–2000s: Decline &amp; Rise of Alternatives</a></li>
  </ul></li>
  <li><a href="#overview-of-machine-learning" id="toc-overview-of-machine-learning" class="nav-link" data-scroll-target="#overview-of-machine-learning"><span class="header-section-number">11.2</span> Overview of Machine Learning</a>
  <ul class="collapse">
  <li><a href="#core-components" id="toc-core-components" class="nav-link" data-scroll-target="#core-components"><span class="header-section-number">11.2.1</span> Core Components</a></li>
  <li><a href="#key-concepts-overfitting-and-underfitting" id="toc-key-concepts-overfitting-and-underfitting" class="nav-link" data-scroll-target="#key-concepts-overfitting-and-underfitting"><span class="header-section-number">11.2.2</span> Key Concepts: Overfitting and Underfitting</a></li>
  </ul></li>
  <li><a href="#neural-networks-from-biological-inspiration-to-deep-learning" id="toc-neural-networks-from-biological-inspiration-to-deep-learning" class="nav-link" data-scroll-target="#neural-networks-from-biological-inspiration-to-deep-learning"><span class="header-section-number">11.3</span> Neural Networks: From Biological Inspiration to Deep Learning</a>
  <ul class="collapse">
  <li><a href="#why-are-they-called-neural-networks" id="toc-why-are-they-called-neural-networks" class="nav-link" data-scroll-target="#why-are-they-called-neural-networks"><span class="header-section-number">11.3.1</span> Why Are They Called Neural Networks?</a></li>
  </ul></li>
  <li><a href="#linearity-and-non-linearity-in-neural-networks" id="toc-linearity-and-non-linearity-in-neural-networks" class="nav-link" data-scroll-target="#linearity-and-non-linearity-in-neural-networks"><span class="header-section-number">11.4</span> Linearity and Non-Linearity in Neural Networks</a>
  <ul class="collapse">
  <li><a href="#the-concept-of-linearity" id="toc-the-concept-of-linearity" class="nav-link" data-scroll-target="#the-concept-of-linearity"><span class="header-section-number">11.4.1</span> The Concept of Linearity</a></li>
  <li><a href="#why-we-need-non-linearity" id="toc-why-we-need-non-linearity" class="nav-link" data-scroll-target="#why-we-need-non-linearity"><span class="header-section-number">11.4.2</span> Why We Need Non-Linearity</a></li>
  <li><a href="#non-linearity-in-neural-networks" id="toc-non-linearity-in-neural-networks" class="nav-link" data-scroll-target="#non-linearity-in-neural-networks"><span class="header-section-number">11.4.3</span> Non-Linearity in Neural Networks</a></li>
  </ul></li>
  <li><a href="#activation-functions-adding-non-linearity" id="toc-activation-functions-adding-non-linearity" class="nav-link" data-scroll-target="#activation-functions-adding-non-linearity"><span class="header-section-number">11.5</span> Activation Functions: Adding Non-Linearity</a></li>
  <li><a href="#activation-functions-understanding-the-neural-networks-decision-making-process" id="toc-activation-functions-understanding-the-neural-networks-decision-making-process" class="nav-link" data-scroll-target="#activation-functions-understanding-the-neural-networks-decision-making-process"><span class="header-section-number">11.6</span> Activation Functions: Understanding the Neural Network’s Decision-Making Process</a>
  <ul class="collapse">
  <li><a href="#the-conceptual-role-of-activation-functions" id="toc-the-conceptual-role-of-activation-functions" class="nav-link" data-scroll-target="#the-conceptual-role-of-activation-functions"><span class="header-section-number">11.6.1</span> The Conceptual Role of Activation Functions</a></li>
  <li><a href="#sigmoid-function-the-s-shaped-decision-maker" id="toc-sigmoid-function-the-s-shaped-decision-maker" class="nav-link" data-scroll-target="#sigmoid-function-the-s-shaped-decision-maker"><span class="header-section-number">11.6.2</span> Sigmoid Function: The S-Shaped Decision Maker</a></li>
  <li><a href="#relu-the-modern-workhorse" id="toc-relu-the-modern-workhorse" class="nav-link" data-scroll-target="#relu-the-modern-workhorse"><span class="header-section-number">11.6.3</span> ReLU: The Modern Workhorse</a></li>
  <li><a href="#variants-leaky-relu-elu-and-gelu" id="toc-variants-leaky-relu-elu-and-gelu" class="nav-link" data-scroll-target="#variants-leaky-relu-elu-and-gelu"><span class="header-section-number">11.6.4</span> Variants: Leaky ReLU, ELU, and GELU</a></li>
  <li><a href="#softmax-the-probability-distributor" id="toc-softmax-the-probability-distributor" class="nav-link" data-scroll-target="#softmax-the-probability-distributor"><span class="header-section-number">11.6.5</span> Softmax: The Probability Distributor</a></li>
  <li><a href="#choosing-the-right-activation-function" id="toc-choosing-the-right-activation-function" class="nav-link" data-scroll-target="#choosing-the-right-activation-function"><span class="header-section-number">11.6.6</span> Choosing the Right Activation Function</a></li>
  <li><a href="#the-mathematics-behind-learning" id="toc-the-mathematics-behind-learning" class="nav-link" data-scroll-target="#the-mathematics-behind-learning"><span class="header-section-number">11.6.7</span> The Mathematics Behind Learning</a></li>
  <li><a href="#visualizing-decision-boundaries" id="toc-visualizing-decision-boundaries" class="nav-link" data-scroll-target="#visualizing-decision-boundaries"><span class="header-section-number">11.6.8</span> Visualizing Decision Boundaries</a></li>
  </ul></li>
  <li><a href="#the-role-of-bias-in-neural-networks" id="toc-the-role-of-bias-in-neural-networks" class="nav-link" data-scroll-target="#the-role-of-bias-in-neural-networks"><span class="header-section-number">11.7</span> The Role of Bias in Neural Networks</a>
  <ul class="collapse">
  <li><a href="#understanding-bias-in-the-statistical-sense" id="toc-understanding-bias-in-the-statistical-sense" class="nav-link" data-scroll-target="#understanding-bias-in-the-statistical-sense"><span class="header-section-number">11.7.1</span> Understanding Bias in the Statistical Sense</a></li>
  <li><a href="#bias-nodes-in-neural-networks" id="toc-bias-nodes-in-neural-networks" class="nav-link" data-scroll-target="#bias-nodes-in-neural-networks"><span class="header-section-number">11.7.2</span> Bias Nodes in Neural Networks</a></li>
  <li><a href="#practical-implementation-in-python" id="toc-practical-implementation-in-python" class="nav-link" data-scroll-target="#practical-implementation-in-python"><span class="header-section-number">11.7.3</span> Practical Implementation in Python</a></li>
  </ul></li>
  <li><a href="#putting-it-all-together-building-a-neural-network" id="toc-putting-it-all-together-building-a-neural-network" class="nav-link" data-scroll-target="#putting-it-all-together-building-a-neural-network"><span class="header-section-number">11.8</span> Putting It All Together: Building a Neural Network</a></li>
  <li><a href="#connection-to-deep-learning" id="toc-connection-to-deep-learning" class="nav-link" data-scroll-target="#connection-to-deep-learning"><span class="header-section-number">11.9</span> Connection to Deep Learning</a></li>
  <li><a href="#real-world-analogy-building-a-house" id="toc-real-world-analogy-building-a-house" class="nav-link" data-scroll-target="#real-world-analogy-building-a-house"><span class="header-section-number">11.10</span> Real-World Analogy: Building a House</a></li>
  <li><a href="#conclusion" id="toc-conclusion" class="nav-link" data-scroll-target="#conclusion"><span class="header-section-number">11.11</span> Conclusion</a></li>
  <li><a href="#neural-networks-foundations-tlus-perceptrons-and-mlps" id="toc-neural-networks-foundations-tlus-perceptrons-and-mlps" class="nav-link" data-scroll-target="#neural-networks-foundations-tlus-perceptrons-and-mlps"><span class="header-section-number">12</span> Neural Networks Foundations: TLUs, Perceptrons, and MLPs</a>
  <ul class="collapse">
  <li><a href="#threshold-logic-units-tlus" id="toc-threshold-logic-units-tlus" class="nav-link" data-scroll-target="#threshold-logic-units-tlus"><span class="header-section-number">12.1</span> Threshold Logic Units (TLUs)</a>
  <ul class="collapse">
  <li><a href="#the-building-blocks-of-neural-networks" id="toc-the-building-blocks-of-neural-networks" class="nav-link" data-scroll-target="#the-building-blocks-of-neural-networks"><span class="header-section-number">12.1.1</span> The Building Blocks of Neural Networks</a></li>
  <li><a href="#how-tlus-work" id="toc-how-tlus-work" class="nav-link" data-scroll-target="#how-tlus-work"><span class="header-section-number">12.1.2</span> How TLUs Work</a></li>
  <li><a href="#analogy-the-voting-committee" id="toc-analogy-the-voting-committee" class="nav-link" data-scroll-target="#analogy-the-voting-committee"><span class="header-section-number">12.1.3</span> Analogy: The Voting Committee</a></li>
  <li><a href="#tlus-as-logic-gates" id="toc-tlus-as-logic-gates" class="nav-link" data-scroll-target="#tlus-as-logic-gates"><span class="header-section-number">12.1.4</span> TLUs as Logic Gates</a></li>
  <li><a href="#python-implementation-of-a-tlu" id="toc-python-implementation-of-a-tlu" class="nav-link" data-scroll-target="#python-implementation-of-a-tlu"><span class="header-section-number">12.1.5</span> Python Implementation of a TLU</a></li>
  </ul></li>
  <li><a href="#perceptron" id="toc-perceptron" class="nav-link" data-scroll-target="#perceptron"><span class="header-section-number">12.2</span> Perceptron</a>
  <ul class="collapse">
  <li><a href="#the-first-learning-neural-network" id="toc-the-first-learning-neural-network" class="nav-link" data-scroll-target="#the-first-learning-neural-network"><span class="header-section-number">12.2.1</span> The First Learning Neural Network</a></li>
  <li><a href="#structure-of-a-perceptron" id="toc-structure-of-a-perceptron" class="nav-link" data-scroll-target="#structure-of-a-perceptron"><span class="header-section-number">12.2.2</span> Structure of a Perceptron</a></li>
  <li><a href="#analogy-learning-to-balance-a-scale" id="toc-analogy-learning-to-balance-a-scale" class="nav-link" data-scroll-target="#analogy-learning-to-balance-a-scale"><span class="header-section-number">12.2.3</span> Analogy: Learning to Balance a Scale</a></li>
  <li><a href="#binary-classification-with-perceptrons" id="toc-binary-classification-with-perceptrons" class="nav-link" data-scroll-target="#binary-classification-with-perceptrons"><span class="header-section-number">12.2.4</span> Binary Classification with Perceptrons</a></li>
  <li><a href="#multi-output-perceptrons" id="toc-multi-output-perceptrons" class="nav-link" data-scroll-target="#multi-output-perceptrons"><span class="header-section-number">12.2.5</span> Multi-Output Perceptrons</a></li>
  <li><a href="#activation-functions" id="toc-activation-functions" class="nav-link" data-scroll-target="#activation-functions"><span class="header-section-number">12.2.6</span> Activation Functions</a></li>
  <li><a href="#the-perceptron-learning-algorithm" id="toc-the-perceptron-learning-algorithm" class="nav-link" data-scroll-target="#the-perceptron-learning-algorithm"><span class="header-section-number">12.2.7</span> The Perceptron Learning Algorithm</a></li>
  <li><a href="#python-implementation-of-a-perceptron" id="toc-python-implementation-of-a-perceptron" class="nav-link" data-scroll-target="#python-implementation-of-a-perceptron"><span class="header-section-number">12.2.8</span> Python Implementation of a Perceptron</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#generate-a-simple-dataset" id="toc-generate-a-simple-dataset" class="nav-link" data-scroll-target="#generate-a-simple-dataset"><span class="header-section-number">13</span> Generate a simple dataset</a>
  <ul class="collapse">
  <li><a href="#comparison-with-logistic-regression" id="toc-comparison-with-logistic-regression" class="nav-link" data-scroll-target="#comparison-with-logistic-regression"><span class="header-section-number">13.0.1</span> Comparison with Logistic Regression</a></li>
  <li><a href="#limitations-of-perceptrons" id="toc-limitations-of-perceptrons" class="nav-link" data-scroll-target="#limitations-of-perceptrons"><span class="header-section-number">13.0.2</span> Limitations of Perceptrons</a></li>
  <li><a href="#multilayer-perceptron-mlp" id="toc-multilayer-perceptron-mlp" class="nav-link" data-scroll-target="#multilayer-perceptron-mlp"><span class="header-section-number">13.1</span> Multilayer Perceptron (MLP)</a>
  <ul class="collapse">
  <li><a href="#beyond-simple-linear-boundaries" id="toc-beyond-simple-linear-boundaries" class="nav-link" data-scroll-target="#beyond-simple-linear-boundaries"><span class="header-section-number">13.1.1</span> Beyond Simple Linear Boundaries</a></li>
  <li><a href="#structure-of-an-mlp" id="toc-structure-of-an-mlp" class="nav-link" data-scroll-target="#structure-of-an-mlp"><span class="header-section-number">13.1.2</span> Structure of an MLP</a></li>
  <li><a href="#analogy-hierarchical-decision-making" id="toc-analogy-hierarchical-decision-making" class="nav-link" data-scroll-target="#analogy-hierarchical-decision-making"><span class="header-section-number">13.1.3</span> Analogy: Hierarchical Decision Making</a></li>
  <li><a href="#why-mlps-can-solve-xor" id="toc-why-mlps-can-solve-xor" class="nav-link" data-scroll-target="#why-mlps-can-solve-xor"><span class="header-section-number">13.1.4</span> Why MLPs Can Solve XOR</a></li>
  <li><a href="#the-need-for-non-linear-activation-functions" id="toc-the-need-for-non-linear-activation-functions" class="nav-link" data-scroll-target="#the-need-for-non-linear-activation-functions"><span class="header-section-number">13.1.5</span> The Need for Non-Linear Activation Functions</a></li>
  <li><a href="#training-mlps-with-backpropagation" id="toc-training-mlps-with-backpropagation" class="nav-link" data-scroll-target="#training-mlps-with-backpropagation"><span class="header-section-number">13.1.6</span> Training MLPs with Backpropagation</a></li>
  <li><a href="#python-implementation-of-an-mlp" id="toc-python-implementation-of-an-mlp" class="nav-link" data-scroll-target="#python-implementation-of-an-mlp"><span class="header-section-number">13.1.7</span> Python Implementation of an MLP</a></li>
  <li><a href="#using-mlps-with-scikit-learn" id="toc-using-mlps-with-scikit-learn" class="nav-link" data-scroll-target="#using-mlps-with-scikit-learn"><span class="header-section-number">13.1.8</span> Using MLPs with Scikit-Learn</a></li>
  <li><a href="#mlp-applications" id="toc-mlp-applications" class="nav-link" data-scroll-target="#mlp-applications"><span class="header-section-number">13.1.9</span> MLP Applications</a></li>
  <li><a href="#advantages-and-disadvantages-of-mlps" id="toc-advantages-and-disadvantages-of-mlps" class="nav-link" data-scroll-target="#advantages-and-disadvantages-of-mlps"><span class="header-section-number">13.1.10</span> Advantages and Disadvantages of MLPs</a></li>
  </ul></li>
  <li><a href="#summary-from-tlus-to-deep-learning" id="toc-summary-from-tlus-to-deep-learning" class="nav-link" data-scroll-target="#summary-from-tlus-to-deep-learning"><span class="header-section-number">13.2</span> Summary: From TLUs to Deep Learning</a></li>
  <li><a href="#feed-forward-neural-networks" id="toc-feed-forward-neural-networks" class="nav-link" data-scroll-target="#feed-forward-neural-networks"><span class="header-section-number">13.3</span> Feed-Forward Neural Networks</a>
  <ul class="collapse">
  <li><a href="#key-characteristics" id="toc-key-characteristics" class="nav-link" data-scroll-target="#key-characteristics"><span class="header-section-number">13.3.1</span> Key Characteristics</a></li>
  <li><a href="#understanding-feed-forward-networks-the-assembly-line-analogy" id="toc-understanding-feed-forward-networks-the-assembly-line-analogy" class="nav-link" data-scroll-target="#understanding-feed-forward-networks-the-assembly-line-analogy"><span class="header-section-number">13.3.2</span> Understanding Feed-Forward Networks: The Assembly Line Analogy</a></li>
  <li><a href="#mathematical-representation" id="toc-mathematical-representation" class="nav-link" data-scroll-target="#mathematical-representation"><span class="header-section-number">13.3.3</span> Mathematical Representation</a></li>
  <li><a href="#python-implementation-of-feed-forward-network" id="toc-python-implementation-of-feed-forward-network" class="nav-link" data-scroll-target="#python-implementation-of-feed-forward-network"><span class="header-section-number">13.3.4</span> Python Implementation of Feed-Forward Network</a></li>
  <li><a href="#example-binary-classification-with-feed-forward-network" id="toc-example-binary-classification-with-feed-forward-network" class="nav-link" data-scroll-target="#example-binary-classification-with-feed-forward-network"><span class="header-section-number">13.3.5</span> Example: Binary Classification with Feed-Forward Network</a></li>
  </ul></li>
  <li><a href="#backpropagation" id="toc-backpropagation" class="nav-link" data-scroll-target="#backpropagation"><span class="header-section-number">13.4</span> Backpropagation</a>
  <ul class="collapse">
  <li><a href="#the-learning-process-backpropagation-explained" id="toc-the-learning-process-backpropagation-explained" class="nav-link" data-scroll-target="#the-learning-process-backpropagation-explained"><span class="header-section-number">13.4.1</span> The Learning Process: Backpropagation Explained</a></li>
  <li><a href="#the-mail-delivery-analogy" id="toc-the-mail-delivery-analogy" class="nav-link" data-scroll-target="#the-mail-delivery-analogy"><span class="header-section-number">13.4.2</span> The Mail Delivery Analogy</a></li>
  <li><a href="#mathematical-framework" id="toc-mathematical-framework" class="nav-link" data-scroll-target="#mathematical-framework"><span class="header-section-number">13.4.3</span> Mathematical Framework</a></li>
  <li><a href="#implementing-backpropagation" id="toc-implementing-backpropagation" class="nav-link" data-scroll-target="#implementing-backpropagation"><span class="header-section-number">13.4.4</span> Implementing Backpropagation</a></li>
  <li><a href="#complete-example-training-a-network-with-backpropagation" id="toc-complete-example-training-a-network-with-backpropagation" class="nav-link" data-scroll-target="#complete-example-training-a-network-with-backpropagation"><span class="header-section-number">13.4.5</span> Complete Example: Training a Network with Backpropagation</a></li>
  </ul></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Neural Networks and Deep Learning Foundations</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<section id="ml-and-deep-learning-history" class="level2" data-number="11.1">
<h2 data-number="11.1" class="anchored" data-anchor-id="ml-and-deep-learning-history"><span class="header-section-number">11.1</span> ML and Deep Learning History</h2>
<section id="s-1960s-early-developments" class="level3" data-number="11.1.1">
<h3 data-number="11.1.1" class="anchored" data-anchor-id="s-1960s-early-developments"><span class="header-section-number">11.1.1</span> 1950s-1960s: Early Developments</h3>
<p>Initial excitement about machine learning began in the 1950s.</p>
<p>Perceptron was one of the first models: a linear classifier, trained using a method similar to stochastic gradient descent.</p>
<p>Limitation: Perceptrons cannot solve non-linearly separable problems (e.g., XOR), which led to a decline in popularity.</p>
<p>📌 Analogy: Think of a perceptron like a straight knife trying to slice a circular cake with internal patterns—it can’t reach the inner parts effectively.</p>
<div id="cd1911a9" class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Simple Perceptron Example in Python</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> np.array([[<span class="dv">0</span>, <span class="dv">0</span>], [<span class="dv">0</span>, <span class="dv">1</span>], [<span class="dv">1</span>, <span class="dv">0</span>], [<span class="dv">1</span>, <span class="dv">1</span>]])</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> np.array([<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">1</span>])  <span class="co"># AND operation</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>weights <span class="op">=</span> np.zeros(<span class="dv">2</span>)</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>bias <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>lr <span class="op">=</span> <span class="fl">0.1</span></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">10</span>):</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> xi, target <span class="kw">in</span> <span class="bu">zip</span>(X, y):</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>        output <span class="op">=</span> np.dot(xi, weights) <span class="op">+</span> bias</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>        prediction <span class="op">=</span> <span class="dv">1</span> <span class="cf">if</span> output <span class="op">&gt;</span> <span class="dv">0</span> <span class="cf">else</span> <span class="dv">0</span></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>        error <span class="op">=</span> target <span class="op">-</span> prediction</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>        weights <span class="op">+=</span> lr <span class="op">*</span> error <span class="op">*</span> xi</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>        bias <span class="op">+=</span> lr <span class="op">*</span> error</span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Trained weights:"</span>, weights)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Trained weights: [0.2 0.1]</code></pre>
</div>
</div>
</section>
<section id="s-1980s-rise-of-connectionism" class="level3" data-number="11.1.2">
<h3 data-number="11.1.2" class="anchored" data-anchor-id="s-1980s-rise-of-connectionism"><span class="header-section-number">11.1.2</span> 1970s-1980s: Rise of Connectionism</h3>
<p>Interest shifted to connectionism — models inspired by how the brain works.</p>
<p>Hidden layers were introduced, increasing model expressiveness.</p>
<p>Achieved success in tasks like optical character recognition.</p>
<p>📌 Analogy: Adding hidden layers is like giving a child a set of crayons instead of just a pencil—it allows more flexibility and richer representations.</p>
</section>
<section id="s2000s-decline-rise-of-alternatives" class="level3" data-number="11.1.3">
<h3 data-number="11.1.3" class="anchored" data-anchor-id="s2000s-decline-rise-of-alternatives"><span class="header-section-number">11.1.3</span> 1990s–2000s: Decline &amp; Rise of Alternatives</h3>
<p>Interest declined again due to slow training and lack of data.</p>
<p>Logistic Regression and SVMs with regularization and kernel tricks became dominant.</p>
</section>
</section>
<section id="overview-of-machine-learning" class="level2" data-number="11.2">
<h2 data-number="11.2" class="anchored" data-anchor-id="overview-of-machine-learning"><span class="header-section-number">11.2</span> Overview of Machine Learning</h2>
<section id="core-components" class="level3" data-number="11.2.1">
<h3 data-number="11.2.1" class="anchored" data-anchor-id="core-components"><span class="header-section-number">11.2.1</span> Core Components</h3>
<p><strong>Data</strong>: The foundation of learning. Inputs (features) and Outputs (labels/targets).</p>
<p>Represented using a design matrix: rows are examples, columns are features.</p>
<div id="c872ba4e" class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Design matrix representation</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> pd.DataFrame({</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>    <span class="st">'feature1'</span>: [<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>],</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>    <span class="st">'feature2'</span>: [<span class="dv">4</span>, <span class="dv">5</span>, <span class="dv">6</span>],</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>    <span class="st">'output'</span>: [<span class="dv">7</span>, <span class="dv">8</span>, <span class="dv">9</span>]</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>})</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(df)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>   feature1  feature2  output
0         1         4       7
1         2         5       8
2         3         6       9</code></pre>
</div>
</div>
<p><strong>Model</strong>: Maps input to output.</p>
<ul>
<li>Parametric (e.g., Linear Regression): fixed number of parameters.</li>
<li>Non-parametric (e.g., k-NN): model complexity grows with data.</li>
</ul>
<p><strong>Objective Function</strong>: Measures how well the model is performing.</p>
<ul>
<li>Mean Squared Error (MSE) for regression.</li>
<li>Cross-Entropy Loss for classification.</li>
<li>Often derived from Maximum Likelihood Estimation (MLE).</li>
</ul>
<div id="7a4d4a3a" class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> mean_squared_error, log_loss</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>y_true <span class="op">=</span> [<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">0</span>]</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>y_pred_prob <span class="op">=</span> [<span class="fl">0.1</span>, <span class="fl">0.9</span>, <span class="fl">0.8</span>, <span class="fl">0.3</span>]</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Cross-entropy loss:"</span>, log_loss(y_true, y_pred_prob))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Cross-entropy loss: 0.19763488164214868</code></pre>
</div>
</div>
<p><strong>Optimization Algorithm</strong>: Minimizes the objective function.</p>
<p>Gradient Descent: Move in the direction opposite to the gradient.</p>
<div id="16d9f0a3" class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Simple Gradient Descent</span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> f(x): <span class="cf">return</span> (x <span class="op">-</span> <span class="dv">3</span>) <span class="op">**</span> <span class="dv">2</span></span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> grad_f(x): <span class="cf">return</span> <span class="dv">2</span> <span class="op">*</span> (x <span class="op">-</span> <span class="dv">3</span>)</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>lr <span class="op">=</span> <span class="fl">0.1</span></span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">20</span>):</span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a>    x <span class="op">-=</span> lr <span class="op">*</span> grad_f(x)</span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Minimum at:"</span>, x)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Minimum at: 2.9654123548617948</code></pre>
</div>
</div>
<p>📌 Analogy: Optimization is like hiking down a mountain by always stepping downhill (gradient direction).</p>
</section>
<section id="key-concepts-overfitting-and-underfitting" class="level3" data-number="11.2.2">
<h3 data-number="11.2.2" class="anchored" data-anchor-id="key-concepts-overfitting-and-underfitting"><span class="header-section-number">11.2.2</span> Key Concepts: Overfitting and Underfitting</h3>
<p><strong>Underfitting</strong>: Model is too simple and fails to capture the underlying pattern in data.</p>
<p><strong>Overfitting</strong>: Model is too complex and memorizes the training data, performing poorly on unseen data.</p>
<p>The goal is to balance complexity to minimize both training and test error.</p>
</section>
</section>
<section id="neural-networks-from-biological-inspiration-to-deep-learning" class="level2" data-number="11.3">
<h2 data-number="11.3" class="anchored" data-anchor-id="neural-networks-from-biological-inspiration-to-deep-learning"><span class="header-section-number">11.3</span> Neural Networks: From Biological Inspiration to Deep Learning</h2>
<section id="why-are-they-called-neural-networks" class="level3" data-number="11.3.1">
<h3 data-number="11.3.1" class="anchored" data-anchor-id="why-are-they-called-neural-networks"><span class="header-section-number">11.3.1</span> Why Are They Called Neural Networks?</h3>
<p>Neural networks draw inspiration from the structure and function of the human brain, hence the name. Let’s explore this analogy:</p>
<p><strong>Biological Neurons</strong>: In our brains, neurons receive signals through dendrites, process them in the cell body, and transmit output signals through axons to other neurons.</p>
<p><strong>Artificial Neurons</strong>: In artificial neural networks (ANNs), we have mathematical “neurons” that:</p>
<ol type="1">
<li>Receive inputs (like dendrites receiving signals)</li>
<li>Apply weights to these inputs (representing synaptic strengths)</li>
<li>Sum the weighted inputs and apply an activation function (like the neuron firing)</li>
<li>Produce an output that connects to other neurons</li>
</ol>
<div id="046c8d9f" class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Simple artificial neuron</span></span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> neuron(inputs, weights, bias, activation_function):</span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Calculate weighted sum of inputs (like dendrites and cell body function)</span></span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a>    weighted_sum <span class="op">=</span> np.dot(inputs, weights) <span class="op">+</span> bias</span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Apply activation function (like neuron firing)</span></span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a>    output <span class="op">=</span> activation_function(weighted_sum)</span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> output</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>While actual brains are infinitely more complex, this simplified model has proven remarkably effective for machine learning tasks.</p>
</section>
</section>
<section id="linearity-and-non-linearity-in-neural-networks" class="level2" data-number="11.4">
<h2 data-number="11.4" class="anchored" data-anchor-id="linearity-and-non-linearity-in-neural-networks"><span class="header-section-number">11.4</span> Linearity and Non-Linearity in Neural Networks</h2>
<section id="the-concept-of-linearity" class="level3" data-number="11.4.1">
<h3 data-number="11.4.1" class="anchored" data-anchor-id="the-concept-of-linearity"><span class="header-section-number">11.4.1</span> The Concept of Linearity</h3>
<p>A linear function has two key properties: 1. Additivity: <span class="math inline">\(f(x + y) = f(x) + f(y)\)</span> 2. Homogeneity: <span class="math inline">\(f(αx) = αf(x)\)</span></p>
<p>In neural networks, a linear transformation of inputs can be represented as:</p>
<div id="21eff685" class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> linear_layer(x, W, b):</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Linear transformation: z = Wx + b</span></span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a>    z <span class="op">=</span> np.dot(W, x) <span class="op">+</span> b</span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> z</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="why-we-need-non-linearity" class="level3" data-number="11.4.2">
<h3 data-number="11.4.2" class="anchored" data-anchor-id="why-we-need-non-linearity"><span class="header-section-number">11.4.2</span> Why We Need Non-Linearity</h3>
<p>If we were to stack multiple linear layers together:</p>
<div id="bd1b1253" class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Example of stacking linear layers</span></span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Sample data</span></span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> np.array([<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">4</span>])</span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a><span class="co"># First layer parameters</span></span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"></a>W1 <span class="op">=</span> np.array([[<span class="fl">0.1</span>, <span class="fl">0.2</span>, <span class="fl">0.3</span>, <span class="fl">0.4</span>], </span>
<span id="cb11-10"><a href="#cb11-10" aria-hidden="true" tabindex="-1"></a>               [<span class="fl">0.5</span>, <span class="fl">0.6</span>, <span class="fl">0.7</span>, <span class="fl">0.8</span>]])</span>
<span id="cb11-11"><a href="#cb11-11" aria-hidden="true" tabindex="-1"></a>b1 <span class="op">=</span> np.array([<span class="fl">0.1</span>, <span class="fl">0.2</span>])</span>
<span id="cb11-12"><a href="#cb11-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-13"><a href="#cb11-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Second layer parameters</span></span>
<span id="cb11-14"><a href="#cb11-14" aria-hidden="true" tabindex="-1"></a>W2 <span class="op">=</span> np.array([[<span class="fl">0.3</span>, <span class="fl">0.5</span>], </span>
<span id="cb11-15"><a href="#cb11-15" aria-hidden="true" tabindex="-1"></a>               [<span class="fl">0.7</span>, <span class="fl">0.9</span>]])</span>
<span id="cb11-16"><a href="#cb11-16" aria-hidden="true" tabindex="-1"></a>b2 <span class="op">=</span> np.array([<span class="fl">0.2</span>, <span class="fl">0.4</span>])</span>
<span id="cb11-17"><a href="#cb11-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-18"><a href="#cb11-18" aria-hidden="true" tabindex="-1"></a><span class="co"># First linear layer</span></span>
<span id="cb11-19"><a href="#cb11-19" aria-hidden="true" tabindex="-1"></a>z1 <span class="op">=</span> np.dot(W1, x) <span class="op">+</span> b1</span>
<span id="cb11-20"><a href="#cb11-20" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Output of first layer:"</span>, z1)</span>
<span id="cb11-21"><a href="#cb11-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-22"><a href="#cb11-22" aria-hidden="true" tabindex="-1"></a><span class="co"># Second linear layer</span></span>
<span id="cb11-23"><a href="#cb11-23" aria-hidden="true" tabindex="-1"></a>z2 <span class="op">=</span> np.dot(W2, z1) <span class="op">+</span> b2</span>
<span id="cb11-24"><a href="#cb11-24" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Output of second layer:"</span>, z2)</span>
<span id="cb11-25"><a href="#cb11-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-26"><a href="#cb11-26" aria-hidden="true" tabindex="-1"></a><span class="co"># Equivalent single layer</span></span>
<span id="cb11-27"><a href="#cb11-27" aria-hidden="true" tabindex="-1"></a>W_combined <span class="op">=</span> np.dot(W2, W1)</span>
<span id="cb11-28"><a href="#cb11-28" aria-hidden="true" tabindex="-1"></a>b_combined <span class="op">=</span> np.dot(W2, b1) <span class="op">+</span> b2</span>
<span id="cb11-29"><a href="#cb11-29" aria-hidden="true" tabindex="-1"></a>z_combined <span class="op">=</span> np.dot(W_combined, x) <span class="op">+</span> b_combined</span>
<span id="cb11-30"><a href="#cb11-30" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Output of combined layer:"</span>, z_combined)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Output of first layer: [3.1 7.2]
Output of second layer: [4.73 9.05]
Output of combined layer: [4.73 9.05]</code></pre>
</div>
</div>
<p>Mathematically, this is equivalent to a single linear transformation: <span class="math inline">\(z2 = W2(W1x + b1) + b2 = (W2W1)x + (W2b1 + b2)\)</span></p>
<p>This means that without non-linearity, a deep neural network would be no more powerful than a single layer! This is where non-linear activation functions become crucial.</p>
</section>
<section id="non-linearity-in-neural-networks" class="level3" data-number="11.4.3">
<h3 data-number="11.4.3" class="anchored" data-anchor-id="non-linearity-in-neural-networks"><span class="header-section-number">11.4.3</span> Non-Linearity in Neural Networks</h3>
<p>Non-linear functions enable neural networks to learn complex patterns:</p>
<div id="6501cc1b" class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> non_linear_layer(x, W, b, activation_function):</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Linear transformation</span></span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a>    z <span class="op">=</span> np.dot(W, x) <span class="op">+</span> b</span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Non-linear activation</span></span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a>    a <span class="op">=</span> activation_function(z)</span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> a</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>This non-linearity allows neural networks to: - Approximate any continuous function (universal approximation theorem) - Learn hierarchical features - Solve complex, non-linear problems like image recognition, natural language processing, etc.</p>
</section>
</section>
<section id="activation-functions-adding-non-linearity" class="level2" data-number="11.5">
<h2 data-number="11.5" class="anchored" data-anchor-id="activation-functions-adding-non-linearity"><span class="header-section-number">11.5</span> Activation Functions: Adding Non-Linearity</h2>
</section>
<section id="activation-functions-understanding-the-neural-networks-decision-making-process" class="level2" data-number="11.6">
<h2 data-number="11.6" class="anchored" data-anchor-id="activation-functions-understanding-the-neural-networks-decision-making-process"><span class="header-section-number">11.6</span> Activation Functions: Understanding the Neural Network’s Decision-Making Process</h2>
<p>Activation functions are the critical components that give neural networks their power to learn complex patterns. Without them, neural networks would be nothing more than linear regression models, unable to solve interesting problems.</p>
<section id="the-conceptual-role-of-activation-functions" class="level3" data-number="11.6.1">
<h3 data-number="11.6.1" class="anchored" data-anchor-id="the-conceptual-role-of-activation-functions"><span class="header-section-number">11.6.1</span> The Conceptual Role of Activation Functions</h3>
<p>Activation functions serve several crucial purposes:</p>
<ol type="1">
<li><p><strong>Introducing Non-Linearity</strong>: The real world is rarely linear. Activation functions allow networks to model complex, non-linear relationships.</p></li>
<li><p><strong>Feature Transformation</strong>: They transform input signals into more useful representations.</p></li>
<li><p><strong>Decision Boundaries</strong>: They help create complex decision boundaries that separate different classes of data.</p></li>
<li><p><strong>Biological Inspiration</strong>: They mimic the “firing” behavior of biological neurons, which activate only when inputs reach certain thresholds.</p></li>
</ol>
</section>
<section id="sigmoid-function-the-s-shaped-decision-maker" class="level3" data-number="11.6.2">
<h3 data-number="11.6.2" class="anchored" data-anchor-id="sigmoid-function-the-s-shaped-decision-maker"><span class="header-section-number">11.6.2</span> Sigmoid Function: The S-Shaped Decision Maker</h3>
<p>The sigmoid function maps any input to a value between 0 and 1, creating a smooth S-shaped curve:</p>
<div id="52cba772" class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> sigmoid(z):</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="dv">1</span> <span class="op">/</span> (<span class="dv">1</span> <span class="op">+</span> np.exp(<span class="op">-</span>z))</span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualization</span></span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a>z <span class="op">=</span> np.linspace(<span class="op">-</span><span class="dv">10</span>, <span class="dv">10</span>, <span class="dv">100</span>)</span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a>plt.plot(z, sigmoid(z))</span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Sigmoid Activation Function"</span>)</span>
<span id="cb14-10"><a href="#cb14-10" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"z"</span>)</span>
<span id="cb14-11"><a href="#cb14-11" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"sigmoid(z)"</span>)</span>
<span id="cb14-12"><a href="#cb14-12" aria-hidden="true" tabindex="-1"></a>plt.grid(<span class="va">True</span>)</span>
<span id="cb14-13"><a href="#cb14-13" aria-hidden="true" tabindex="-1"></a>plt.axhline(y<span class="op">=</span><span class="dv">0</span>, color<span class="op">=</span><span class="st">'k'</span>, linestyle<span class="op">=</span><span class="st">'-'</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb14-14"><a href="#cb14-14" aria-hidden="true" tabindex="-1"></a>plt.axhline(y<span class="op">=</span><span class="dv">1</span>, color<span class="op">=</span><span class="st">'k'</span>, linestyle<span class="op">=</span><span class="st">'-'</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb14-15"><a href="#cb14-15" aria-hidden="true" tabindex="-1"></a>plt.axvline(x<span class="op">=</span><span class="dv">0</span>, color<span class="op">=</span><span class="st">'k'</span>, linestyle<span class="op">=</span><span class="st">'-'</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="chapter9_files/figure-html/cell-10-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p><strong>Conceptual Understanding</strong>: - Acts like a “probability estimator” - values close to 1 represent high confidence, values close to 0 represent low confidence. - Creates a smooth transition between inactive (0) and active (1) states. - Historically important but less used in hidden layers today.</p>
<p><strong>Mental Model</strong>: Imagine a thermostat that gradually turns on heating as the temperature drops, with a smooth transition rather than an abrupt on/off switch.</p>
<p><strong>Mathematical Intuition</strong>: The sigmoid compresses extreme values while being most sensitive to changes around z=0.</p>
</section>
<section id="relu-the-modern-workhorse" class="level3" data-number="11.6.3">
<h3 data-number="11.6.3" class="anchored" data-anchor-id="relu-the-modern-workhorse"><span class="header-section-number">11.6.3</span> ReLU: The Modern Workhorse</h3>
<p>ReLU (Rectified Linear Unit) is elegantly simple: it returns the input if positive, otherwise zero.</p>
<div id="6c742a5f" class="cell" data-execution_count="10">
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> relu(z):</span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.maximum(<span class="dv">0</span>, z)  </span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualization</span></span>
<span id="cb15-8"><a href="#cb15-8" aria-hidden="true" tabindex="-1"></a>z <span class="op">=</span> np.linspace(<span class="op">-</span><span class="dv">10</span>, <span class="dv">10</span>, <span class="dv">100</span>)</span>
<span id="cb15-9"><a href="#cb15-9" aria-hidden="true" tabindex="-1"></a>plt.plot(z, relu(z))</span>
<span id="cb15-10"><a href="#cb15-10" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"ReLU Activation Function"</span>)</span>
<span id="cb15-11"><a href="#cb15-11" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"z"</span>)</span>
<span id="cb15-12"><a href="#cb15-12" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"ReLU(z)"</span>)</span>
<span id="cb15-13"><a href="#cb15-13" aria-hidden="true" tabindex="-1"></a>plt.grid(<span class="va">True</span>)</span>
<span id="cb15-14"><a href="#cb15-14" aria-hidden="true" tabindex="-1"></a>plt.axhline(y<span class="op">=</span><span class="dv">0</span>, color<span class="op">=</span><span class="st">'k'</span>, linestyle<span class="op">=</span><span class="st">'-'</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb15-15"><a href="#cb15-15" aria-hidden="true" tabindex="-1"></a>plt.axvline(x<span class="op">=</span><span class="dv">0</span>, color<span class="op">=</span><span class="st">'k'</span>, linestyle<span class="op">=</span><span class="st">'-'</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb15-16"><a href="#cb15-16" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="chapter9_files/figure-html/cell-11-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p><strong>Conceptual Understanding</strong>: - Acts as a feature selector - it highlights useful signals (positive values) while suppressing noise (negative values). - Creates sparsity in the network, as many neurons will output zero for any given input. - Computationally efficient and helps signals flow more effectively through deep networks.</p>
<p><strong>Mental Model</strong>: Think of ReLU as a filter that only allows positive information to pass through, completely blocking negative information.</p>
<p><strong>Biological Intuition</strong>: Similar to biological neurons that fire only when stimulation exceeds a threshold, ReLU neurons are only “activated” by positive inputs.</p>
</section>
<section id="variants-leaky-relu-elu-and-gelu" class="level3" data-number="11.6.4">
<h3 data-number="11.6.4" class="anchored" data-anchor-id="variants-leaky-relu-elu-and-gelu"><span class="header-section-number">11.6.4</span> Variants: Leaky ReLU, ELU, and GELU</h3>
<p>These ReLU variants address some of its limitations:</p>
<p><strong>Leaky ReLU</strong>: Allows a small gradient when the unit is not active:</p>
<div id="40b93074" class="cell" data-execution_count="11">
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> leaky_relu(z, alpha<span class="op">=</span><span class="fl">0.01</span>):</span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.maximum(alpha <span class="op">*</span> z, z)</span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualization</span></span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a>z <span class="op">=</span> np.linspace(<span class="op">-</span><span class="dv">10</span>, <span class="dv">10</span>, <span class="dv">100</span>)</span>
<span id="cb16-6"><a href="#cb16-6" aria-hidden="true" tabindex="-1"></a>plt.plot(z, leaky_relu(z))</span>
<span id="cb16-7"><a href="#cb16-7" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Leaky ReLU Activation Function"</span>)</span>
<span id="cb16-8"><a href="#cb16-8" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"z"</span>)</span>
<span id="cb16-9"><a href="#cb16-9" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"Leaky ReLU(z)"</span>)</span>
<span id="cb16-10"><a href="#cb16-10" aria-hidden="true" tabindex="-1"></a>plt.grid(<span class="va">True</span>)</span>
<span id="cb16-11"><a href="#cb16-11" aria-hidden="true" tabindex="-1"></a>plt.axhline(y<span class="op">=</span><span class="dv">0</span>, color<span class="op">=</span><span class="st">'k'</span>, linestyle<span class="op">=</span><span class="st">'-'</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb16-12"><a href="#cb16-12" aria-hidden="true" tabindex="-1"></a>plt.axvline(x<span class="op">=</span><span class="dv">0</span>, color<span class="op">=</span><span class="st">'k'</span>, linestyle<span class="op">=</span><span class="st">'-'</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb16-13"><a href="#cb16-13" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="chapter9_files/figure-html/cell-12-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p><strong>Conceptual Understanding</strong>: Gives “dying” neurons a chance to recover by allowing a small gradient when inactive.</p>
<p><strong>ELU</strong> (Exponential Linear Unit): Smoothes the transition at zero:</p>
<div id="ebe5e0f3" class="cell" data-execution_count="12">
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> elu(z, alpha<span class="op">=</span><span class="fl">1.0</span>):</span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Use NumPy's where function for element-wise conditional operations</span></span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.where(z <span class="op">&gt;</span> <span class="dv">0</span>, z, alpha <span class="op">*</span> (np.exp(z) <span class="op">-</span> <span class="dv">1</span>))</span>
<span id="cb17-7"><a href="#cb17-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-8"><a href="#cb17-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualization</span></span>
<span id="cb17-9"><a href="#cb17-9" aria-hidden="true" tabindex="-1"></a>z <span class="op">=</span> np.linspace(<span class="op">-</span><span class="dv">10</span>, <span class="dv">10</span>, <span class="dv">100</span>)</span>
<span id="cb17-10"><a href="#cb17-10" aria-hidden="true" tabindex="-1"></a>plt.plot(z, elu(z))</span>
<span id="cb17-11"><a href="#cb17-11" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"ELU Activation Function"</span>)</span>
<span id="cb17-12"><a href="#cb17-12" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"z"</span>)</span>
<span id="cb17-13"><a href="#cb17-13" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"ELU(z)"</span>)</span>
<span id="cb17-14"><a href="#cb17-14" aria-hidden="true" tabindex="-1"></a>plt.grid(<span class="va">True</span>)</span>
<span id="cb17-15"><a href="#cb17-15" aria-hidden="true" tabindex="-1"></a>plt.axhline(y<span class="op">=</span><span class="dv">0</span>, color<span class="op">=</span><span class="st">'k'</span>, linestyle<span class="op">=</span><span class="st">'-'</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb17-16"><a href="#cb17-16" aria-hidden="true" tabindex="-1"></a>plt.axvline(x<span class="op">=</span><span class="dv">0</span>, color<span class="op">=</span><span class="st">'k'</span>, linestyle<span class="op">=</span><span class="st">'-'</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb17-17"><a href="#cb17-17" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="chapter9_files/figure-html/cell-13-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p><strong>Conceptual Understanding</strong>: Combines ReLU’s benefits with a smoother transition, helping gradients flow better.</p>
<p><strong>GELU</strong> (Gaussian Error Linear Unit): Used in many transformer models like BERT:</p>
<div id="19c23934" class="cell" data-execution_count="13">
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> gelu(z):</span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="fl">0.5</span> <span class="op">*</span> z <span class="op">*</span> (<span class="dv">1</span> <span class="op">+</span> np.tanh(np.sqrt(<span class="dv">2</span> <span class="op">/</span> np.pi) <span class="op">*</span> (z <span class="op">+</span> <span class="fl">0.044715</span> <span class="op">*</span> z<span class="op">**</span><span class="dv">3</span>)))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p><strong>Conceptual Understanding</strong>: Weighs inputs by their value, approximating a smooth step function modulated by the input’s magnitude.</p>
</section>
<section id="softmax-the-probability-distributor" class="level3" data-number="11.6.5">
<h3 data-number="11.6.5" class="anchored" data-anchor-id="softmax-the-probability-distributor"><span class="header-section-number">11.6.5</span> Softmax: The Probability Distributor</h3>
<p>Softmax converts a vector of values into a probability distribution:</p>
<div id="bad9b2a6" class="cell" data-execution_count="14">
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> softmax(z):</span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a>    exp_z <span class="op">=</span> np.exp(z <span class="op">-</span> np.<span class="bu">max</span>(z))</span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> exp_z <span class="op">/</span> np.<span class="bu">sum</span>(exp_z)</span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualization</span></span>
<span id="cb19-6"><a href="#cb19-6" aria-hidden="true" tabindex="-1"></a>z <span class="op">=</span> np.array([<span class="fl">2.0</span>, <span class="fl">1.0</span>, <span class="fl">0.1</span>])</span>
<span id="cb19-7"><a href="#cb19-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Softmax probabilities:"</span>, softmax(z))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Softmax probabilities: [0.65900114 0.24243297 0.09856589]</code></pre>
</div>
</div>
<p><strong>Conceptual Understanding</strong>: - Acts as a “decision maker” for multi-class problems. - Emphasizes the largest values while suppressing smaller ones. - Creates competition between outputs - increasing one probability must decrease others.</p>
<p><strong>Mental Model</strong>: Imagine softmax as a committee voting system where members with stronger opinions (higher values) get more voting power, but the total votes must add up to 100%.</p>
</section>
<section id="choosing-the-right-activation-function" class="level3" data-number="11.6.6">
<h3 data-number="11.6.6" class="anchored" data-anchor-id="choosing-the-right-activation-function"><span class="header-section-number">11.6.6</span> Choosing the Right Activation Function</h3>
<p>The choice of activation function depends on:</p>
<ol type="1">
<li><strong>Layer Type</strong>:
<ul>
<li>Hidden layers: ReLU and variants work well</li>
<li>Output layer: Sigmoid for binary classification, Softmax for multi-class, Linear for regression</li>
</ul></li>
<li><strong>Problem Domain</strong>:
<ul>
<li>Image recognition: ReLU family performs well</li>
<li>Time series and sequence modeling: GELU and Swish often excel</li>
<li>Generative models: Leaky ReLU can help</li>
</ul></li>
<li><strong>Network Depth</strong>:
<ul>
<li>Deeper networks often benefit from ReLU variants that help mitigate vanishing gradients</li>
</ul></li>
</ol>
</section>
<section id="the-mathematics-behind-learning" class="level3" data-number="11.6.7">
<h3 data-number="11.6.7" class="anchored" data-anchor-id="the-mathematics-behind-learning"><span class="header-section-number">11.6.7</span> The Mathematics Behind Learning</h3>
<p>Activation functions critically affect how networks learn through their derivatives:</p>
<div id="d9c6bfe3" class="cell" data-execution_count="15">
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Derivatives</span></span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> sigmoid_derivative(z):</span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a>    sig <span class="op">=</span> sigmoid(z)</span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> sig <span class="op">*</span> (<span class="dv">1</span> <span class="op">-</span> sig)</span>
<span id="cb21-5"><a href="#cb21-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-6"><a href="#cb21-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualization</span></span>
<span id="cb21-7"><a href="#cb21-7" aria-hidden="true" tabindex="-1"></a>z <span class="op">=</span> np.linspace(<span class="op">-</span><span class="dv">10</span>, <span class="dv">10</span>, <span class="dv">100</span>)</span>
<span id="cb21-8"><a href="#cb21-8" aria-hidden="true" tabindex="-1"></a>plt.plot(z, sigmoid_derivative(z))</span>
<span id="cb21-9"><a href="#cb21-9" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Sigmoid Derivative"</span>)</span>
<span id="cb21-10"><a href="#cb21-10" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"z"</span>)</span>
<span id="cb21-11"><a href="#cb21-11" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"sigmoid'(z)"</span>)</span>
<span id="cb21-12"><a href="#cb21-12" aria-hidden="true" tabindex="-1"></a>plt.grid(<span class="va">True</span>)</span>
<span id="cb21-13"><a href="#cb21-13" aria-hidden="true" tabindex="-1"></a>plt.axhline(y<span class="op">=</span><span class="dv">0</span>, color<span class="op">=</span><span class="st">'k'</span>, linestyle<span class="op">=</span><span class="st">'-'</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb21-14"><a href="#cb21-14" aria-hidden="true" tabindex="-1"></a>plt.axvline(x<span class="op">=</span><span class="dv">0</span>, color<span class="op">=</span><span class="st">'k'</span>, linestyle<span class="op">=</span><span class="st">'-'</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb21-15"><a href="#cb21-15" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb21-16"><a href="#cb21-16" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb21-17"><a href="#cb21-17" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> relu_derivative(z):</span>
<span id="cb21-18"><a href="#cb21-18" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="dv">1</span> <span class="cf">if</span> z <span class="op">&gt;</span> <span class="dv">0</span> <span class="cf">else</span> <span class="dv">0</span></span>
<span id="cb21-19"><a href="#cb21-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-20"><a href="#cb21-20" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualization</span></span>
<span id="cb21-21"><a href="#cb21-21" aria-hidden="true" tabindex="-1"></a>z <span class="op">=</span> np.linspace(<span class="op">-</span><span class="dv">10</span>, <span class="dv">10</span>, <span class="dv">100</span>)</span>
<span id="cb21-22"><a href="#cb21-22" aria-hidden="true" tabindex="-1"></a>plt.plot(z, [relu_derivative(zi) <span class="cf">for</span> zi <span class="kw">in</span> z])</span>
<span id="cb21-23"><a href="#cb21-23" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"ReLU Derivative"</span>)</span>
<span id="cb21-24"><a href="#cb21-24" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"z"</span>)</span>
<span id="cb21-25"><a href="#cb21-25" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"ReLU'(z)"</span>)</span>
<span id="cb21-26"><a href="#cb21-26" aria-hidden="true" tabindex="-1"></a>plt.grid(<span class="va">True</span>)</span>
<span id="cb21-27"><a href="#cb21-27" aria-hidden="true" tabindex="-1"></a>plt.axhline(y<span class="op">=</span><span class="dv">0</span>, color<span class="op">=</span><span class="st">'k'</span>, linestyle<span class="op">=</span><span class="st">'-'</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb21-28"><a href="#cb21-28" aria-hidden="true" tabindex="-1"></a>plt.axvline(x<span class="op">=</span><span class="dv">0</span>, color<span class="op">=</span><span class="st">'k'</span>, linestyle<span class="op">=</span><span class="st">'-'</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb21-29"><a href="#cb21-29" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="chapter9_files/figure-html/cell-16-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="chapter9_files/figure-html/cell-16-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p><strong>Conceptual Understanding</strong>: - The derivative determines how much a neuron’s weights update during learning. - Functions with stronger derivatives in useful ranges learn faster. - Functions with derivatives that approach zero (like sigmoid at extremes) can slow or stop learning.</p>
</section>
<section id="visualizing-decision-boundaries" class="level3" data-number="11.6.8">
<h3 data-number="11.6.8" class="anchored" data-anchor-id="visualizing-decision-boundaries"><span class="header-section-number">11.6.8</span> Visualizing Decision Boundaries</h3>
<p>Different activation functions create different decision boundaries:</p>
<ul>
<li>Linear: Can only create linear boundaries (lines, planes)</li>
<li>Sigmoid/Tanh: Can approximate curved boundaries with enough neurons</li>
<li>ReLU: Creates piecewise linear boundaries, which can approximate any shape with enough neurons</li>
</ul>
<p>This ability to create complex boundaries is what gives neural networks their remarkable flexibility.</p>
</section>
</section>
<section id="the-role-of-bias-in-neural-networks" class="level2" data-number="11.7">
<h2 data-number="11.7" class="anchored" data-anchor-id="the-role-of-bias-in-neural-networks"><span class="header-section-number">11.7</span> The Role of Bias in Neural Networks</h2>
<section id="understanding-bias-in-the-statistical-sense" class="level3" data-number="11.7.1">
<h3 data-number="11.7.1" class="anchored" data-anchor-id="understanding-bias-in-the-statistical-sense"><span class="header-section-number">11.7.1</span> Understanding Bias in the Statistical Sense</h3>
<p>In statistics and machine learning, bias refers to the model’s tendency to consistently over or underestimate the true values. It’s one component of the generalization error along with variance and irreducible error.</p>
<ul>
<li><strong>High Bias</strong>: Model is too simple to capture the underlying pattern (underfitting)</li>
<li><strong>High Variance</strong>: Model is too complex and captures noise in the training data (overfitting)</li>
</ul>
</section>
<section id="bias-nodes-in-neural-networks" class="level3" data-number="11.7.2">
<h3 data-number="11.7.2" class="anchored" data-anchor-id="bias-nodes-in-neural-networks"><span class="header-section-number">11.7.2</span> Bias Nodes in Neural Networks</h3>
<p>In neural networks, “bias” has a specific technical meaning: it’s an additional parameter added to each layer that allows the activation function to shift:</p>
<div id="b76957f7" class="cell" data-execution_count="16">
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Example showing the effect of bias</span></span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-4"><a href="#cb22-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Without bias</span></span>
<span id="cb22-5"><a href="#cb22-5" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> no_bias_output(inputs, weights, activation_function):</span>
<span id="cb22-6"><a href="#cb22-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> activation_function(np.dot(weights, inputs))</span>
<span id="cb22-7"><a href="#cb22-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-8"><a href="#cb22-8" aria-hidden="true" tabindex="-1"></a><span class="co"># With bias</span></span>
<span id="cb22-9"><a href="#cb22-9" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> with_bias_output(inputs, weights, bias, activation_function):</span>
<span id="cb22-10"><a href="#cb22-10" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> activation_function(np.dot(weights, inputs) <span class="op">+</span> bias)</span>
<span id="cb22-11"><a href="#cb22-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-12"><a href="#cb22-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Define a simple sigmoid function</span></span>
<span id="cb22-13"><a href="#cb22-13" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> sigmoid(x):</span>
<span id="cb22-14"><a href="#cb22-14" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="dv">1</span> <span class="op">/</span> (<span class="dv">1</span> <span class="op">+</span> np.exp(<span class="op">-</span>x))</span>
<span id="cb22-15"><a href="#cb22-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-16"><a href="#cb22-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Example data</span></span>
<span id="cb22-17"><a href="#cb22-17" aria-hidden="true" tabindex="-1"></a>inputs <span class="op">=</span> np.array([<span class="dv">0</span>, <span class="dv">0</span>])  <span class="co"># Both inputs are zero</span></span>
<span id="cb22-18"><a href="#cb22-18" aria-hidden="true" tabindex="-1"></a>weights <span class="op">=</span> np.array([<span class="fl">0.5</span>, <span class="fl">0.5</span>])</span>
<span id="cb22-19"><a href="#cb22-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-20"><a href="#cb22-20" aria-hidden="true" tabindex="-1"></a><span class="co"># Compare outputs</span></span>
<span id="cb22-21"><a href="#cb22-21" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Without bias:"</span>, no_bias_output(inputs, weights, sigmoid))</span>
<span id="cb22-22"><a href="#cb22-22" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"With bias (b=1):"</span>, with_bias_output(inputs, weights, <span class="dv">1</span>, sigmoid))</span>
<span id="cb22-23"><a href="#cb22-23" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"With bias (b=2):"</span>, with_bias_output(inputs, weights, <span class="dv">2</span>, sigmoid))</span>
<span id="cb22-24"><a href="#cb22-24" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"With bias (b=-2):"</span>, with_bias_output(inputs, weights, <span class="op">-</span><span class="dv">2</span>, sigmoid))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Without bias: 0.5
With bias (b=1): 0.7310585786300049
With bias (b=2): 0.8807970779778823
With bias (b=-2): 0.11920292202211755</code></pre>
</div>
</div>
<p><strong>Analogy</strong>: Think of bias as the “y-intercept” in a linear equation y = mx + b. It shifts the entire activation function up or down, allowing the neuron to fire even when all inputs are zero.</p>
<p>Let’s visualize the effect of bias on a sigmoid neuron:</p>
<div id="a30add71" class="cell" data-execution_count="17">
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> sigmoid_with_bias(x, bias):</span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="dv">1</span> <span class="op">/</span> (<span class="dv">1</span> <span class="op">+</span> np.exp(<span class="op">-</span>(x <span class="op">+</span> bias)))</span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-4"><a href="#cb24-4" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> np.linspace(<span class="op">-</span><span class="dv">10</span>, <span class="dv">10</span>, <span class="dv">100</span>)</span>
<span id="cb24-5"><a href="#cb24-5" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">6</span>))</span>
<span id="cb24-6"><a href="#cb24-6" aria-hidden="true" tabindex="-1"></a>biases <span class="op">=</span> [<span class="op">-</span><span class="dv">5</span>, <span class="op">-</span><span class="dv">2</span>, <span class="dv">0</span>, <span class="dv">2</span>, <span class="dv">5</span>]</span>
<span id="cb24-7"><a href="#cb24-7" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> b <span class="kw">in</span> biases:</span>
<span id="cb24-8"><a href="#cb24-8" aria-hidden="true" tabindex="-1"></a>    plt.plot(x, sigmoid_with_bias(x, b), label<span class="op">=</span><span class="ss">f"bias = </span><span class="sc">{</span>b<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb24-9"><a href="#cb24-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-10"><a href="#cb24-10" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Effect of Bias on Sigmoid Function"</span>)</span>
<span id="cb24-11"><a href="#cb24-11" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Input"</span>)</span>
<span id="cb24-12"><a href="#cb24-12" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"Output"</span>)</span>
<span id="cb24-13"><a href="#cb24-13" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb24-14"><a href="#cb24-14" aria-hidden="true" tabindex="-1"></a>plt.grid(<span class="va">True</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="chapter9_files/figure-html/cell-18-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="practical-implementation-in-python" class="level3" data-number="11.7.3">
<h3 data-number="11.7.3" class="anchored" data-anchor-id="practical-implementation-in-python"><span class="header-section-number">11.7.3</span> Practical Implementation in Python</h3>
<p>Here’s how bias is typically implemented in a neural network layer using NumPy:</p>
<div id="5adc1f27" class="cell" data-execution_count="18">
<div class="sourceCode cell-code" id="cb25"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> NeuralNetworkLayer:</span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, input_size, output_size, activation_function):</span>
<span id="cb25-3"><a href="#cb25-3" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Initialize weights and biases</span></span>
<span id="cb25-4"><a href="#cb25-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.weights <span class="op">=</span> np.random.randn(output_size, input_size) <span class="op">*</span> <span class="fl">0.01</span></span>
<span id="cb25-5"><a href="#cb25-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.bias <span class="op">=</span> np.zeros((output_size, <span class="dv">1</span>))</span>
<span id="cb25-6"><a href="#cb25-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.activation_function <span class="op">=</span> activation_function</span>
<span id="cb25-7"><a href="#cb25-7" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb25-8"><a href="#cb25-8" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, inputs):</span>
<span id="cb25-9"><a href="#cb25-9" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Calculate the weighted sum including bias</span></span>
<span id="cb25-10"><a href="#cb25-10" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.z <span class="op">=</span> np.dot(<span class="va">self</span>.weights, inputs) <span class="op">+</span> <span class="va">self</span>.bias</span>
<span id="cb25-11"><a href="#cb25-11" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Apply activation function</span></span>
<span id="cb25-12"><a href="#cb25-12" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.a <span class="op">=</span> <span class="va">self</span>.activation_function(<span class="va">self</span>.z)</span>
<span id="cb25-13"><a href="#cb25-13" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.a</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>And here’s how it looks in a modern deep learning framework like TensorFlow/Keras:</p>
<div class="sourceCode" id="cb26"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a><span class="co">#| eval: false</span></span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> tensorflow <span class="im">as</span> tf</span>
<span id="cb26-3"><a href="#cb26-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tensorflow.keras.models <span class="im">import</span> Sequential</span>
<span id="cb26-4"><a href="#cb26-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tensorflow.keras.layers <span class="im">import</span> Dense</span>
<span id="cb26-5"><a href="#cb26-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-6"><a href="#cb26-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a model with two hidden layers</span></span>
<span id="cb26-7"><a href="#cb26-7" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> Sequential([</span>
<span id="cb26-8"><a href="#cb26-8" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Input layer to first hidden layer</span></span>
<span id="cb26-9"><a href="#cb26-9" aria-hidden="true" tabindex="-1"></a>    Dense(<span class="dv">128</span>, activation<span class="op">=</span><span class="st">'relu'</span>, input_shape<span class="op">=</span>(<span class="dv">784</span>,), use_bias<span class="op">=</span><span class="va">True</span>),</span>
<span id="cb26-10"><a href="#cb26-10" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Second hidden layer</span></span>
<span id="cb26-11"><a href="#cb26-11" aria-hidden="true" tabindex="-1"></a>    Dense(<span class="dv">64</span>, activation<span class="op">=</span><span class="st">'relu'</span>, use_bias<span class="op">=</span><span class="va">True</span>),</span>
<span id="cb26-12"><a href="#cb26-12" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Output layer</span></span>
<span id="cb26-13"><a href="#cb26-13" aria-hidden="true" tabindex="-1"></a>    Dense(<span class="dv">10</span>, activation<span class="op">=</span><span class="st">'softmax'</span>, use_bias<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb26-14"><a href="#cb26-14" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb26-15"><a href="#cb26-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-16"><a href="#cb26-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Note: use_bias=True is actually the default in Keras</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
</section>
<section id="putting-it-all-together-building-a-neural-network" class="level2" data-number="11.8">
<h2 data-number="11.8" class="anchored" data-anchor-id="putting-it-all-together-building-a-neural-network"><span class="header-section-number">11.8</span> Putting It All Together: Building a Neural Network</h2>
<p>Let’s build a simple neural network that demonstrates these concepts:</p>
<div id="9671d1a8" class="cell" data-execution_count="19">
<div class="sourceCode cell-code" id="cb27"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-3"><a href="#cb27-3" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> SimpleNeuralNetwork:</span>
<span id="cb27-4"><a href="#cb27-4" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, input_size, hidden_size, output_size):</span>
<span id="cb27-5"><a href="#cb27-5" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Initialize weights and biases</span></span>
<span id="cb27-6"><a href="#cb27-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.W1 <span class="op">=</span> np.random.randn(hidden_size, input_size) <span class="op">*</span> <span class="fl">0.01</span></span>
<span id="cb27-7"><a href="#cb27-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.b1 <span class="op">=</span> np.zeros((hidden_size, <span class="dv">1</span>))</span>
<span id="cb27-8"><a href="#cb27-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.W2 <span class="op">=</span> np.random.randn(output_size, hidden_size) <span class="op">*</span> <span class="fl">0.01</span></span>
<span id="cb27-9"><a href="#cb27-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.b2 <span class="op">=</span> np.zeros((output_size, <span class="dv">1</span>))</span>
<span id="cb27-10"><a href="#cb27-10" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb27-11"><a href="#cb27-11" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> relu(<span class="va">self</span>, Z):</span>
<span id="cb27-12"><a href="#cb27-12" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> np.maximum(<span class="dv">0</span>, Z)</span>
<span id="cb27-13"><a href="#cb27-13" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb27-14"><a href="#cb27-14" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> softmax(<span class="va">self</span>, Z):</span>
<span id="cb27-15"><a href="#cb27-15" aria-hidden="true" tabindex="-1"></a>        exp_Z <span class="op">=</span> np.exp(Z <span class="op">-</span> np.<span class="bu">max</span>(Z, axis<span class="op">=</span><span class="dv">0</span>, keepdims<span class="op">=</span><span class="va">True</span>))</span>
<span id="cb27-16"><a href="#cb27-16" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> exp_Z <span class="op">/</span> np.<span class="bu">sum</span>(exp_Z, axis<span class="op">=</span><span class="dv">0</span>, keepdims<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb27-17"><a href="#cb27-17" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb27-18"><a href="#cb27-18" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, X):</span>
<span id="cb27-19"><a href="#cb27-19" aria-hidden="true" tabindex="-1"></a>        <span class="co"># First layer linear transformation</span></span>
<span id="cb27-20"><a href="#cb27-20" aria-hidden="true" tabindex="-1"></a>        Z1 <span class="op">=</span> np.dot(<span class="va">self</span>.W1, X) <span class="op">+</span> <span class="va">self</span>.b1</span>
<span id="cb27-21"><a href="#cb27-21" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Apply non-linear activation</span></span>
<span id="cb27-22"><a href="#cb27-22" aria-hidden="true" tabindex="-1"></a>        A1 <span class="op">=</span> <span class="va">self</span>.relu(Z1)</span>
<span id="cb27-23"><a href="#cb27-23" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Second layer linear transformation</span></span>
<span id="cb27-24"><a href="#cb27-24" aria-hidden="true" tabindex="-1"></a>        Z2 <span class="op">=</span> np.dot(<span class="va">self</span>.W2, A1) <span class="op">+</span> <span class="va">self</span>.b2</span>
<span id="cb27-25"><a href="#cb27-25" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Apply softmax activation for output layer</span></span>
<span id="cb27-26"><a href="#cb27-26" aria-hidden="true" tabindex="-1"></a>        A2 <span class="op">=</span> <span class="va">self</span>.softmax(Z2)</span>
<span id="cb27-27"><a href="#cb27-27" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb27-28"><a href="#cb27-28" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> A2</span>
<span id="cb27-29"><a href="#cb27-29" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb27-30"><a href="#cb27-30" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> predict(<span class="va">self</span>, X):</span>
<span id="cb27-31"><a href="#cb27-31" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Get probabilities</span></span>
<span id="cb27-32"><a href="#cb27-32" aria-hidden="true" tabindex="-1"></a>        probs <span class="op">=</span> <span class="va">self</span>.forward(X)</span>
<span id="cb27-33"><a href="#cb27-33" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Return class with highest probability</span></span>
<span id="cb27-34"><a href="#cb27-34" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> np.argmax(probs, axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb27-35"><a href="#cb27-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-36"><a href="#cb27-36" aria-hidden="true" tabindex="-1"></a><span class="co"># Example usage</span></span>
<span id="cb27-37"><a href="#cb27-37" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> <span class="va">__name__</span> <span class="op">==</span> <span class="st">"__main__"</span>:</span>
<span id="cb27-38"><a href="#cb27-38" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Create dummy data: 3 samples with 4 features each</span></span>
<span id="cb27-39"><a href="#cb27-39" aria-hidden="true" tabindex="-1"></a>    X <span class="op">=</span> np.random.randn(<span class="dv">4</span>, <span class="dv">3</span>)</span>
<span id="cb27-40"><a href="#cb27-40" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb27-41"><a href="#cb27-41" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Create a neural network with 4 input neurons, 5 hidden neurons, and 2 output classes</span></span>
<span id="cb27-42"><a href="#cb27-42" aria-hidden="true" tabindex="-1"></a>    nn <span class="op">=</span> SimpleNeuralNetwork(<span class="dv">4</span>, <span class="dv">5</span>, <span class="dv">2</span>)</span>
<span id="cb27-43"><a href="#cb27-43" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb27-44"><a href="#cb27-44" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Make predictions</span></span>
<span id="cb27-45"><a href="#cb27-45" aria-hidden="true" tabindex="-1"></a>    predictions <span class="op">=</span> nn.predict(X)</span>
<span id="cb27-46"><a href="#cb27-46" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"Predictions:"</span>, predictions)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Predictions: [0 1 1]</code></pre>
</div>
</div>
</section>
<section id="connection-to-deep-learning" class="level2" data-number="11.9">
<h2 data-number="11.9" class="anchored" data-anchor-id="connection-to-deep-learning"><span class="header-section-number">11.9</span> Connection to Deep Learning</h2>
<p>Deep learning refers to neural networks with many layers (hence “deep”). The key insight of deep learning is that:</p>
<ol type="1">
<li>Multiple layers allow the network to learn hierarchical representations</li>
<li>Lower layers learn simple features</li>
<li>Higher layers combine these features into more complex patterns</li>
</ol>
<p>For example, in a deep convolutional network for image recognition: - First layer might detect edges and simple textures - Middle layers might detect patterns like corners and curves - Higher layers might detect entire objects like faces or cars</p>
<p>The non-linearity introduced by activation functions is what makes this hierarchical learning possible.</p>
</section>
<section id="real-world-analogy-building-a-house" class="level2" data-number="11.10">
<h2 data-number="11.10" class="anchored" data-anchor-id="real-world-analogy-building-a-house"><span class="header-section-number">11.10</span> Real-World Analogy: Building a House</h2>
<p>Let’s use the analogy of building a house to understand neural networks:</p>
<ol type="1">
<li><strong>Inputs (X)</strong>: These are the raw materials (bricks, wood, cement, etc.)</li>
<li><strong>Weights (W)</strong>: These represent the importance of each material for different parts of the house</li>
<li><strong>Bias (b)</strong>: This represents the architectural style or design preferences</li>
<li><strong>Activation function</strong>: This represents the construction techniques that transform materials into structures</li>
<li><strong>Hidden layers</strong>: These are the intermediate structures (walls, roof frames, plumbing)</li>
<li><strong>Output</strong>: The completed house</li>
</ol>
<p>Without non-linearity (activation functions), we could only build simple, linear structures. The non-linear activation functions allow us to create complex, curved, and intricate designs.</p>
</section>
<section id="conclusion" class="level2" data-number="11.11">
<h2 data-number="11.11" class="anchored" data-anchor-id="conclusion"><span class="header-section-number">11.11</span> Conclusion</h2>
<p>Neural networks derive their power from:</p>
<ol type="1">
<li><strong>Biological inspiration</strong>: Learning from how our brains process information</li>
<li><strong>Non-linearity</strong>: Enabling the approximation of complex functions</li>
<li><strong>Bias parameters</strong>: Allowing flexibility in neuron activation</li>
<li><strong>Hierarchical representation</strong>: Building complex concepts from simpler ones</li>
</ol>
<p>Together, these elements have revolutionized machine learning and enabled breakthroughs in various fields including computer vision, natural language processing, and many other domains.</p>
</section>
<section id="neural-networks-foundations-tlus-perceptrons-and-mlps" class="level1" data-number="12">
<h1 data-number="12"><span class="header-section-number">12</span> Neural Networks Foundations: TLUs, Perceptrons, and MLPs</h1>
<section id="threshold-logic-units-tlus" class="level2" data-number="12.1">
<h2 data-number="12.1" class="anchored" data-anchor-id="threshold-logic-units-tlus"><span class="header-section-number">12.1</span> Threshold Logic Units (TLUs)</h2>
<section id="the-building-blocks-of-neural-networks" class="level3" data-number="12.1.1">
<h3 data-number="12.1.1" class="anchored" data-anchor-id="the-building-blocks-of-neural-networks"><span class="header-section-number">12.1.1</span> The Building Blocks of Neural Networks</h3>
<p>A Threshold Logic Unit (TLU), also known as a Linear Threshold Unit (LTU), is the foundational building block of early neural networks. Think of a TLU as a simplified model of a biological neuron - it receives multiple inputs, processes them, and produces a single output based on whether the combined input exceeds a certain threshold.</p>
</section>
<section id="how-tlus-work" class="level3" data-number="12.1.2">
<h3 data-number="12.1.2" class="anchored" data-anchor-id="how-tlus-work"><span class="header-section-number">12.1.2</span> How TLUs Work</h3>
<p>A TLU can be visualized as a decision-making unit with the following components:</p>
<ul>
<li><strong>Inputs</strong>: A set of numerical values <span class="math inline">\(x_1, x_2, ..., x_n\)</span></li>
<li><strong>Weights</strong>: Each input is assigned a weight <span class="math inline">\(w_1, w_2, ..., w_n\)</span></li>
<li><strong>Threshold</strong>: A value <span class="math inline">\(\theta\)</span> that determines when the unit activates</li>
<li><strong>Output</strong>: A single value <span class="math inline">\(y\)</span> (typically 0 or 1)</li>
</ul>
<p>The TLU computes its output using this simple formula:</p>
<p><span class="math display">\[y = \begin{cases}
1, &amp; \text{if } \sum_{i=1}^{n} w_i x_i \geq \theta \\
0, &amp; \text{otherwise}
\end{cases}\]</span></p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>This is often rewritten by moving the threshold to the other side, creating what we call a “bias” term <span class="math inline">\(b = -\theta\)</span>:</p>
<p><span class="math display">\[y = \begin{cases}
1, &amp; \text{if } \sum_{i=1}^{n} w_i x_i + b \geq 0 \\
0, &amp; \text{otherwise}
\end{cases}\]</span></p>
</div>
</div>
</section>
<section id="analogy-the-voting-committee" class="level3" data-number="12.1.3">
<h3 data-number="12.1.3" class="anchored" data-anchor-id="analogy-the-voting-committee"><span class="header-section-number">12.1.3</span> Analogy: The Voting Committee</h3>
<p>Imagine a committee where each member (input) has a different level of influence (weight) on the final decision. The committee is voting on whether to approve a proposal (output 1) or reject it (output 0). Each member casts their vote, which is then weighted by their influence. If the weighted sum of votes exceeds a certain threshold, the proposal is approved; otherwise, it’s rejected.</p>
</section>
<section id="tlus-as-logic-gates" class="level3" data-number="12.1.4">
<h3 data-number="12.1.4" class="anchored" data-anchor-id="tlus-as-logic-gates"><span class="header-section-number">12.1.4</span> TLUs as Logic Gates</h3>
<p>TLUs can implement basic logical operations. Let’s look at two examples:</p>
<section id="example-1-logical-and-operation-x₁-x₂" class="level4" data-number="12.1.4.1">
<h4 data-number="12.1.4.1" class="anchored" data-anchor-id="example-1-logical-and-operation-x₁-x₂"><span class="header-section-number">12.1.4.1</span> Example 1: Logical AND Operation (x₁ ∧ x₂)</h4>
<p>For the logical AND of two binary inputs, we want: - Output 1 only when both inputs are 1 - Output 0 otherwise</p>
<p>We can achieve this with the following weights and threshold: - <span class="math inline">\(w_1 = 1\)</span> - <span class="math inline">\(w_2 = 1\)</span> - <span class="math inline">\(\theta = 1.5\)</span></p>
<p>With these parameters: - When <span class="math inline">\(x_1 = 0, x_2 = 0\)</span>: <span class="math inline">\(0 \cdot 1 + 0 \cdot 1 = 0 &lt; 1.5\)</span>, so output is 0 - When <span class="math inline">\(x_1 = 1, x_2 = 0\)</span>: <span class="math inline">\(1 \cdot 1 + 0 \cdot 1 = 1 &lt; 1.5\)</span>, so output is 0 - When <span class="math inline">\(x_1 = 0, x_2 = 1\)</span>: <span class="math inline">\(0 \cdot 1 + 1 \cdot 1 = 1 &lt; 1.5\)</span>, so output is 0 - When <span class="math inline">\(x_1 = 1, x_2 = 1\)</span>: <span class="math inline">\(1 \cdot 1 + 1 \cdot 1 = 2 &gt; 1.5\)</span>, so output is 1</p>
</section>
<section id="example-2-logical-implication-x₂-x₁" class="level4" data-number="12.1.4.2">
<h4 data-number="12.1.4.2" class="anchored" data-anchor-id="example-2-logical-implication-x₂-x₁"><span class="header-section-number">12.1.4.2</span> Example 2: Logical Implication (x₂ → x₁)</h4>
<p>For the logical implication “if x₂ then x₁”, we want: - Output 0 only when x₂ is 1 and x₁ is 0 (the only case where the implication fails) - Output 1 otherwise</p>
<p>We can achieve this with: - <span class="math inline">\(w_1 = 1\)</span> - <span class="math inline">\(w_2 = -1\)</span> - <span class="math inline">\(\theta = -0.5\)</span></p>
<p>Let’s verify: - When <span class="math inline">\(x_1 = 0, x_2 = 0\)</span>: <span class="math inline">\(0 \cdot 1 + 0 \cdot (-1) = 0 &gt; -0.5\)</span>, so output is 1 - When <span class="math inline">\(x_1 = 1, x_2 = 0\)</span>: <span class="math inline">\(1 \cdot 1 + 0 \cdot (-1) = 1 &gt; -0.5\)</span>, so output is 1 - When <span class="math inline">\(x_1 = 0, x_2 = 1\)</span>: <span class="math inline">\(0 \cdot 1 + 1 \cdot (-1) = -1 &lt; -0.5\)</span>, so output is 0 - When <span class="math inline">\(x_1 = 1, x_2 = 1\)</span>: <span class="math inline">\(1 \cdot 1 + 1 \cdot (-1) = 0 &gt; -0.5\)</span>, so output is 1</p>
</section>
</section>
<section id="python-implementation-of-a-tlu" class="level3" data-number="12.1.5">
<h3 data-number="12.1.5" class="anchored" data-anchor-id="python-implementation-of-a-tlu"><span class="header-section-number">12.1.5</span> Python Implementation of a TLU</h3>
<div class="sourceCode" id="cb29"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb29-2"><a href="#cb29-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-3"><a href="#cb29-3" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> ThresholdLogicUnit:</span>
<span id="cb29-4"><a href="#cb29-4" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, weights, threshold):</span>
<span id="cb29-5"><a href="#cb29-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.weights <span class="op">=</span> np.array(weights)</span>
<span id="cb29-6"><a href="#cb29-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.threshold <span class="op">=</span> threshold</span>
<span id="cb29-7"><a href="#cb29-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-8"><a href="#cb29-8" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> activate(<span class="va">self</span>, inputs):</span>
<span id="cb29-9"><a href="#cb29-9" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Ensure inputs is a numpy array</span></span>
<span id="cb29-10"><a href="#cb29-10" aria-hidden="true" tabindex="-1"></a>        inputs <span class="op">=</span> np.array(inputs)</span>
<span id="cb29-11"><a href="#cb29-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-12"><a href="#cb29-12" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Calculate weighted sum</span></span>
<span id="cb29-13"><a href="#cb29-13" aria-hidden="true" tabindex="-1"></a>        weighted_sum <span class="op">=</span> np.dot(inputs, <span class="va">self</span>.weights)</span>
<span id="cb29-14"><a href="#cb29-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-15"><a href="#cb29-15" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Apply threshold</span></span>
<span id="cb29-16"><a href="#cb29-16" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="dv">1</span> <span class="cf">if</span> weighted_sum <span class="op">&gt;=</span> <span class="va">self</span>.threshold <span class="cf">else</span> <span class="dv">0</span></span>
<span id="cb29-17"><a href="#cb29-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-18"><a href="#cb29-18" aria-hidden="true" tabindex="-1"></a><span class="co"># Example: TLU implementing logical AND</span></span>
<span id="cb29-19"><a href="#cb29-19" aria-hidden="true" tabindex="-1"></a>and_tlu <span class="op">=</span> ThresholdLogicUnit([<span class="dv">1</span>, <span class="dv">1</span>], <span class="fl">1.5</span>)</span>
<span id="cb29-20"><a href="#cb29-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-21"><a href="#cb29-21" aria-hidden="true" tabindex="-1"></a><span class="co"># Test all possible inputs</span></span>
<span id="cb29-22"><a href="#cb29-22" aria-hidden="true" tabindex="-1"></a>inputs <span class="op">=</span> [[<span class="dv">0</span>, <span class="dv">0</span>], [<span class="dv">0</span>, <span class="dv">1</span>], [<span class="dv">1</span>, <span class="dv">0</span>], [<span class="dv">1</span>, <span class="dv">1</span>]]</span>
<span id="cb29-23"><a href="#cb29-23" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> input_pair <span class="kw">in</span> inputs:</span>
<span id="cb29-24"><a href="#cb29-24" aria-hidden="true" tabindex="-1"></a>    output <span class="op">=</span> and_tlu.activate(input_pair)</span>
<span id="cb29-25"><a href="#cb29-25" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Inputs: </span><span class="sc">{</span>input_pair<span class="sc">}</span><span class="ss">, Output: </span><span class="sc">{</span>output<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb29-26"><a href="#cb29-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-27"><a href="#cb29-27" aria-hidden="true" tabindex="-1"></a><span class="co"># Example: TLU implementing logical implication (x₂ → x₁)</span></span>
<span id="cb29-28"><a href="#cb29-28" aria-hidden="true" tabindex="-1"></a>implication_tlu <span class="op">=</span> ThresholdLogicUnit([<span class="dv">1</span>, <span class="op">-</span><span class="dv">1</span>], <span class="op">-</span><span class="fl">0.5</span>)</span>
<span id="cb29-29"><a href="#cb29-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-30"><a href="#cb29-30" aria-hidden="true" tabindex="-1"></a><span class="co"># Test all possible inputs</span></span>
<span id="cb29-31"><a href="#cb29-31" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> input_pair <span class="kw">in</span> inputs:</span>
<span id="cb29-32"><a href="#cb29-32" aria-hidden="true" tabindex="-1"></a>    output <span class="op">=</span> implication_tlu.activate(input_pair)</span>
<span id="cb29-33"><a href="#cb29-33" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Inputs: </span><span class="sc">{</span>input_pair<span class="sc">}</span><span class="ss">, Implication Output: </span><span class="sc">{</span>output<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
</section>
<section id="perceptron" class="level2" data-number="12.2">
<h2 data-number="12.2" class="anchored" data-anchor-id="perceptron"><span class="header-section-number">12.2</span> Perceptron</h2>
<section id="the-first-learning-neural-network" class="level3" data-number="12.2.1">
<h3 data-number="12.2.1" class="anchored" data-anchor-id="the-first-learning-neural-network"><span class="header-section-number">12.2.1</span> The First Learning Neural Network</h3>
<p>The Perceptron, invented by Frank Rosenblatt in 1957, was one of the first algorithmic implementations of a learning neural network. It builds upon the TLU by adding a mechanism to automatically learn the weights and threshold.</p>
</section>
<section id="structure-of-a-perceptron" class="level3" data-number="12.2.2">
<h3 data-number="12.2.2" class="anchored" data-anchor-id="structure-of-a-perceptron"><span class="header-section-number">12.2.2</span> Structure of a Perceptron</h3>
<p>A basic Perceptron consists of:</p>
<ul>
<li>A single layer of TLUs</li>
<li>Each TLU connected to all inputs</li>
<li>An additional bias input (usually fixed at 1)</li>
</ul>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Tip
</div>
</div>
<div class="callout-body-container callout-body">
<p>The bias input allows the Perceptron to learn the threshold automatically as part of the weight vector.</p>
</div>
</div>
</section>
<section id="analogy-learning-to-balance-a-scale" class="level3" data-number="12.2.3">
<h3 data-number="12.2.3" class="anchored" data-anchor-id="analogy-learning-to-balance-a-scale"><span class="header-section-number">12.2.3</span> Analogy: Learning to Balance a Scale</h3>
<p>Imagine a scale with multiple weight plates (inputs) on one side. Initially, you don’t know how much each plate weighs (the weights are unknown). The Perceptron is like a system that gradually learns the weight of each plate by making guesses, checking whether the scale tips (the output), and adjusting its estimates based on errors.</p>
</section>
<section id="binary-classification-with-perceptrons" class="level3" data-number="12.2.4">
<h3 data-number="12.2.4" class="anchored" data-anchor-id="binary-classification-with-perceptrons"><span class="header-section-number">12.2.4</span> Binary Classification with Perceptrons</h3>
<p>The simplest use of a Perceptron is for binary classification:</p>
<ol type="1">
<li>Inputs are fed into the Perceptron</li>
<li>The Perceptron computes a weighted sum and applies a step function</li>
<li>If the result exceeds the threshold, it classifies the input as positive; otherwise, as negative</li>
</ol>
<p>The decision boundary created by a Perceptron is always a straight line (in 2D), a plane (in 3D), or a hyperplane (in higher dimensions).</p>
</section>
<section id="multi-output-perceptrons" class="level3" data-number="12.2.5">
<h3 data-number="12.2.5" class="anchored" data-anchor-id="multi-output-perceptrons"><span class="header-section-number">12.2.5</span> Multi-Output Perceptrons</h3>
<p>A Perceptron can have multiple outputs, with each output neuron solving a different binary classification problem. This creates a multi-output classifier.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="perceptron_multi_output.png" class="img-fluid figure-img"></p>
<figcaption>Perceptron with two inputs and three outputs</figcaption>
</figure>
</div>
</section>
<section id="activation-functions" class="level3" data-number="12.2.6">
<h3 data-number="12.2.6" class="anchored" data-anchor-id="activation-functions"><span class="header-section-number">12.2.6</span> Activation Functions</h3>
<p>The original Perceptron uses a step function for activation. Common choices include:</p>
<ol type="1">
<li><p><strong>Heaviside Step Function</strong>: <span class="math display">\[H(x) = \begin{cases}
1, &amp; \text{if } x \geq 0 \\
0, &amp; \text{otherwise}
\end{cases}\]</span></p></li>
<li><p><strong>Signum Function</strong>: <span class="math display">\[\text{sgn}(x) = \begin{cases}
1, &amp; \text{if } x &gt; 0 \\
0, &amp; \text{if } x = 0 \\
-1, &amp; \text{if } x &lt; 0
\end{cases}\]</span></p></li>
</ol>
</section>
<section id="the-perceptron-learning-algorithm" class="level3" data-number="12.2.7">
<h3 data-number="12.2.7" class="anchored" data-anchor-id="the-perceptron-learning-algorithm"><span class="header-section-number">12.2.7</span> The Perceptron Learning Algorithm</h3>
<p>The Perceptron learning algorithm follows these steps:</p>
<ol type="1">
<li>Initialize weights to small random values</li>
<li>For each training instance:
<ol type="a">
<li>Compute the output ŷ based on current weights</li>
<li>Update weights using the Perceptron learning rule: <span class="math inline">\(w_i \leftarrow w_i + \eta (y - \hat{y}) x_i\)</span> where η is the learning rate, y is the true label, and ŷ is the predicted label</li>
</ol></li>
</ol>
<p>This algorithm was inspired by Hebbian learning, which is often summarized as “neurons that fire together, wire together.”</p>
</section>
<section id="python-implementation-of-a-perceptron" class="level3" data-number="12.2.8">
<h3 data-number="12.2.8" class="anchored" data-anchor-id="python-implementation-of-a-perceptron"><span class="header-section-number">12.2.8</span> Python Implementation of a Perceptron</h3>
<div id="4f7a0883" class="cell" data-execution_count="20">
<div class="sourceCode cell-code" id="cb30"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb30-2"><a href="#cb30-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb30-3"><a href="#cb30-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.datasets <span class="im">import</span> make_classification</span>
<span id="cb30-4"><a href="#cb30-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-5"><a href="#cb30-5" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Perceptron:</span>
<span id="cb30-6"><a href="#cb30-6" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, learning_rate<span class="op">=</span><span class="fl">0.01</span>, n_iterations<span class="op">=</span><span class="dv">1000</span>):</span>
<span id="cb30-7"><a href="#cb30-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.learning_rate <span class="op">=</span> learning_rate</span>
<span id="cb30-8"><a href="#cb30-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.n_iterations <span class="op">=</span> n_iterations</span>
<span id="cb30-9"><a href="#cb30-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.weights <span class="op">=</span> <span class="va">None</span></span>
<span id="cb30-10"><a href="#cb30-10" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.bias <span class="op">=</span> <span class="va">None</span></span>
<span id="cb30-11"><a href="#cb30-11" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb30-12"><a href="#cb30-12" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> fit(<span class="va">self</span>, X, y):</span>
<span id="cb30-13"><a href="#cb30-13" aria-hidden="true" tabindex="-1"></a>        n_samples, n_features <span class="op">=</span> X.shape</span>
<span id="cb30-14"><a href="#cb30-14" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb30-15"><a href="#cb30-15" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Initialize weights and bias</span></span>
<span id="cb30-16"><a href="#cb30-16" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.weights <span class="op">=</span> np.zeros(n_features)</span>
<span id="cb30-17"><a href="#cb30-17" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.bias <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb30-18"><a href="#cb30-18" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb30-19"><a href="#cb30-19" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Ensure y is in proper format (-1, 1)</span></span>
<span id="cb30-20"><a href="#cb30-20" aria-hidden="true" tabindex="-1"></a>        y_ <span class="op">=</span> np.array([<span class="dv">1</span> <span class="cf">if</span> i <span class="op">&gt;</span> <span class="dv">0</span> <span class="cf">else</span> <span class="op">-</span><span class="dv">1</span> <span class="cf">for</span> i <span class="kw">in</span> y])</span>
<span id="cb30-21"><a href="#cb30-21" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb30-22"><a href="#cb30-22" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Learning</span></span>
<span id="cb30-23"><a href="#cb30-23" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(<span class="va">self</span>.n_iterations):</span>
<span id="cb30-24"><a href="#cb30-24" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> idx, x_i <span class="kw">in</span> <span class="bu">enumerate</span>(X):</span>
<span id="cb30-25"><a href="#cb30-25" aria-hidden="true" tabindex="-1"></a>                linear_output <span class="op">=</span> np.dot(x_i, <span class="va">self</span>.weights) <span class="op">+</span> <span class="va">self</span>.bias</span>
<span id="cb30-26"><a href="#cb30-26" aria-hidden="true" tabindex="-1"></a>                y_predicted <span class="op">=</span> <span class="dv">1</span> <span class="cf">if</span> linear_output <span class="op">&gt;=</span> <span class="dv">0</span> <span class="cf">else</span> <span class="op">-</span><span class="dv">1</span></span>
<span id="cb30-27"><a href="#cb30-27" aria-hidden="true" tabindex="-1"></a>                </span>
<span id="cb30-28"><a href="#cb30-28" aria-hidden="true" tabindex="-1"></a>                <span class="co"># Update weights if prediction is wrong</span></span>
<span id="cb30-29"><a href="#cb30-29" aria-hidden="true" tabindex="-1"></a>                <span class="cf">if</span> y_predicted <span class="op">!=</span> y_[idx]:</span>
<span id="cb30-30"><a href="#cb30-30" aria-hidden="true" tabindex="-1"></a>                    update <span class="op">=</span> <span class="va">self</span>.learning_rate <span class="op">*</span> y_[idx]</span>
<span id="cb30-31"><a href="#cb30-31" aria-hidden="true" tabindex="-1"></a>                    <span class="va">self</span>.weights <span class="op">+=</span> update <span class="op">*</span> x_i</span>
<span id="cb30-32"><a href="#cb30-32" aria-hidden="true" tabindex="-1"></a>                    <span class="va">self</span>.bias <span class="op">+=</span> update</span>
<span id="cb30-33"><a href="#cb30-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-34"><a href="#cb30-34" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> predict(<span class="va">self</span>, X):</span>
<span id="cb30-35"><a href="#cb30-35" aria-hidden="true" tabindex="-1"></a>        linear_output <span class="op">=</span> np.dot(X, <span class="va">self</span>.weights) <span class="op">+</span> <span class="va">self</span>.bias</span>
<span id="cb30-36"><a href="#cb30-36" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> np.array([<span class="dv">1</span> <span class="cf">if</span> i <span class="op">&gt;=</span> <span class="dv">0</span> <span class="cf">else</span> <span class="dv">0</span> <span class="cf">for</span> i <span class="kw">in</span> linear_output])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
</section>
</section>
<section id="generate-a-simple-dataset" class="level1" data-number="13">
<h1 data-number="13"><span class="header-section-number">13</span> Generate a simple dataset</h1>
<div class="sourceCode" id="cb31"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a>X, y <span class="op">=</span> make_classification(n_samples<span class="op">=</span><span class="dv">100</span>, n_features<span class="op">=</span><span class="dv">2</span>, n_redundant<span class="op">=</span><span class="dv">0</span>, n_informative<span class="op">=</span><span class="dv">2</span>, random_state<span class="op">=</span><span class="dv">1</span>, n_clusters_per_class<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb31-2"><a href="#cb31-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-3"><a href="#cb31-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Train Perceptron</span></span>
<span id="cb31-4"><a href="#cb31-4" aria-hidden="true" tabindex="-1"></a>perceptron <span class="op">=</span> Perceptron(learning_rate<span class="op">=</span><span class="fl">0.1</span>, n_iterations<span class="op">=</span><span class="dv">100</span>)</span>
<span id="cb31-5"><a href="#cb31-5" aria-hidden="true" tabindex="-1"></a>perceptron.fit(X, y)</span>
<span id="cb31-6"><a href="#cb31-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-7"><a href="#cb31-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualize decision boundary</span></span>
<span id="cb31-8"><a href="#cb31-8" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> plot_decision_boundary(X, y, classifier):</span>
<span id="cb31-9"><a href="#cb31-9" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Create mesh grid</span></span>
<span id="cb31-10"><a href="#cb31-10" aria-hidden="true" tabindex="-1"></a>    h <span class="op">=</span> <span class="fl">0.02</span>  <span class="co"># step size in the mesh</span></span>
<span id="cb31-11"><a href="#cb31-11" aria-hidden="true" tabindex="-1"></a>    x_min, x_max <span class="op">=</span> X[:, <span class="dv">0</span>].<span class="bu">min</span>() <span class="op">-</span> <span class="dv">1</span>, X[:, <span class="dv">0</span>].<span class="bu">max</span>() <span class="op">+</span> <span class="dv">1</span></span>
<span id="cb31-12"><a href="#cb31-12" aria-hidden="true" tabindex="-1"></a>    y_min, y_max <span class="op">=</span> X[:, <span class="dv">1</span>].<span class="bu">min</span>() <span class="op">-</span> <span class="dv">1</span>, X[:, <span class="dv">1</span>].<span class="bu">max</span>() <span class="op">+</span> <span class="dv">1</span></span>
<span id="cb31-13"><a href="#cb31-13" aria-hidden="true" tabindex="-1"></a>    xx, yy <span class="op">=</span> np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))</span>
<span id="cb31-14"><a href="#cb31-14" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb31-15"><a href="#cb31-15" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Plot decision boundary</span></span>
<span id="cb31-16"><a href="#cb31-16" aria-hidden="true" tabindex="-1"></a>    Z <span class="op">=</span> classifier.predict(np.c_[xx.ravel(), yy.ravel()])</span>
<span id="cb31-17"><a href="#cb31-17" aria-hidden="true" tabindex="-1"></a>    Z <span class="op">=</span> Z.reshape(xx.shape)</span>
<span id="cb31-18"><a href="#cb31-18" aria-hidden="true" tabindex="-1"></a>    plt.contourf(xx, yy, Z, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb31-19"><a href="#cb31-19" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb31-20"><a href="#cb31-20" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Plot data points</span></span>
<span id="cb31-21"><a href="#cb31-21" aria-hidden="true" tabindex="-1"></a>    plt.scatter(X[:, <span class="dv">0</span>], X[:, <span class="dv">1</span>], c<span class="op">=</span>y, edgecolors<span class="op">=</span><span class="st">'k'</span>, marker<span class="op">=</span><span class="st">'o'</span>)</span>
<span id="cb31-22"><a href="#cb31-22" aria-hidden="true" tabindex="-1"></a>    plt.xlabel(<span class="st">'Feature 1'</span>)</span>
<span id="cb31-23"><a href="#cb31-23" aria-hidden="true" tabindex="-1"></a>    plt.ylabel(<span class="st">'Feature 2'</span>)</span>
<span id="cb31-24"><a href="#cb31-24" aria-hidden="true" tabindex="-1"></a>    plt.title(<span class="st">'Perceptron Decision Boundary'</span>)</span>
<span id="cb31-25"><a href="#cb31-25" aria-hidden="true" tabindex="-1"></a>    plt.show()</span>
<span id="cb31-26"><a href="#cb31-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-27"><a href="#cb31-27" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot decision boundary</span></span>
<span id="cb31-28"><a href="#cb31-28" aria-hidden="true" tabindex="-1"></a>plot_decision_boundary(X, y, perceptron)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<section id="comparison-with-logistic-regression" class="level3" data-number="13.0.1">
<h3 data-number="13.0.1" class="anchored" data-anchor-id="comparison-with-logistic-regression"><span class="header-section-number">13.0.1</span> Comparison with Logistic Regression</h3>
<p>While Perceptrons and Logistic Regression both create linear decision boundaries, they differ in important ways:</p>
<ol type="1">
<li><strong>Output type</strong>:
<ul>
<li>Perceptron: Hard binary output (0 or 1)</li>
<li>Logistic Regression: Probability (0 to 1)</li>
</ul></li>
<li><strong>Training objective</strong>:
<ul>
<li>Perceptron: Minimize misclassification</li>
<li>Logistic Regression: Maximize likelihood</li>
</ul></li>
<li><strong>Convergence</strong>:
<ul>
<li>Perceptron: Guaranteed to converge only for linearly separable data</li>
<li>Logistic Regression: Can converge even for non-separable data</li>
</ul></li>
</ol>
</section>
<section id="limitations-of-perceptrons" class="level3" data-number="13.0.2">
<h3 data-number="13.0.2" class="anchored" data-anchor-id="limitations-of-perceptrons"><span class="header-section-number">13.0.2</span> Limitations of Perceptrons</h3>
<p>The most famous limitation of the Perceptron is its inability to solve problems that aren’t linearly separable, such as the XOR problem:</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th style="text-align: center;">x₁</th>
<th style="text-align: center;">x₂</th>
<th style="text-align: center;">XOR</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
</tr>
<tr class="even">
<td style="text-align: center;">0</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
</tr>
<tr class="odd">
<td style="text-align: center;">1</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">1</td>
</tr>
<tr class="even">
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">0</td>
</tr>
</tbody>
</table>
<p>No straight line can separate the 0s from the 1s in this case. This limitation was highlighted in the famous 1969 book “Perceptrons” by Minsky and Papert, which temporarily slowed down neural network research.</p>
<p>The solution to this problem was to stack multiple Perceptrons together, creating a Multilayer Perceptron.</p>
</section>
<section id="multilayer-perceptron-mlp" class="level2" data-number="13.1">
<h2 data-number="13.1" class="anchored" data-anchor-id="multilayer-perceptron-mlp"><span class="header-section-number">13.1</span> Multilayer Perceptron (MLP)</h2>
<section id="beyond-simple-linear-boundaries" class="level3" data-number="13.1.1">
<h3 data-number="13.1.1" class="anchored" data-anchor-id="beyond-simple-linear-boundaries"><span class="header-section-number">13.1.1</span> Beyond Simple Linear Boundaries</h3>
<p>A Multilayer Perceptron (MLP) is a neural network that consists of multiple layers of TLUs, allowing it to learn non-linear decision boundaries and solve complex problems like XOR.</p>
</section>
<section id="structure-of-an-mlp" class="level3" data-number="13.1.2">
<h3 data-number="13.1.2" class="anchored" data-anchor-id="structure-of-an-mlp"><span class="header-section-number">13.1.2</span> Structure of an MLP</h3>
<p>An MLP consists of:</p>
<ol type="1">
<li><strong>Input layer</strong>: Receives the input features</li>
<li><strong>Hidden layers</strong>: One or more intermediate layers of neurons</li>
<li><strong>Output layer</strong>: Produces the final prediction</li>
</ol>
<p>Each layer is typically fully connected to the next layer, meaning every neuron connects to all neurons in the adjacent layer.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>Layers closer to the input are called “lower” layers, while those closer to the output are called “upper” layers.</p>
</div>
</div>
</section>
<section id="analogy-hierarchical-decision-making" class="level3" data-number="13.1.3">
<h3 data-number="13.1.3" class="anchored" data-anchor-id="analogy-hierarchical-decision-making"><span class="header-section-number">13.1.3</span> Analogy: Hierarchical Decision Making</h3>
<p>Imagine a company where decisions pass through multiple levels of management: - Level 1 managers (first hidden layer) each focus on specific aspects of the input data - Level 2 managers (second hidden layer) combine the insights from Level 1 managers - Executives (output layer) make the final decision based on the processed information</p>
<p>Each level transforms and refines the information, allowing complex patterns to be recognized that wouldn’t be visible at any single level.</p>
</section>
<section id="why-mlps-can-solve-xor" class="level3" data-number="13.1.4">
<h3 data-number="13.1.4" class="anchored" data-anchor-id="why-mlps-can-solve-xor"><span class="header-section-number">13.1.4</span> Why MLPs Can Solve XOR</h3>
<p>The XOR problem can be solved with a 2-layer MLP: 1. The first hidden layer can learn two different lines 2. The second layer combines these lines to create a non-linear boundary</p>
<p>To solve XOR: - One hidden neuron learns to activate when (x₁=0, x₂=1) - Another hidden neuron learns to activate when (x₁=1, x₂=0) - The output neuron activates when either hidden neuron is active</p>
</section>
<section id="the-need-for-non-linear-activation-functions" class="level3" data-number="13.1.5">
<h3 data-number="13.1.5" class="anchored" data-anchor-id="the-need-for-non-linear-activation-functions"><span class="header-section-number">13.1.5</span> The Need for Non-Linear Activation Functions</h3>
<p>In modern MLPs, the step function is typically replaced with differentiable non-linear functions such as:</p>
<ol type="1">
<li><p><strong>Sigmoid</strong>: <span class="math display">\[\sigma(x) = \frac{1}{1 + e^{-x}}\]</span></p></li>
<li><p><strong>Hyperbolic Tangent (tanh)</strong>: <span class="math display">\[\tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}\]</span></p></li>
<li><p><strong>Rectified Linear Unit (ReLU)</strong>: <span class="math display">\[\text{ReLU}(x) = \max(0, x)\]</span></p></li>
</ol>
<p>These non-linear functions are crucial - without them, multiple linear layers would simply collapse into a single linear transformation.</p>
</section>
<section id="training-mlps-with-backpropagation" class="level3" data-number="13.1.6">
<h3 data-number="13.1.6" class="anchored" data-anchor-id="training-mlps-with-backpropagation"><span class="header-section-number">13.1.6</span> Training MLPs with Backpropagation</h3>
<p>MLPs are trained using the backpropagation algorithm, which: 1. Performs a forward pass to compute predictions 2. Calculates the error between predictions and targets 3. Propagates the error backward through the network 4. Updates weights using gradient descent</p>
<p>This process allows the MLP to learn complex patterns by gradually adjusting its internal parameters.</p>
</section>
<section id="python-implementation-of-an-mlp" class="level3" data-number="13.1.7">
<h3 data-number="13.1.7" class="anchored" data-anchor-id="python-implementation-of-an-mlp"><span class="header-section-number">13.1.7</span> Python Implementation of an MLP</h3>
<div id="64849433" class="cell" data-execution_count="21">
<div class="sourceCode cell-code" id="cb32"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb32-2"><a href="#cb32-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.datasets <span class="im">import</span> make_moons</span>
<span id="cb32-3"><a href="#cb32-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb32-4"><a href="#cb32-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-5"><a href="#cb32-5" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> MLP:</span>
<span id="cb32-6"><a href="#cb32-6" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, input_size, hidden_size, output_size, learning_rate<span class="op">=</span><span class="fl">0.1</span>):</span>
<span id="cb32-7"><a href="#cb32-7" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Initialize weights with small random values</span></span>
<span id="cb32-8"><a href="#cb32-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.W1 <span class="op">=</span> np.random.randn(input_size, hidden_size) <span class="op">*</span> <span class="fl">0.01</span></span>
<span id="cb32-9"><a href="#cb32-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.b1 <span class="op">=</span> np.zeros((<span class="dv">1</span>, hidden_size))</span>
<span id="cb32-10"><a href="#cb32-10" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.W2 <span class="op">=</span> np.random.randn(hidden_size, output_size) <span class="op">*</span> <span class="fl">0.01</span></span>
<span id="cb32-11"><a href="#cb32-11" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.b2 <span class="op">=</span> np.zeros((<span class="dv">1</span>, output_size))</span>
<span id="cb32-12"><a href="#cb32-12" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.learning_rate <span class="op">=</span> learning_rate</span>
<span id="cb32-13"><a href="#cb32-13" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb32-14"><a href="#cb32-14" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> sigmoid(<span class="va">self</span>, x):</span>
<span id="cb32-15"><a href="#cb32-15" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="dv">1</span> <span class="op">/</span> (<span class="dv">1</span> <span class="op">+</span> np.exp(<span class="op">-</span>np.clip(x, <span class="op">-</span><span class="dv">500</span>, <span class="dv">500</span>)))</span>
<span id="cb32-16"><a href="#cb32-16" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb32-17"><a href="#cb32-17" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> sigmoid_derivative(<span class="va">self</span>, x):</span>
<span id="cb32-18"><a href="#cb32-18" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x <span class="op">*</span> (<span class="dv">1</span> <span class="op">-</span> x)</span>
<span id="cb32-19"><a href="#cb32-19" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb32-20"><a href="#cb32-20" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, X):</span>
<span id="cb32-21"><a href="#cb32-21" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Forward pass through the network</span></span>
<span id="cb32-22"><a href="#cb32-22" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.z1 <span class="op">=</span> np.dot(X, <span class="va">self</span>.W1) <span class="op">+</span> <span class="va">self</span>.b1</span>
<span id="cb32-23"><a href="#cb32-23" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.a1 <span class="op">=</span> <span class="va">self</span>.sigmoid(<span class="va">self</span>.z1)</span>
<span id="cb32-24"><a href="#cb32-24" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.z2 <span class="op">=</span> np.dot(<span class="va">self</span>.a1, <span class="va">self</span>.W2) <span class="op">+</span> <span class="va">self</span>.b2</span>
<span id="cb32-25"><a href="#cb32-25" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.a2 <span class="op">=</span> <span class="va">self</span>.sigmoid(<span class="va">self</span>.z2)</span>
<span id="cb32-26"><a href="#cb32-26" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.a2</span>
<span id="cb32-27"><a href="#cb32-27" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb32-28"><a href="#cb32-28" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> backward(<span class="va">self</span>, X, y, output):</span>
<span id="cb32-29"><a href="#cb32-29" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Backward pass</span></span>
<span id="cb32-30"><a href="#cb32-30" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.output_error <span class="op">=</span> y <span class="op">-</span> output</span>
<span id="cb32-31"><a href="#cb32-31" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.output_delta <span class="op">=</span> <span class="va">self</span>.output_error <span class="op">*</span> <span class="va">self</span>.sigmoid_derivative(output)</span>
<span id="cb32-32"><a href="#cb32-32" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb32-33"><a href="#cb32-33" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.hidden_error <span class="op">=</span> <span class="va">self</span>.output_delta.dot(<span class="va">self</span>.W2.T)</span>
<span id="cb32-34"><a href="#cb32-34" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.hidden_delta <span class="op">=</span> <span class="va">self</span>.hidden_error <span class="op">*</span> <span class="va">self</span>.sigmoid_derivative(<span class="va">self</span>.a1)</span>
<span id="cb32-35"><a href="#cb32-35" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb32-36"><a href="#cb32-36" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Update weights</span></span>
<span id="cb32-37"><a href="#cb32-37" aria-hidden="true" tabindex="-1"></a>        m <span class="op">=</span> X.shape[<span class="dv">0</span>]  <span class="co"># number of examples</span></span>
<span id="cb32-38"><a href="#cb32-38" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.W2 <span class="op">+=</span> <span class="va">self</span>.a1.T.dot(<span class="va">self</span>.output_delta) <span class="op">*</span> <span class="va">self</span>.learning_rate <span class="op">/</span> m</span>
<span id="cb32-39"><a href="#cb32-39" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.b2 <span class="op">+=</span> np.<span class="bu">sum</span>(<span class="va">self</span>.output_delta, axis<span class="op">=</span><span class="dv">0</span>, keepdims<span class="op">=</span><span class="va">True</span>) <span class="op">*</span> <span class="va">self</span>.learning_rate <span class="op">/</span> m</span>
<span id="cb32-40"><a href="#cb32-40" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.W1 <span class="op">+=</span> X.T.dot(<span class="va">self</span>.hidden_delta) <span class="op">*</span> <span class="va">self</span>.learning_rate <span class="op">/</span> m</span>
<span id="cb32-41"><a href="#cb32-41" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.b1 <span class="op">+=</span> np.<span class="bu">sum</span>(<span class="va">self</span>.hidden_delta, axis<span class="op">=</span><span class="dv">0</span>, keepdims<span class="op">=</span><span class="va">True</span>) <span class="op">*</span> <span class="va">self</span>.learning_rate <span class="op">/</span> m</span>
<span id="cb32-42"><a href="#cb32-42" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb32-43"><a href="#cb32-43" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> train(<span class="va">self</span>, X, y, epochs<span class="op">=</span><span class="dv">10000</span>):</span>
<span id="cb32-44"><a href="#cb32-44" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(epochs):</span>
<span id="cb32-45"><a href="#cb32-45" aria-hidden="true" tabindex="-1"></a>            output <span class="op">=</span> <span class="va">self</span>.forward(X)</span>
<span id="cb32-46"><a href="#cb32-46" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.backward(X, y, output)</span>
<span id="cb32-47"><a href="#cb32-47" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb32-48"><a href="#cb32-48" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> epoch <span class="op">%</span> <span class="dv">1000</span> <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb32-49"><a href="#cb32-49" aria-hidden="true" tabindex="-1"></a>                loss <span class="op">=</span> np.mean(np.square(y <span class="op">-</span> output))</span>
<span id="cb32-50"><a href="#cb32-50" aria-hidden="true" tabindex="-1"></a>                <span class="bu">print</span>(<span class="ss">f"Epoch </span><span class="sc">{</span>epoch<span class="sc">}</span><span class="ss">, Loss: </span><span class="sc">{</span>loss<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb32-51"><a href="#cb32-51" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb32-52"><a href="#cb32-52" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> predict(<span class="va">self</span>, X):</span>
<span id="cb32-53"><a href="#cb32-53" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> (<span class="va">self</span>.forward(X) <span class="op">&gt;</span> <span class="fl">0.5</span>).astype(<span class="bu">int</span>)</span>
<span id="cb32-54"><a href="#cb32-54" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-55"><a href="#cb32-55" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate a moon-shaped dataset (non-linearly separable)</span></span>
<span id="cb32-56"><a href="#cb32-56" aria-hidden="true" tabindex="-1"></a>X, y <span class="op">=</span> make_moons(n_samples<span class="op">=</span><span class="dv">100</span>, noise<span class="op">=</span><span class="fl">0.1</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb32-57"><a href="#cb32-57" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> y.reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>)  <span class="co"># Reshape for matrix operations</span></span>
<span id="cb32-58"><a href="#cb32-58" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-59"><a href="#cb32-59" aria-hidden="true" tabindex="-1"></a><span class="co"># Create and train MLP</span></span>
<span id="cb32-60"><a href="#cb32-60" aria-hidden="true" tabindex="-1"></a>mlp <span class="op">=</span> MLP(input_size<span class="op">=</span><span class="dv">2</span>, hidden_size<span class="op">=</span><span class="dv">4</span>, output_size<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb32-61"><a href="#cb32-61" aria-hidden="true" tabindex="-1"></a>mlp.train(X, y, epochs<span class="op">=</span><span class="dv">10000</span>)</span>
<span id="cb32-62"><a href="#cb32-62" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-63"><a href="#cb32-63" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualize decision boundary</span></span>
<span id="cb32-64"><a href="#cb32-64" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> plot_decision_boundary(X, y, model):</span>
<span id="cb32-65"><a href="#cb32-65" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Create mesh grid</span></span>
<span id="cb32-66"><a href="#cb32-66" aria-hidden="true" tabindex="-1"></a>    h <span class="op">=</span> <span class="fl">0.02</span></span>
<span id="cb32-67"><a href="#cb32-67" aria-hidden="true" tabindex="-1"></a>    x_min, x_max <span class="op">=</span> X[:, <span class="dv">0</span>].<span class="bu">min</span>() <span class="op">-</span> <span class="dv">1</span>, X[:, <span class="dv">0</span>].<span class="bu">max</span>() <span class="op">+</span> <span class="dv">1</span></span>
<span id="cb32-68"><a href="#cb32-68" aria-hidden="true" tabindex="-1"></a>    y_min, y_max <span class="op">=</span> X[:, <span class="dv">1</span>].<span class="bu">min</span>() <span class="op">-</span> <span class="dv">1</span>, X[:, <span class="dv">1</span>].<span class="bu">max</span>() <span class="op">+</span> <span class="dv">1</span></span>
<span id="cb32-69"><a href="#cb32-69" aria-hidden="true" tabindex="-1"></a>    xx, yy <span class="op">=</span> np.meshgrid(np.arange(x_min, x_max, h),</span>
<span id="cb32-70"><a href="#cb32-70" aria-hidden="true" tabindex="-1"></a>                         np.arange(y_min, y_max, h))</span>
<span id="cb32-71"><a href="#cb32-71" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb32-72"><a href="#cb32-72" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Get predictions for mesh grid points</span></span>
<span id="cb32-73"><a href="#cb32-73" aria-hidden="true" tabindex="-1"></a>    Z <span class="op">=</span> model.predict(np.c_[xx.ravel(), yy.ravel()])</span>
<span id="cb32-74"><a href="#cb32-74" aria-hidden="true" tabindex="-1"></a>    Z <span class="op">=</span> Z.reshape(xx.shape)</span>
<span id="cb32-75"><a href="#cb32-75" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb32-76"><a href="#cb32-76" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Plot decision boundary</span></span>
<span id="cb32-77"><a href="#cb32-77" aria-hidden="true" tabindex="-1"></a>    plt.contourf(xx, yy, Z, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb32-78"><a href="#cb32-78" aria-hidden="true" tabindex="-1"></a>    plt.scatter(X[:, <span class="dv">0</span>], X[:, <span class="dv">1</span>], c<span class="op">=</span>y.reshape(<span class="op">-</span><span class="dv">1</span>), edgecolors<span class="op">=</span><span class="st">'k'</span>)</span>
<span id="cb32-79"><a href="#cb32-79" aria-hidden="true" tabindex="-1"></a>    plt.xlabel(<span class="st">'Feature 1'</span>)</span>
<span id="cb32-80"><a href="#cb32-80" aria-hidden="true" tabindex="-1"></a>    plt.ylabel(<span class="st">'Feature 2'</span>)</span>
<span id="cb32-81"><a href="#cb32-81" aria-hidden="true" tabindex="-1"></a>    plt.title(<span class="st">'MLP Decision Boundary on Moon Dataset'</span>)</span>
<span id="cb32-82"><a href="#cb32-82" aria-hidden="true" tabindex="-1"></a>    plt.show()</span>
<span id="cb32-83"><a href="#cb32-83" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-84"><a href="#cb32-84" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot decision boundary</span></span>
<span id="cb32-85"><a href="#cb32-85" aria-hidden="true" tabindex="-1"></a>plot_decision_boundary(X, y, mlp)</span>
<span id="cb32-86"><a href="#cb32-86" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-87"><a href="#cb32-87" aria-hidden="true" tabindex="-1"></a><span class="co"># Demonstrate XOR solution with MLP</span></span>
<span id="cb32-88"><a href="#cb32-88" aria-hidden="true" tabindex="-1"></a>X_xor <span class="op">=</span> np.array([[<span class="dv">0</span>, <span class="dv">0</span>], [<span class="dv">0</span>, <span class="dv">1</span>], [<span class="dv">1</span>, <span class="dv">0</span>], [<span class="dv">1</span>, <span class="dv">1</span>]])</span>
<span id="cb32-89"><a href="#cb32-89" aria-hidden="true" tabindex="-1"></a>y_xor <span class="op">=</span> np.array([[<span class="dv">0</span>], [<span class="dv">1</span>], [<span class="dv">1</span>], [<span class="dv">0</span>]])</span>
<span id="cb32-90"><a href="#cb32-90" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-91"><a href="#cb32-91" aria-hidden="true" tabindex="-1"></a>mlp_xor <span class="op">=</span> MLP(input_size<span class="op">=</span><span class="dv">2</span>, hidden_size<span class="op">=</span><span class="dv">2</span>, output_size<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb32-92"><a href="#cb32-92" aria-hidden="true" tabindex="-1"></a>mlp_xor.train(X_xor, y_xor, epochs<span class="op">=</span><span class="dv">10000</span>)</span>
<span id="cb32-93"><a href="#cb32-93" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-94"><a href="#cb32-94" aria-hidden="true" tabindex="-1"></a><span class="co"># Show XOR predictions</span></span>
<span id="cb32-95"><a href="#cb32-95" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">XOR Problem Predictions:"</span>)</span>
<span id="cb32-96"><a href="#cb32-96" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Input | Target | Prediction"</span>)</span>
<span id="cb32-97"><a href="#cb32-97" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"-"</span> <span class="op">*</span> <span class="dv">30</span>)</span>
<span id="cb32-98"><a href="#cb32-98" aria-hidden="true" tabindex="-1"></a>predictions <span class="op">=</span> mlp_xor.predict(X_xor)</span>
<span id="cb32-99"><a href="#cb32-99" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(X_xor)):</span>
<span id="cb32-100"><a href="#cb32-100" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span>X_xor[i]<span class="sc">}</span><span class="ss"> | </span><span class="sc">{</span>y_xor[i][<span class="dv">0</span>]<span class="sc">}</span><span class="ss">      | </span><span class="sc">{</span>predictions[i][<span class="dv">0</span>]<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 0, Loss: 0.25001138785301963
Epoch 1000, Loss: 0.2497513119269541
Epoch 2000, Loss: 0.24126218462511925
Epoch 3000, Loss: 0.16082747083207807
Epoch 4000, Loss: 0.11387645348530671
Epoch 5000, Loss: 0.09915008034892134
Epoch 6000, Loss: 0.0933089299783808
Epoch 7000, Loss: 0.09089055434391609
Epoch 8000, Loss: 0.08979520363944743
Epoch 9000, Loss: 0.08925172959696549</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="chapter9_files/figure-html/cell-22-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 0, Loss: 0.25000509448111063
Epoch 1000, Loss: 0.25000000001448863
Epoch 2000, Loss: 0.25000000001445216
Epoch 3000, Loss: 0.2500000000144492
Epoch 4000, Loss: 0.2500000000144462
Epoch 5000, Loss: 0.2500000000144433
Epoch 6000, Loss: 0.25000000001444034
Epoch 7000, Loss: 0.2500000000144374
Epoch 8000, Loss: 0.2500000000144344
Epoch 9000, Loss: 0.25000000001443146

XOR Problem Predictions:
Input | Target | Prediction
------------------------------
[0 0] | 0      | 1
[0 1] | 1      | 0
[1 0] | 1      | 1
[1 1] | 0      | 0</code></pre>
</div>
</div>
</section>
<section id="using-mlps-with-scikit-learn" class="level3" data-number="13.1.8">
<h3 data-number="13.1.8" class="anchored" data-anchor-id="using-mlps-with-scikit-learn"><span class="header-section-number">13.1.8</span> Using MLPs with Scikit-Learn</h3>
<p>Scikit-learn provides a convenient implementation of MLPs through the <code>MLPClassifier</code> class:</p>
<div id="134b74f1" class="cell" data-execution_count="22">
<div class="sourceCode cell-code" id="cb35"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.neural_network <span class="im">import</span> MLPClassifier</span>
<span id="cb35-2"><a href="#cb35-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.datasets <span class="im">import</span> make_moons</span>
<span id="cb35-3"><a href="#cb35-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb35-4"><a href="#cb35-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> accuracy_score</span>
<span id="cb35-5"><a href="#cb35-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-6"><a href="#cb35-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate non-linear dataset</span></span>
<span id="cb35-7"><a href="#cb35-7" aria-hidden="true" tabindex="-1"></a>X, y <span class="op">=</span> make_moons(n_samples<span class="op">=</span><span class="dv">1000</span>, noise<span class="op">=</span><span class="fl">0.2</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb35-8"><a href="#cb35-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-9"><a href="#cb35-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Split into training and test sets</span></span>
<span id="cb35-10"><a href="#cb35-10" aria-hidden="true" tabindex="-1"></a>X_train, X_test, y_train, y_test <span class="op">=</span> train_test_split(X, y, test_size<span class="op">=</span><span class="fl">0.2</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb35-11"><a href="#cb35-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-12"><a href="#cb35-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Create and train MLP</span></span>
<span id="cb35-13"><a href="#cb35-13" aria-hidden="true" tabindex="-1"></a>mlp <span class="op">=</span> MLPClassifier(hidden_layer_sizes<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">5</span>), activation<span class="op">=</span><span class="st">'relu'</span>, </span>
<span id="cb35-14"><a href="#cb35-14" aria-hidden="true" tabindex="-1"></a>                    max_iter<span class="op">=</span><span class="dv">1000</span>, alpha<span class="op">=</span><span class="fl">0.0001</span>,</span>
<span id="cb35-15"><a href="#cb35-15" aria-hidden="true" tabindex="-1"></a>                    solver<span class="op">=</span><span class="st">'adam'</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb35-16"><a href="#cb35-16" aria-hidden="true" tabindex="-1"></a>mlp.fit(X_train, y_train)</span>
<span id="cb35-17"><a href="#cb35-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-18"><a href="#cb35-18" aria-hidden="true" tabindex="-1"></a><span class="co"># Evaluate</span></span>
<span id="cb35-19"><a href="#cb35-19" aria-hidden="true" tabindex="-1"></a>y_pred <span class="op">=</span> mlp.predict(X_test)</span>
<span id="cb35-20"><a href="#cb35-20" aria-hidden="true" tabindex="-1"></a>accuracy <span class="op">=</span> accuracy_score(y_test, y_pred)</span>
<span id="cb35-21"><a href="#cb35-21" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Test accuracy: </span><span class="sc">{</span>accuracy<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb35-22"><a href="#cb35-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-23"><a href="#cb35-23" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualize decision boundary</span></span>
<span id="cb35-24"><a href="#cb35-24" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">6</span>))</span>
<span id="cb35-25"><a href="#cb35-25" aria-hidden="true" tabindex="-1"></a>plot_decision_boundary(X, y, mlp)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Test accuracy: 0.9700</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="chapter9_files/figure-html/cell-23-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="mlp-applications" class="level3" data-number="13.1.9">
<h3 data-number="13.1.9" class="anchored" data-anchor-id="mlp-applications"><span class="header-section-number">13.1.9</span> MLP Applications</h3>
<p>MLPs are versatile and can be applied to various tasks:</p>
<ol type="1">
<li><strong>Classification</strong>: Identifying categories (e.g., digit recognition)</li>
<li><strong>Regression</strong>: Predicting continuous values (e.g., house prices)</li>
<li><strong>Pattern Recognition</strong>: Detecting patterns in data</li>
<li><strong>Function Approximation</strong>: Learning complex mathematical functions</li>
</ol>
</section>
<section id="advantages-and-disadvantages-of-mlps" class="level3" data-number="13.1.10">
<h3 data-number="13.1.10" class="anchored" data-anchor-id="advantages-and-disadvantages-of-mlps"><span class="header-section-number">13.1.10</span> Advantages and Disadvantages of MLPs</h3>
<p><strong>Advantages:</strong> - Can learn non-linear patterns - Versatile and applicable to many problems - Form the foundation for deeper neural networks</p>
<p><strong>Disadvantages:</strong> - Prone to local minima during training - Sensitive to feature scaling - Require careful hyperparameter tuning - May overfit with insufficient data</p>
</section>
</section>
<section id="summary-from-tlus-to-deep-learning" class="level2" data-number="13.2">
<h2 data-number="13.2" class="anchored" data-anchor-id="summary-from-tlus-to-deep-learning"><span class="header-section-number">13.2</span> Summary: From TLUs to Deep Learning</h2>
<p>The progression from TLUs to Perceptrons to MLPs represents the foundational evolution of neural networks:</p>
<ol type="1">
<li><strong>TLUs</strong> introduced the concept of a thresholding artificial neuron</li>
<li><strong>Perceptrons</strong> added the ability to learn from data but were limited to linear problems</li>
<li><strong>MLPs</strong> overcame these limitations by stacking multiple layers, enabling non-linear learning</li>
</ol>
<p>These concepts form the foundation of modern deep learning architectures like Convolutional Neural Networks (CNNs), Recurrent Neural Networks (RNNs), and Transformers.</p>
</section>
<section id="feed-forward-neural-networks" class="level2" data-number="13.3">
<h2 data-number="13.3" class="anchored" data-anchor-id="feed-forward-neural-networks"><span class="header-section-number">13.3</span> Feed-Forward Neural Networks</h2>
<p>Feed-forward neural networks are the foundation of deep learning architectures. Unlike recurrent or graph-based neural networks, feed-forward networks have a simple, unidirectional flow of information.</p>
<section id="key-characteristics" class="level3" data-number="13.3.1">
<h3 data-number="13.3.1" class="anchored" data-anchor-id="key-characteristics"><span class="header-section-number">13.3.1</span> Key Characteristics</h3>
<ul>
<li>Information flows in a single direction: from input to output through hidden layers</li>
<li>No cycles or loops in the network structure</li>
<li>Each neuron in a layer connects to every neuron in the subsequent layer</li>
<li>The network makes predictions by propagating signals forward through the network</li>
</ul>
</section>
<section id="understanding-feed-forward-networks-the-assembly-line-analogy" class="level3" data-number="13.3.2">
<h3 data-number="13.3.2" class="anchored" data-anchor-id="understanding-feed-forward-networks-the-assembly-line-analogy"><span class="header-section-number">13.3.2</span> Understanding Feed-Forward Networks: The Assembly Line Analogy</h3>
<p>Think of a feed-forward neural network as an assembly line in a factory:</p>
<ol type="1">
<li><strong>Raw materials</strong> (input data) enter at the beginning of the line</li>
<li><strong>Workers at different stations</strong> (neurons in hidden layers) process and transform the materials</li>
<li>Each station adds value and passes materials to the next station</li>
<li><strong>Final inspection</strong> (output layer) produces the finished product (prediction)</li>
</ol>
<p>Like a manufacturing assembly line, there’s no going backward - materials only move forward. Each worker makes decisions based solely on what they receive from the previous station, not from stations further ahead.</p>
</section>
<section id="mathematical-representation" class="level3" data-number="13.3.3">
<h3 data-number="13.3.3" class="anchored" data-anchor-id="mathematical-representation"><span class="header-section-number">13.3.3</span> Mathematical Representation</h3>
<p>A simple feed-forward network with one hidden layer can be represented as:</p>
<p><span class="math display">\[z^{(1)} = W^{(1)}x + b^{(1)}\]</span> <span class="math display">\[a^{(1)} = \sigma(z^{(1)})\]</span> <span class="math display">\[z^{(2)} = W^{(2)}a^{(1)} + b^{(2)}\]</span> <span class="math display">\[\hat{y} = \sigma(z^{(2)})\]</span></p>
<p>Where: - <span class="math inline">\(x\)</span> is the input vector - <span class="math inline">\(W^{(1)}\)</span> and <span class="math inline">\(W^{(2)}\)</span> are weight matrices - <span class="math inline">\(b^{(1)}\)</span> and <span class="math inline">\(b^{(2)}\)</span> are bias vectors - <span class="math inline">\(\sigma\)</span> is an activation function (commonly sigmoid, tanh, or ReLU) - <span class="math inline">\(\hat{y}\)</span> is the predicted output</p>
</section>
<section id="python-implementation-of-feed-forward-network" class="level3" data-number="13.3.4">
<h3 data-number="13.3.4" class="anchored" data-anchor-id="python-implementation-of-feed-forward-network"><span class="header-section-number">13.3.4</span> Python Implementation of Feed-Forward Network</h3>
<p>Here’s a simple implementation of a feed-forward neural network using NumPy:</p>
<div id="28b1605c" class="cell" data-execution_count="23">
<div class="sourceCode cell-code" id="cb37"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb37-2"><a href="#cb37-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-3"><a href="#cb37-3" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> FeedForwardNetwork:</span>
<span id="cb37-4"><a href="#cb37-4" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, input_size, hidden_size, output_size):</span>
<span id="cb37-5"><a href="#cb37-5" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Initialize weights with small random values</span></span>
<span id="cb37-6"><a href="#cb37-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.W1 <span class="op">=</span> np.random.randn(input_size, hidden_size) <span class="op">*</span> <span class="fl">0.01</span></span>
<span id="cb37-7"><a href="#cb37-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.b1 <span class="op">=</span> np.zeros((<span class="dv">1</span>, hidden_size))</span>
<span id="cb37-8"><a href="#cb37-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.W2 <span class="op">=</span> np.random.randn(hidden_size, output_size) <span class="op">*</span> <span class="fl">0.01</span></span>
<span id="cb37-9"><a href="#cb37-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.b2 <span class="op">=</span> np.zeros((<span class="dv">1</span>, output_size))</span>
<span id="cb37-10"><a href="#cb37-10" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb37-11"><a href="#cb37-11" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> sigmoid(<span class="va">self</span>, x):</span>
<span id="cb37-12"><a href="#cb37-12" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Sigmoid activation function"""</span></span>
<span id="cb37-13"><a href="#cb37-13" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="dv">1</span> <span class="op">/</span> (<span class="dv">1</span> <span class="op">+</span> np.exp(<span class="op">-</span>x))</span>
<span id="cb37-14"><a href="#cb37-14" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb37-15"><a href="#cb37-15" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, X):</span>
<span id="cb37-16"><a href="#cb37-16" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Forward pass through the network"""</span></span>
<span id="cb37-17"><a href="#cb37-17" aria-hidden="true" tabindex="-1"></a>        <span class="co"># First layer</span></span>
<span id="cb37-18"><a href="#cb37-18" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.z1 <span class="op">=</span> np.dot(X, <span class="va">self</span>.W1) <span class="op">+</span> <span class="va">self</span>.b1</span>
<span id="cb37-19"><a href="#cb37-19" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.a1 <span class="op">=</span> <span class="va">self</span>.sigmoid(<span class="va">self</span>.z1)</span>
<span id="cb37-20"><a href="#cb37-20" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb37-21"><a href="#cb37-21" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Output layer</span></span>
<span id="cb37-22"><a href="#cb37-22" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.z2 <span class="op">=</span> np.dot(<span class="va">self</span>.a1, <span class="va">self</span>.W2) <span class="op">+</span> <span class="va">self</span>.b2</span>
<span id="cb37-23"><a href="#cb37-23" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.a2 <span class="op">=</span> <span class="va">self</span>.sigmoid(<span class="va">self</span>.z2)</span>
<span id="cb37-24"><a href="#cb37-24" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb37-25"><a href="#cb37-25" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.a2</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="example-binary-classification-with-feed-forward-network" class="level3" data-number="13.3.5">
<h3 data-number="13.3.5" class="anchored" data-anchor-id="example-binary-classification-with-feed-forward-network"><span class="header-section-number">13.3.5</span> Example: Binary Classification with Feed-Forward Network</h3>
<p>Let’s use our feed-forward network to solve a simple binary classification problem:</p>
<div id="8f3c663c" class="cell" data-execution_count="24">
<div class="sourceCode cell-code" id="cb38"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb38-1"><a href="#cb38-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb38-2"><a href="#cb38-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb38-3"><a href="#cb38-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.datasets <span class="im">import</span> make_moons</span>
<span id="cb38-4"><a href="#cb38-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb38-5"><a href="#cb38-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> accuracy_score</span>
<span id="cb38-6"><a href="#cb38-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-7"><a href="#cb38-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate a non-linear dataset</span></span>
<span id="cb38-8"><a href="#cb38-8" aria-hidden="true" tabindex="-1"></a>X, y <span class="op">=</span> make_moons(n_samples<span class="op">=</span><span class="dv">1000</span>, noise<span class="op">=</span><span class="fl">0.2</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb38-9"><a href="#cb38-9" aria-hidden="true" tabindex="-1"></a>X_train, X_test, y_train, y_test <span class="op">=</span> train_test_split(X, y, test_size<span class="op">=</span><span class="fl">0.2</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb38-10"><a href="#cb38-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-11"><a href="#cb38-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Initialize our network</span></span>
<span id="cb38-12"><a href="#cb38-12" aria-hidden="true" tabindex="-1"></a>input_size <span class="op">=</span> <span class="dv">2</span>  <span class="co"># Two features</span></span>
<span id="cb38-13"><a href="#cb38-13" aria-hidden="true" tabindex="-1"></a>hidden_size <span class="op">=</span> <span class="dv">5</span>  <span class="co"># Five neurons in hidden layer</span></span>
<span id="cb38-14"><a href="#cb38-14" aria-hidden="true" tabindex="-1"></a>output_size <span class="op">=</span> <span class="dv">1</span>  <span class="co"># Binary classification</span></span>
<span id="cb38-15"><a href="#cb38-15" aria-hidden="true" tabindex="-1"></a>network <span class="op">=</span> FeedForwardNetwork(input_size, hidden_size, output_size)</span>
<span id="cb38-16"><a href="#cb38-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-17"><a href="#cb38-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Make predictions</span></span>
<span id="cb38-18"><a href="#cb38-18" aria-hidden="true" tabindex="-1"></a>predictions <span class="op">=</span> network.forward(X_test)</span>
<span id="cb38-19"><a href="#cb38-19" aria-hidden="true" tabindex="-1"></a>binary_predictions <span class="op">=</span> (predictions <span class="op">&gt;</span> <span class="fl">0.5</span>).astype(<span class="bu">int</span>)</span>
<span id="cb38-20"><a href="#cb38-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-21"><a href="#cb38-21" aria-hidden="true" tabindex="-1"></a><span class="co"># Check initial accuracy (should be close to random guessing)</span></span>
<span id="cb38-22"><a href="#cb38-22" aria-hidden="true" tabindex="-1"></a>accuracy <span class="op">=</span> accuracy_score(y_test, binary_predictions)</span>
<span id="cb38-23"><a href="#cb38-23" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Initial accuracy: </span><span class="sc">{</span>accuracy<span class="sc">:.2f}</span><span class="ss">"</span>)</span>
<span id="cb38-24"><a href="#cb38-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-25"><a href="#cb38-25" aria-hidden="true" tabindex="-1"></a><span class="co"># Note: The network needs training to improve accuracy</span></span>
<span id="cb38-26"><a href="#cb38-26" aria-hidden="true" tabindex="-1"></a><span class="co"># We'll implement training with backpropagation in the next section</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Initial accuracy: 0.50</code></pre>
</div>
</div>
</section>
</section>
<section id="backpropagation" class="level2" data-number="13.4">
<h2 data-number="13.4" class="anchored" data-anchor-id="backpropagation"><span class="header-section-number">13.4</span> Backpropagation</h2>
<p>Backpropagation is the key algorithm that enables neural networks to learn. It’s a method for computing gradients efficiently in feed-forward networks, which are then used to update the weights.</p>
<section id="the-learning-process-backpropagation-explained" class="level3" data-number="13.4.1">
<h3 data-number="13.4.1" class="anchored" data-anchor-id="the-learning-process-backpropagation-explained"><span class="header-section-number">13.4.1</span> The Learning Process: Backpropagation Explained</h3>
<p>Backpropagation works in three steps:</p>
<ol type="1">
<li><strong>Forward Pass</strong>: The network makes a prediction given an input</li>
<li><strong>Error Calculation</strong>: The error between the prediction and actual target is measured</li>
<li><strong>Backward Pass</strong>: The error is propagated backward through the network to adjust weights</li>
</ol>
</section>
<section id="the-mail-delivery-analogy" class="level3" data-number="13.4.2">
<h3 data-number="13.4.2" class="anchored" data-anchor-id="the-mail-delivery-analogy"><span class="header-section-number">13.4.2</span> The Mail Delivery Analogy</h3>
<p>Think of backpropagation like a postal service with feedback:</p>
<ol type="1">
<li>You send a package (input) to a destination (output) through multiple sorting centers (hidden layers)</li>
<li>When the package arrives, the recipient checks if it’s correct (error calculation)</li>
<li>The recipient sends feedback about the error back through the same route</li>
<li>Each sorting center learns from this feedback and adjusts its processing (weight updates)</li>
<li>Over time, the entire system improves its delivery accuracy</li>
</ol>
</section>
<section id="mathematical-framework" class="level3" data-number="13.4.3">
<h3 data-number="13.4.3" class="anchored" data-anchor-id="mathematical-framework"><span class="header-section-number">13.4.3</span> Mathematical Framework</h3>
<p>For a network with one hidden layer:</p>
<ol type="1">
<li><strong>Forward Pass</strong>: Calculate predicted output <span class="math inline">\(\hat{y}\)</span> as shown earlier</li>
<li><strong>Calculate Loss</strong>: Using a loss function like Mean Squared Error (MSE): <span class="math inline">\(L = \frac{1}{2}(y - \hat{y})^2\)</span></li>
<li><strong>Backward Pass</strong>: Compute gradients using the chain rule
<ul>
<li><span class="math inline">\(\frac{\partial L}{\partial W^{(2)}} = \frac{\partial L}{\partial \hat{y}} \cdot \frac{\partial \hat{y}}{\partial z^{(2)}} \cdot \frac{\partial z^{(2)}}{\partial W^{(2)}}\)</span></li>
<li><span class="math inline">\(\frac{\partial L}{\partial W^{(1)}} = \frac{\partial L}{\partial \hat{y}} \cdot \frac{\partial \hat{y}}{\partial z^{(2)}} \cdot \frac{\partial z^{(2)}}{\partial a^{(1)}} \cdot \frac{\partial a^{(1)}}{\partial z^{(1)}} \cdot \frac{\partial z^{(1)}}{\partial W^{(1)}}\)</span></li>
</ul></li>
<li><strong>Update Weights</strong>: <span class="math inline">\(W^{(l)} = W^{(l)} - \alpha \cdot \frac{\partial L}{\partial W^{(l)}}\)</span>, where <span class="math inline">\(\alpha\)</span> is the learning rate</li>
</ol>
</section>
<section id="implementing-backpropagation" class="level3" data-number="13.4.4">
<h3 data-number="13.4.4" class="anchored" data-anchor-id="implementing-backpropagation"><span class="header-section-number">13.4.4</span> Implementing Backpropagation</h3>
<p>Let’s extend our FeedForwardNetwork class to include backpropagation:</p>
<div id="5e3d94ad" class="cell" data-execution_count="25">
<div class="sourceCode cell-code" id="cb40"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb40-1"><a href="#cb40-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> FeedForwardNetwork:</span>
<span id="cb40-2"><a href="#cb40-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, input_size, hidden_size, output_size, learning_rate<span class="op">=</span><span class="fl">0.01</span>):</span>
<span id="cb40-3"><a href="#cb40-3" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Initialize weights with small random values</span></span>
<span id="cb40-4"><a href="#cb40-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.W1 <span class="op">=</span> np.random.randn(input_size, hidden_size) <span class="op">*</span> <span class="fl">0.01</span></span>
<span id="cb40-5"><a href="#cb40-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.b1 <span class="op">=</span> np.zeros((<span class="dv">1</span>, hidden_size))</span>
<span id="cb40-6"><a href="#cb40-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.W2 <span class="op">=</span> np.random.randn(hidden_size, output_size) <span class="op">*</span> <span class="fl">0.01</span></span>
<span id="cb40-7"><a href="#cb40-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.b2 <span class="op">=</span> np.zeros((<span class="dv">1</span>, output_size))</span>
<span id="cb40-8"><a href="#cb40-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.learning_rate <span class="op">=</span> learning_rate</span>
<span id="cb40-9"><a href="#cb40-9" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb40-10"><a href="#cb40-10" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> sigmoid(<span class="va">self</span>, x):</span>
<span id="cb40-11"><a href="#cb40-11" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Sigmoid activation function"""</span></span>
<span id="cb40-12"><a href="#cb40-12" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="dv">1</span> <span class="op">/</span> (<span class="dv">1</span> <span class="op">+</span> np.exp(<span class="op">-</span>x))</span>
<span id="cb40-13"><a href="#cb40-13" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb40-14"><a href="#cb40-14" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> sigmoid_derivative(<span class="va">self</span>, x):</span>
<span id="cb40-15"><a href="#cb40-15" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Derivative of sigmoid function"""</span></span>
<span id="cb40-16"><a href="#cb40-16" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x <span class="op">*</span> (<span class="dv">1</span> <span class="op">-</span> x)</span>
<span id="cb40-17"><a href="#cb40-17" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb40-18"><a href="#cb40-18" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, X):</span>
<span id="cb40-19"><a href="#cb40-19" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Forward pass through the network"""</span></span>
<span id="cb40-20"><a href="#cb40-20" aria-hidden="true" tabindex="-1"></a>        <span class="co"># First layer</span></span>
<span id="cb40-21"><a href="#cb40-21" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.z1 <span class="op">=</span> np.dot(X, <span class="va">self</span>.W1) <span class="op">+</span> <span class="va">self</span>.b1</span>
<span id="cb40-22"><a href="#cb40-22" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.a1 <span class="op">=</span> <span class="va">self</span>.sigmoid(<span class="va">self</span>.z1)</span>
<span id="cb40-23"><a href="#cb40-23" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb40-24"><a href="#cb40-24" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Output layer</span></span>
<span id="cb40-25"><a href="#cb40-25" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.z2 <span class="op">=</span> np.dot(<span class="va">self</span>.a1, <span class="va">self</span>.W2) <span class="op">+</span> <span class="va">self</span>.b2</span>
<span id="cb40-26"><a href="#cb40-26" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.a2 <span class="op">=</span> <span class="va">self</span>.sigmoid(<span class="va">self</span>.z2)</span>
<span id="cb40-27"><a href="#cb40-27" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb40-28"><a href="#cb40-28" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.a2</span>
<span id="cb40-29"><a href="#cb40-29" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb40-30"><a href="#cb40-30" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> backward(<span class="va">self</span>, X, y, output):</span>
<span id="cb40-31"><a href="#cb40-31" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Backward pass to update weights"""</span></span>
<span id="cb40-32"><a href="#cb40-32" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Calculate error</span></span>
<span id="cb40-33"><a href="#cb40-33" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.output_error <span class="op">=</span> y <span class="op">-</span> output</span>
<span id="cb40-34"><a href="#cb40-34" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.output_delta <span class="op">=</span> <span class="va">self</span>.output_error <span class="op">*</span> <span class="va">self</span>.sigmoid_derivative(output)</span>
<span id="cb40-35"><a href="#cb40-35" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb40-36"><a href="#cb40-36" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Calculate hidden layer error</span></span>
<span id="cb40-37"><a href="#cb40-37" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.hidden_error <span class="op">=</span> <span class="va">self</span>.output_delta.dot(<span class="va">self</span>.W2.T)</span>
<span id="cb40-38"><a href="#cb40-38" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.hidden_delta <span class="op">=</span> <span class="va">self</span>.hidden_error <span class="op">*</span> <span class="va">self</span>.sigmoid_derivative(<span class="va">self</span>.a1)</span>
<span id="cb40-39"><a href="#cb40-39" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb40-40"><a href="#cb40-40" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Update weights</span></span>
<span id="cb40-41"><a href="#cb40-41" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.W2 <span class="op">+=</span> <span class="va">self</span>.a1.T.dot(<span class="va">self</span>.output_delta) <span class="op">*</span> <span class="va">self</span>.learning_rate</span>
<span id="cb40-42"><a href="#cb40-42" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.b2 <span class="op">+=</span> np.<span class="bu">sum</span>(<span class="va">self</span>.output_delta, axis<span class="op">=</span><span class="dv">0</span>, keepdims<span class="op">=</span><span class="va">True</span>) <span class="op">*</span> <span class="va">self</span>.learning_rate</span>
<span id="cb40-43"><a href="#cb40-43" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.W1 <span class="op">+=</span> X.T.dot(<span class="va">self</span>.hidden_delta) <span class="op">*</span> <span class="va">self</span>.learning_rate</span>
<span id="cb40-44"><a href="#cb40-44" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.b1 <span class="op">+=</span> np.<span class="bu">sum</span>(<span class="va">self</span>.hidden_delta, axis<span class="op">=</span><span class="dv">0</span>, keepdims<span class="op">=</span><span class="va">True</span>) <span class="op">*</span> <span class="va">self</span>.learning_rate</span>
<span id="cb40-45"><a href="#cb40-45" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb40-46"><a href="#cb40-46" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> train(<span class="va">self</span>, X, y, epochs<span class="op">=</span><span class="dv">1000</span>):</span>
<span id="cb40-47"><a href="#cb40-47" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Train the network"""</span></span>
<span id="cb40-48"><a href="#cb40-48" aria-hidden="true" tabindex="-1"></a>        losses <span class="op">=</span> []</span>
<span id="cb40-49"><a href="#cb40-49" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb40-50"><a href="#cb40-50" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(epochs):</span>
<span id="cb40-51"><a href="#cb40-51" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Forward pass</span></span>
<span id="cb40-52"><a href="#cb40-52" aria-hidden="true" tabindex="-1"></a>            output <span class="op">=</span> <span class="va">self</span>.forward(X)</span>
<span id="cb40-53"><a href="#cb40-53" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb40-54"><a href="#cb40-54" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Calculate loss</span></span>
<span id="cb40-55"><a href="#cb40-55" aria-hidden="true" tabindex="-1"></a>            loss <span class="op">=</span> np.mean(np.square(y <span class="op">-</span> output))</span>
<span id="cb40-56"><a href="#cb40-56" aria-hidden="true" tabindex="-1"></a>            losses.append(loss)</span>
<span id="cb40-57"><a href="#cb40-57" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb40-58"><a href="#cb40-58" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Backward pass</span></span>
<span id="cb40-59"><a href="#cb40-59" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.backward(X, y, output)</span>
<span id="cb40-60"><a href="#cb40-60" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb40-61"><a href="#cb40-61" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Print progress</span></span>
<span id="cb40-62"><a href="#cb40-62" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> epoch <span class="op">%</span> <span class="dv">100</span> <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb40-63"><a href="#cb40-63" aria-hidden="true" tabindex="-1"></a>                <span class="bu">print</span>(<span class="ss">f"Epoch </span><span class="sc">{</span>epoch<span class="sc">}</span><span class="ss">, Loss: </span><span class="sc">{</span>loss<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb40-64"><a href="#cb40-64" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb40-65"><a href="#cb40-65" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> losses</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="complete-example-training-a-network-with-backpropagation" class="level3" data-number="13.4.5">
<h3 data-number="13.4.5" class="anchored" data-anchor-id="complete-example-training-a-network-with-backpropagation"><span class="header-section-number">13.4.5</span> Complete Example: Training a Network with Backpropagation</h3>
<p>Let’s use our implementation to train a network on a non-linear classification problem:</p>
<div id="6bffb5ce" class="cell" data-execution_count="26">
<div class="sourceCode cell-code" id="cb41"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb41-1"><a href="#cb41-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb41-2"><a href="#cb41-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb41-3"><a href="#cb41-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.datasets <span class="im">import</span> make_moons</span>
<span id="cb41-4"><a href="#cb41-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb41-5"><a href="#cb41-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> accuracy_score</span>
<span id="cb41-6"><a href="#cb41-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-7"><a href="#cb41-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate a non-linear dataset</span></span>
<span id="cb41-8"><a href="#cb41-8" aria-hidden="true" tabindex="-1"></a>X, y <span class="op">=</span> make_moons(n_samples<span class="op">=</span><span class="dv">1000</span>, noise<span class="op">=</span><span class="fl">0.2</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb41-9"><a href="#cb41-9" aria-hidden="true" tabindex="-1"></a>X_train, X_test, y_train, y_test <span class="op">=</span> train_test_split(X, y, test_size<span class="op">=</span><span class="fl">0.2</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb41-10"><a href="#cb41-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-11"><a href="#cb41-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Reshape y for training (to match network output)</span></span>
<span id="cb41-12"><a href="#cb41-12" aria-hidden="true" tabindex="-1"></a>y_train <span class="op">=</span> y_train.reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb41-13"><a href="#cb41-13" aria-hidden="true" tabindex="-1"></a>y_test <span class="op">=</span> y_test.reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb41-14"><a href="#cb41-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-15"><a href="#cb41-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Initialize our network</span></span>
<span id="cb41-16"><a href="#cb41-16" aria-hidden="true" tabindex="-1"></a>input_size <span class="op">=</span> <span class="dv">2</span>  <span class="co"># Two features</span></span>
<span id="cb41-17"><a href="#cb41-17" aria-hidden="true" tabindex="-1"></a>hidden_size <span class="op">=</span> <span class="dv">5</span>  <span class="co"># Five neurons in hidden layer</span></span>
<span id="cb41-18"><a href="#cb41-18" aria-hidden="true" tabindex="-1"></a>output_size <span class="op">=</span> <span class="dv">1</span>  <span class="co"># Binary classification</span></span>
<span id="cb41-19"><a href="#cb41-19" aria-hidden="true" tabindex="-1"></a>network <span class="op">=</span> FeedForwardNetwork(input_size, hidden_size, output_size, learning_rate<span class="op">=</span><span class="fl">0.1</span>)</span>
<span id="cb41-20"><a href="#cb41-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-21"><a href="#cb41-21" aria-hidden="true" tabindex="-1"></a><span class="co"># Train the network</span></span>
<span id="cb41-22"><a href="#cb41-22" aria-hidden="true" tabindex="-1"></a>losses <span class="op">=</span> network.train(X_train, y_train, epochs<span class="op">=</span><span class="dv">2000</span>)</span>
<span id="cb41-23"><a href="#cb41-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-24"><a href="#cb41-24" aria-hidden="true" tabindex="-1"></a><span class="co"># Evaluate the trained network</span></span>
<span id="cb41-25"><a href="#cb41-25" aria-hidden="true" tabindex="-1"></a>predictions <span class="op">=</span> network.forward(X_test)</span>
<span id="cb41-26"><a href="#cb41-26" aria-hidden="true" tabindex="-1"></a>binary_predictions <span class="op">=</span> (predictions <span class="op">&gt;</span> <span class="fl">0.5</span>).astype(<span class="bu">int</span>)</span>
<span id="cb41-27"><a href="#cb41-27" aria-hidden="true" tabindex="-1"></a>accuracy <span class="op">=</span> accuracy_score(y_test, binary_predictions)</span>
<span id="cb41-28"><a href="#cb41-28" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Final accuracy: </span><span class="sc">{</span>accuracy<span class="sc">:.2f}</span><span class="ss">"</span>)</span>
<span id="cb41-29"><a href="#cb41-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-30"><a href="#cb41-30" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualize the decision boundary</span></span>
<span id="cb41-31"><a href="#cb41-31" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">8</span>))</span>
<span id="cb41-32"><a href="#cb41-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-33"><a href="#cb41-33" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot the training data</span></span>
<span id="cb41-34"><a href="#cb41-34" aria-hidden="true" tabindex="-1"></a>plt.scatter(X[:, <span class="dv">0</span>], X[:, <span class="dv">1</span>], c<span class="op">=</span>y, cmap<span class="op">=</span>plt.cm.Spectral, edgecolors<span class="op">=</span><span class="st">'k'</span>)</span>
<span id="cb41-35"><a href="#cb41-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-36"><a href="#cb41-36" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a mesh grid to visualize the decision boundary</span></span>
<span id="cb41-37"><a href="#cb41-37" aria-hidden="true" tabindex="-1"></a>h <span class="op">=</span> <span class="fl">0.01</span></span>
<span id="cb41-38"><a href="#cb41-38" aria-hidden="true" tabindex="-1"></a>x_min, x_max <span class="op">=</span> X[:, <span class="dv">0</span>].<span class="bu">min</span>() <span class="op">-</span> <span class="dv">1</span>, X[:, <span class="dv">0</span>].<span class="bu">max</span>() <span class="op">+</span> <span class="dv">1</span></span>
<span id="cb41-39"><a href="#cb41-39" aria-hidden="true" tabindex="-1"></a>y_min, y_max <span class="op">=</span> X[:, <span class="dv">1</span>].<span class="bu">min</span>() <span class="op">-</span> <span class="dv">1</span>, X[:, <span class="dv">1</span>].<span class="bu">max</span>() <span class="op">+</span> <span class="dv">1</span></span>
<span id="cb41-40"><a href="#cb41-40" aria-hidden="true" tabindex="-1"></a>xx, yy <span class="op">=</span> np.meshgrid(np.arange(x_min, x_max, h),</span>
<span id="cb41-41"><a href="#cb41-41" aria-hidden="true" tabindex="-1"></a>                     np.arange(y_min, y_max, h))</span>
<span id="cb41-42"><a href="#cb41-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-43"><a href="#cb41-43" aria-hidden="true" tabindex="-1"></a><span class="co"># Make predictions on the mesh grid</span></span>
<span id="cb41-44"><a href="#cb41-44" aria-hidden="true" tabindex="-1"></a>mesh_inputs <span class="op">=</span> np.c_[xx.ravel(), yy.ravel()]</span>
<span id="cb41-45"><a href="#cb41-45" aria-hidden="true" tabindex="-1"></a>Z <span class="op">=</span> network.forward(mesh_inputs)</span>
<span id="cb41-46"><a href="#cb41-46" aria-hidden="true" tabindex="-1"></a>Z <span class="op">=</span> (Z <span class="op">&gt;</span> <span class="fl">0.5</span>).reshape(xx.shape)</span>
<span id="cb41-47"><a href="#cb41-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-48"><a href="#cb41-48" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot the decision boundary</span></span>
<span id="cb41-49"><a href="#cb41-49" aria-hidden="true" tabindex="-1"></a>plt.contourf(xx, yy, Z, cmap<span class="op">=</span>plt.cm.Spectral, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb41-50"><a href="#cb41-50" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Feature 1'</span>)</span>
<span id="cb41-51"><a href="#cb41-51" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Feature 2'</span>)</span>
<span id="cb41-52"><a href="#cb41-52" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Feed-Forward Network Decision Boundary'</span>)</span>
<span id="cb41-53"><a href="#cb41-53" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 0, Loss: 0.2501
Epoch 100, Loss: 0.0908
Epoch 200, Loss: 0.0508
Epoch 300, Loss: 0.0239
Epoch 400, Loss: 0.0224
Epoch 500, Loss: 0.0221
Epoch 600, Loss: 0.0218
Epoch 700, Loss: 0.0214
Epoch 800, Loss: 0.0211
Epoch 900, Loss: 0.0209
Epoch 1000, Loss: 0.0207
Epoch 1100, Loss: 0.0206
Epoch 1200, Loss: 0.0205
Epoch 1300, Loss: 0.0205
Epoch 1400, Loss: 0.0204
Epoch 1500, Loss: 0.0204
Epoch 1600, Loss: 0.0203
Epoch 1700, Loss: 0.0203
Epoch 1800, Loss: 0.0202
Epoch 1900, Loss: 0.0202
Final accuracy: 0.98</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="chapter9_files/figure-html/cell-27-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>


</section>
</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./chapter8.html" class="pagination-link" aria-label="Gradient Descent: Optimization in Machine Learning">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Gradient Descent: Optimization in Machine Learning</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./chapter10.html" class="pagination-link" aria-label="Batch Normalization, RNN, Distributed Deep Learning and Tensorflow">
        <span class="nav-page-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Batch Normalization, RNN, Distributed Deep Learning and Tensorflow</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->




</body></html>