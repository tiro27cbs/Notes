---
title: "Hyper-Parameter Optimization (HPO) in Machine Learning"
author: "Data Science Research Team"
date: last-modified
format: 
  html:
    code-fold: true
    code-summary: "Show the code"
  pdf:
    documentclass: report
---

## Introduction

### What is Hyper-Parameter Optimization?

Hyper-Parameter Optimization (HPO) is a critical process in machine learning that involves finding the most effective configuration of model hyperparameters to maximize performance. Unlike standard model parameters that are learned during training, hyperparameters are set before the learning process begins and significantly impact model performance.

### Why is HPO Important in Machine Learning?

Hyperparameter optimization is crucial because: - The right hyperparameters can dramatically improve model accuracy - Poorly chosen hyperparameters can lead to underfitting or overfitting - Manual tuning becomes impractical for complex models with many hyperparameters

### Analogy: Baking a Cake - Ingredients vs. Oven Settings

Think of hyperparameters like oven settings when baking a cake: - Model Parameters = Cake Ingredients (learned during mixing) - Hyperparameters = Oven Temperature and Baking Time (set before baking) - Just as precise oven settings can make or break a cake, hyperparameters can make or break a machine learning model

## Understanding Hyper-Parameters {#sec-understanding-hyperparameters}

### Types of Hyper-Parameters in Neural Networks

Common hyperparameters include: 
- **Learning Rate**: Controls the step size during optimization. 
- **Batch Size**: Determines the number of samples per training batch. 
- **Dropout Rate**: Fraction of neurons dropped during training to prevent overfitting. 
- **Regularization Strength**: Penalizes large weights to improve generalization. 
- **Momentum**: Accelerates gradient descent by considering past gradients. 
- **Network Architecture Parameters**: Includes the number of layers, units per layer, and activation functions.

#### Python Example of Hyperparameters

```{python}
import tensorflow as tf

# Hyperparameters
learning_rate = 0.001  # Learning rate
batch_size = 32        # Batch size
dropout_rate = 0.5     # Dropout rate
regularization = 1e-4  # L2 regularization strength

model = tf.keras.Sequential([
    tf.keras.layers.Dense(64, activation='relu', input_shape=(10,)),
    tf.keras.layers.Dropout(dropout_rate),
    tf.keras.layers.Dense(1, activation='sigmoid')
])

model.compile(
    optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),
    loss='binary_crossentropy',
    metrics=['accuracy']
)
```

### Role of Hyper-Parameters in Model Performance

Hyperparameters play a critical role in determining the behavior and effectiveness of machine learning models. They influence several key aspects, including: 
- **Model Complexity**: Hyperparameters such as the number of layers in a neural network or the depth of a decision tree directly affect how complex the model can be. 
- **Training Dynamics**: Parameters like the learning rate or batch size control how the model learns during training, impacting convergence speed and stability. 
- **Regularization**: Techniques like dropout rates or L2 regularization coefficients help prevent overfitting by constraining the model's capacity. 
- **Generalization Capabilities**: Proper tuning of hyperparameters ensures the model performs well on unseen data, balancing underfitting and overfitting.

### Why Hyper-Parameters Cannot Be Learned Like Normal Parameters

Unlike model parameters (e.g., weights in a neural network), hyperparameters cannot be optimized directly during training due to several reasons: 
- **No Direct Gradient Computation**: Hyperparameters are not part of the model's computational graph, so their impact on the loss function cannot be differentiated to compute gradients. 
- **Discrete or Categorical Nature**: Many hyperparameters, such as the number of layers or choice of activation functions, are not continuous, making gradient-based optimization infeasible. 
- **Computational Expense**: Evaluating different hyperparameter configurations often requires retraining the model from scratch, which can be computationally expensive and time-consuming.

## Challenges in Hyper-Parameter Optimization

### Computational Challenges

1.  **Expensive Function Evaluations**
    -   Large models require significant computational resources
    -   Each hyperparameter configuration needs full model training
2.  **High-Dimensional Search Space**
    -   Multiple hyperparameters to tune
    -   Complex interactions between hyperparameters
3.  **No Direct Gradient Access**
    -   Traditional optimization techniques fail
    -   Cannot use standard gradient descent methods
4.  **Overfitting Risks**

```{python}
import matplotlib.pyplot as plt
import numpy as np
import tensorflow as tf

# Define input variables
input_shape = (10,)
num_classes = 2
X_train = np.random.rand(100, 10)  # Random training data
y_train = np.random.randint(0, 2, size=(100,))  # Random binary labels
X_val = np.random.rand(20, 10)  # Random validation data
y_val = np.random.randint(0, 2, size=(20,))  # Random binary labels

# Define a simple model
model = tf.keras.Sequential([
    tf.keras.layers.Dense(64, activation='relu', input_shape=input_shape),
    tf.keras.layers.Dense(num_classes, activation='softmax')
])

model.compile(
    optimizer=tf.keras.optimizers.Adam(learning_rate=0.01),
    loss='sparse_categorical_crossentropy',
    metrics=['accuracy']
)

def demonstrate_overfitting(model, X_train, y_train, X_val, y_val):
    """
    Demonstrate model overfitting with poor hyperparameter selection
    """
    # Overfitting scenario with no regularization
    history = model.fit(X_train, y_train, 
                        epochs=100,  # Too many epochs
                        batch_size=4,  # Very small batch size
                        validation_data=(X_val, y_val),
                        verbose=0)
    
    # Plot training and validation loss
    epochs = np.arange(1, 101)
    train_loss = history.history['loss']
    val_loss = history.history['val_loss']
    
    plt.figure(figsize=(10, 6))
    plt.plot(epochs, train_loss, label='Training Loss')
    plt.plot(epochs, val_loss, label='Validation Loss')
    plt.title('Overfitting Demonstration')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()
    plt.show()

# Call the function to demonstrate overfitting
demonstrate_overfitting(model, X_train, y_train, X_val, y_val)
```

## Parameter Initialization {#sec-initialization}

### Importance of Weight Initialization

#### Problems with Poor Initialization

-   Symmetry Breaking
-   Vanishing/Exploding Gradients
-   Slow Convergence

#### Initialization Techniques

1.  **Random Initialization**
    -   Simple approach
    -   Risks include uniform weight distribution
2.  **Xavier (Glorot) Initialization**
    -   Maintains variance across layers
    -   Works well for sigmoid and tanh activations
3.  **He Initialization**
    -   Optimized for ReLU activation functions
    -   Prevents vanishing/exploding gradients

``` python
def compare_initializations(input_shape, num_classes):
    """
    Compare different weight initialization techniques
    """
    initializers = {
        'random_normal': tf.keras.initializers.RandomNormal(),
        'xavier': tf.keras.initializers.GlorotUniform(),
        'he': tf.keras.initializers.HeNormal()
    }
    
    for name, initializer in initializers.items():
        model = tf.keras.Sequential([
            tf.keras.layers.Dense(64, activation='relu', 
                                   kernel_initializer=initializer, 
                                   input_shape=input_shape),
            tf.keras.layers.Dense(num_classes, activation='softmax')
        ])
        print(f"Initialization: {name}")
        # Model compilation and training would follow
```

## Learning Rate Strategies {#sec-learning-rate}

### Importance of Learning Rate

The learning rate determines the step size during optimization: - Too high: Model may diverge - Too low: Extremely slow convergence

#### Learning Rate Effects

``` python
def plot_learning_rate_effects(model, X_train, y_train, learning_rates):
    """
    Visualize model performance with different learning rates
    """
    histories = []
    for lr in learning_rates:
        model.optimizer.learning_rate.assign(lr)
        history = model.fit(X_train, y_train, epochs=50, verbose=0)
        histories.append(history)
    
    # Plot learning curves
    plt.figure(figsize=(10, 6))
    for lr, history in zip(learning_rates, histories):
        plt.plot(history.history['loss'], label=f'LR = {lr}')
    plt.title('Learning Rate Impact')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()
```

### Learning Rate Scheduling Techniques

Learning rate scheduling involves strategies to reduce the learning rate during training to improve convergence and performance. A good learning rate is crucial for effective training:

- **Too High**: Training may diverge ("Gradient Descent").
- **Too Low**: Training will eventually converge to the optimum, but at the cost of very long training time.

#### Finding a Good Learning Rate

To find an optimal learning rate:
1. Train the model for a few hundred iterations, exponentially increasing the learning rate from a very small value to a very large value.
2. Examine the learning curve and select a learning rate slightly lower than the point where the learning curve starts shooting back up.
3. Reinitialize the model and train it with the selected learning rate.

#### Common Scheduling Techniques

1. **Power Scheduling**
    - The learning rate decreases as a power function of the iteration number \( t \):  
      \[
      \eta(t) = \frac{\eta_0}{(1 + t/s)^c}
      \]
      - \( \eta_0 \): Initial learning rate  
      - \( c \): Power (commonly set to 1)  
      - \( s \): Steps (hyperparameter)

2. **Exponential Scheduling**
    - The learning rate decreases exponentially:  
      \[
      \eta(t) = \eta_0 \cdot 0.1^{t/s}
      \]
      - Gradually reduces the learning rate by a factor of 10 every \( s \) steps.

3. **Piecewise Constant Scheduling**
    - The learning rate remains constant for specific epoch ranges and then decreases:  
      - Example: \( \eta_0 = 0.1 \) for the first 5 epochs, then \( \eta_1 = 0.001 \) for the next 50 epochs.

4. **Performance-Based Scheduling**
    - The learning rate is reduced adaptively based on validation performance:  
      - Measure validation error every \( N \) steps.  
      - Reduce the learning rate by a factor \( \lambda \) when the error stops improving.

## Hyper-Parameter Search Strategies 

### Optimization Techniques

1. **Babysitting or Trial and Error (Grad Student Descent - GSD)**
    - 100% manual tuning.
    - Widely used but highly inefficient for complex models.
    - Challenges include large number of hyperparameters, time-consuming evaluations, and non-linear interactions.

2. **Grid Search (GS)**
    - Most commonly used method.
    - Exhaustive search or brute-force approach.
    - Evaluates the Cartesian product of user-specified finite sets of values.
    - **Problem**: Inefficient for high-dimensional hyperparameter spaces as the number of evaluations grows exponentially with the number of hyperparameters.

3. **Random Search (RS)**
    - Similar to GS but randomly selects a pre-defined number of samples within bounds.
    - Can explore a larger search space with a limited budget compared to GS.
    - **Problem**: May perform unnecessary evaluations as it does not exploit previously well-performing regions.

4. **Gradient-Based Optimization**
    - Traditional technique that moves in the opposite direction of the largest gradient to locate the next point.
    - Fast convergence speed to reach a local optimum.
    - Commonly used to optimize learning rates in neural networks.
    - **Problem**: May get stuck in local optima and is not suitable for discrete or categorical hyperparameters.

``` python
from sklearn.model_selection import GridSearchCV, RandomizedSearchCV

def compare_search_strategies(model, param_grid, X, y):
    """
    Compare Grid Search vs Random Search
    """
    grid_search = GridSearchCV(model, param_grid, cv=5)
    random_search = RandomizedSearchCV(model, param_grid, n_iter=10, cv=5)
    
    grid_search.fit(X, y)
    random_search.fit(X, y)
    
    print("Best Grid Search Parameters:", grid_search.best_params_)
    print("Best Random Search Parameters:", random_search.best_params_)
```

## Hyper-Parameter Optimization Frameworks 

### Popular Frameworks

1.  **Scikit-learn**
    -   GridSearchCV
    -   RandomizedSearchCV
2.  **TensorFlow/Keras**
    -   KerasTuner
    -   Trieste
3.  **Optuna**
    -   Advanced Bayesian optimization
    -   Efficient HPO for deep learning

## Best Practices 

1.  Start with small dataset subsets
2.  Use logarithmic scaling for hyperparameters
3.  Employ adaptive optimization methods
4.  Parallelize tuning when possible
5.  Use separate validation data

## Conclusion 

### Key Takeaways

-   Hyper-Parameter Optimization is crucial for model performance
-   Multiple strategies exist for exploring hyperparameter space
-   Computational efficiency is key
-   Continuous learning and adaptation of techniques

### Future Trends

-   Automated Machine Learning (AutoML)
-   Meta-Learning approaches
-   More sophisticated optimization algorithms