---
title: "Tree-Based Models and Ensemble Learning"
---

## Decision Trees

Decision trees are versatile machine learning models that can handle both classification and regression tasks. They're powerful tools for inductive inference and particularly useful for approximating discrete-valued target functions.

### Key Features

- **Robust to Noisy Data**: Handle imperfections in data, including noise and missing values
- **Complex Datasets**: Capable of fitting complex datasets and representing disjunctive expressions
- **Interpretable**: Trees model decisions through a series of "if/else" questions, providing clear decision-making processes

### Core Concepts

Decision trees operate by recursively partitioning the feature space based on the values of input features. At each node:

1. **Feature Selection**: Choose the most informative feature to split on
2. **Splitting Criterion**: Determine the optimal threshold or condition for the split
3. **Recursive Partitioning**: Continue splitting until stopping criteria are met

The algorithm aims to create homogeneous subsets with respect to the target variable, maximizing information gain at each step.

### Decision Boundaries

Decision trees create piecewise constant decision boundaries that are parallel to the feature axes. This characteristic leads to:

- **Rectangular Partitioning**: Each leaf represents a rectangular region in the feature space
- **Orthogonal Boundaries**: Decision boundaries are always perpendicular to feature axes
- **Staircase Effect**: Complex functions are approximated using axis-parallel rectangles

```{python}
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import load_iris
from sklearn.tree import DecisionTreeClassifier, plot_tree
from sklearn.model_selection import train_test_split

# Load dataset
iris = load_iris()
X, y = iris.data, iris.target

# Split data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Create and train decision tree
clf = DecisionTreeClassifier(max_depth=3, random_state=42)
clf.fit(X_train, y_train)

# Visualize decision tree
plt.figure(figsize=(12, 8))
plot_tree(clf, filled=True, feature_names=iris.feature_names, class_names=iris.target_names)
plt.title("Decision Tree on Iris Dataset")
plt.show()

# Evaluate performance
train_score = clf.score(X_train, y_train)
test_score = clf.score(X_test, y_test)
print(f"Training accuracy: {train_score:.3f}")
print(f"Testing accuracy: {test_score:.3f}")
```

### Representation

Decision trees represent a disjunction (OR) of conjunctions (AND) of constraints on attribute values:

- Each path from root to leaf represents a conjunction (AND) of tests on instance attributes
- The entire tree is a disjunction (OR) of these conjunctions
- Each leaf node represents a classification outcome

### When to Use Decision Trees

- **Attribute-Value Pair Representation**: Instances are represented as attribute-value pairs
- **Discrete Output**: Target function has discrete output values (classification tasks)
- **Disjunctive Descriptions**: Need to represent logical ORs
- **Noisy Data**: Training data contains errors or missing values

### Classification Process

Trees classify instances by sorting them from root to leaf:
- Each node tests an attribute
- Each branch corresponds to a possible attribute value
- Each leaf assigns a class label

### Algorithmic Approaches

Several algorithms exist for constructing decision trees:

1. **ID3 (Iterative Dichotomiser 3)**: Uses entropy and information gain
2. **C4.5**: Extends ID3 by handling continuous attributes and missing values
3. **CART (Classification and Regression Trees)**: Uses Gini impurity for classification
4. **CHAID (Chi-square Automatic Interaction Detector)**: Uses chi-square tests for categorical outputs

### The CART Algorithm

CART (Classification and Regression Trees) is one of the most popular decision tree algorithms:

1. Start with all data at the root node
2. For each feature, find the best split that minimizes impurity
3. Split the data based on the best feature and threshold
4. Recursively apply steps 2-3 to the child nodes
5. Stop when a stopping criterion is met (e.g., max depth, min samples)

### Training a Decision Tree

The training process involves finding the best set of questions (splits) to divide the data:

```{python}
class Leaf:
    def __init__(self, value):
        self.value = value

class Node:
    def __init__(self, attribute):
        self.attribute = attribute
        self.branches = {}

    def add_branch(self, value, subtree):
        self.branches[value] = subtree

def basic_decision_tree_algorithm(examples, target_attribute, attributes):
    """
    Basic implementation of a decision tree algorithm
    """
    # If all examples have the same value for the target attribute, return a leaf node
    if len(set(ex[target_attribute] for ex in examples)) == 1:
        return Leaf(examples[0][target_attribute])
    
    # If no attributes are left, return a leaf node with the majority value
    if not attributes:
        majority_value = max(set(ex[target_attribute] for ex in examples), key=lambda val: sum(ex[target_attribute] == val for ex in examples))
        return Leaf(majority_value)
    
    # Choose the best attribute to split on (placeholder for actual selection logic)
    best_attribute = attributes[0]  # Replace with actual logic to select the best attribute
    
    # Create a new decision tree node
    tree = Node(best_attribute)
    
    # Get unique values for the best attribute
    unique_values = set(ex[best_attribute] for ex in examples)
    
    for value in unique_values:
        # Create a subset of examples where the best attribute equals the current value
        subset = [ex for ex in examples if ex[best_attribute] == value]
        
        if not subset:
            # If the subset is empty, add a leaf with the majority value
            majority_value = max(set(ex[target_attribute] for ex in examples), key=lambda val: sum(ex[target_attribute] == val for ex in examples))
            tree.add_branch(value, Leaf(majority_value))
        else:
            # Recursively build the subtree
            subtree = basic_decision_tree_algorithm(subset, target_attribute, [attr for attr in attributes if attr != best_attribute])
            tree.add_branch(value, subtree)
    
    return tree
```

#### Impurity Measures

To decide the best feature to split on, decision trees use impurity measures:

- **Gini Index**: Measures the likelihood of misclassifying a randomly selected instance
- **Entropy**: Measures disorder or uncertainty in the dataset
- **Misclassification Error**: Proportion of misclassified instances

##### Entropy

Entropy quantifies the uncertainty or randomness in a set of examples:

$H(S) = -\sum_{i=1}^{c} p_i \log_2(p_i)$

Where:
- $S$ is the dataset
- $c$ is the number of classes
- $p_i$ is the proportion of examples in class $i$

##### Information Gain

Information gain measures the reduction in entropy after splitting on an attribute:

$IG(S, A) = H(S) - \sum_{v \in Values(A)} \frac{|S_v|}{|S|} H(S_v)$

Where:
- $A$ is the attribute
- $S_v$ is the subset of $S$ for which attribute $A$ has value $v$

##### Gini Impurity

Gini impurity measures the probability of incorrectly classifying a randomly chosen element if it were randomly labeled according to the class distribution in the subset:

$Gini(S) = 1 - \sum_{i=1}^{c} (p_i)^2$

```{python}
def calculate_entropy(y):
    """Calculate entropy of a label set"""
    classes, counts = np.unique(y, return_counts=True)
    probabilities = counts / counts.sum()
    entropy = -np.sum(probabilities * np.log2(probabilities))
    return entropy

def calculate_gini(y):
    """Calculate Gini impurity of a label set"""
    classes, counts = np.unique(y, return_counts=True)
    probabilities = counts / counts.sum()
    gini = 1 - np.sum(probabilities**2)
    return gini

# Example calculation
sample_labels = np.array([0, 0, 1, 1, 0, 1, 0, 1])
print(f"Entropy: {calculate_entropy(sample_labels):.4f}")
print(f"Gini: {calculate_gini(sample_labels):.4f}")
```

### Pruning Techniques

Decision trees are prone to overfitting, especially when they grow too deep. Pruning helps mitigate this:

#### Pre-pruning (Early Stopping)

Stops the tree from growing before it perfectly fits the training data:
- Maximum depth limit
- Minimum samples per leaf
- Minimum impurity decrease

#### Post-pruning

Builds the full tree, then removes branches that don't improve generalization:
- Cost-complexity pruning (Minimal Cost-Complexity Pruning)
- Reduced Error Pruning
- Pessimistic Error Pruning

### Handling Categorical and Continuous Features

Decision trees can handle both categorical and continuous features:

- **Categorical Features**: Create branches for each category or group similar categories
- **Continuous Features**: Find the optimal threshold that maximizes information gain

### Advantages and Limitations

#### Advantages
- Intuitive and easy to explain
- Require little data preprocessing
- Handle numerical and categorical data
- Non-parametric (no assumptions about data distribution)
- Handle missing values and outliers effectively

#### Limitations
- Prone to overfitting
- Biased toward features with more levels
- Unstable (small changes in data can result in very different trees)
- Struggle with diagonal decision boundaries
- May create biased trees if classes are imbalanced

## Ensemble Methods

Ensemble methods combine multiple predictors to improve accuracy by reducing variance and bias.

### Core Principles

Ensemble methods work based on the "wisdom of crowds" principle:

1. **Diversity**: Individual models make different errors
2. **Independence**: Errors are uncorrelated
3. **Aggregation**: Combining predictions reduces overall error

### The Bias-Variance Tradeoff

Ensemble methods address the fundamental bias-variance tradeoff:

- **Bias**: Error from incorrect assumptions in the learning algorithm
- **Variance**: Error from sensitivity to small fluctuations in the training set
- **Total Error** = Bias² + Variance + Irreducible Error

Different ensemble techniques target different components of this error:
- Bagging primarily reduces variance
- Boosting reduces both bias and variance

```{python}
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.metrics import accuracy_score
import pandas as pd

# Create ensemble models
rf_model = RandomForestClassifier(n_estimators=100, random_state=42)
gb_model = GradientBoostingClassifier(n_estimators=100, random_state=42)

# Train models
rf_model.fit(X_train, y_train)
gb_model.fit(X_train, y_train)

# Make predictions
rf_preds = rf_model.predict(X_test)
gb_preds = gb_model.predict(X_test)

# Compare accuracies
results = pd.DataFrame({
    'Model': ['Decision Tree', 'Random Forest', 'Gradient Boosting'],
    'Test Accuracy': [
        clf.score(X_test, y_test),
        accuracy_score(y_test, rf_preds),
        accuracy_score(y_test, gb_preds)
    ]
})

print(results)
```

### Types of Ensemble Learning

1. **Bagging (Bootstrap Aggregating)**:
   - Builds independent predictors and combines them
   - Models trained on bootstrapped datasets (random samples with replacement)
   - Reduces variance, effective against overfitting
   - Example: Random Forest

2. **Boosting**:
   - Builds predictors sequentially, each correcting errors of previous models
   - Assigns higher weights to misclassified data points
   - Reduces both bias and variance
   - Examples: AdaBoost, Gradient Boosting, XGBoost

3. **Stacking**:
   - Combines multiple models using another model (meta-learner)
   - Base models make predictions independently
   - Meta-learner learns how to combine these predictions optimally
   - Examples: Stacked Generalization, Blending

4. **Voting**:
   - Simple aggregation of predictions from multiple models
   - Hard Voting: Majority vote for classification
   - Soft Voting: Weighted average of probabilities
   - Works best with diverse, uncorrelated models

### Theoretical Foundations

The power of ensemble methods is backed by mathematical proofs:

- **Condorcet's Jury Theorem**: As the number of independent, better-than-random models increases, the probability of a correct majority vote approaches 1
- **Bias-Variance Decomposition**: Ensembles can reduce variance without increasing bias
- **No Free Lunch Theorem**: No single model is optimal for all problems, but ensembles can adapt to different problem structures

## Random Forests

Random Forest is an ensemble method combining multiple decision trees through bagging.

### Core Concepts

Random Forests extend the bagging idea with additional randomness:

1. **Bootstrap Sampling**: Each tree is trained on a random subset of data
2. **Feature Randomization**: At each node, consider only a random subset of features
3. **Ensemble Aggregation**: Combine predictions through voting (classification) or averaging (regression)

### Random Forest Algorithm

1. Create n_estimators bootstrap samples from the original dataset
2. For each sample, grow a decision tree with the following modification:
   - At each node, randomly select m features (typically m ≈ sqrt(total features))
   - Split on the best feature among the m features
3. Predict new data by aggregating predictions from all trees

```{python}
# Train a Random Forest with different parameters
rf = RandomForestClassifier(
    n_estimators=100,
    max_depth=4,
    min_samples_split=10,
    random_state=42
)
rf.fit(X_train, y_train)

# Get feature importances
importances = pd.DataFrame({
    'Feature': iris.feature_names,
    'Importance': rf.feature_importances_
}).sort_values('Importance', ascending=False)

# Plot feature importances
plt.figure(figsize=(10, 6))
plt.barh(importances['Feature'], importances['Importance'])
plt.xlabel('Importance')
plt.title('Feature Importances from Random Forest')
plt.tight_layout()
plt.show()

print(importances)
```

### How Random Forest Works

- Each tree is trained independently on a random subset of the data
- Additional randomness is introduced by selecting a random subset of features at each node
- Final output is determined by aggregating results from all trees (majority voting for classification, averaging for regression)

### Key Features of Random Forests

- Controls the growth of individual trees
- Controls the ensemble as a whole through bagging parameters
- Randomness at each decision tree's growth stage increases tree diversity

### Mathematical Intuition

The error rate of a random forest depends on:

1. **Correlation between trees**: Lower correlation improves performance
2. **Strength of individual trees**: Stronger trees improve performance

The feature randomization helps reduce correlation between trees while maintaining their strength.

### Out-of-Bag (OOB) Error Estimation

A unique advantage of random forests is built-in validation:

- Each bootstrap sample leaves out approximately 1/3 of the data (out-of-bag samples)
- These OOB samples can be used to estimate model performance without a separate validation set
- OOB error is an unbiased estimate of the generalization error

### Feature Importance

Random forests provide a natural way to measure feature importance:

1. **Mean Decrease Impurity (MDI)**: Average reduction in impurity across all trees
2. **Mean Decrease Accuracy (MDA)**: Decrease in model accuracy when a feature is permuted
3. **Permutation Importance**: Randomize one feature at a time and measure the drop in performance

### Proximity Analysis

Random forests can measure the similarity between data points:

- Two points are "close" if they often end up in the same leaf nodes
- This proximity measure can be used for clustering, outlier detection, and missing value imputation

### Hyperparameters

Key parameters affecting random forest performance:

- **n_estimators**: Number of trees (more is usually better)
- **max_features**: Number of features to consider at each split
- **max_depth**: Maximum depth of each tree
- **min_samples_split**: Minimum samples required to split a node
- **min_samples_leaf**: Minimum samples required in a leaf node
- **bootstrap**: Whether to use bootstrap sampling

### Advantages and Limitations

#### Advantages
- Reduced overfitting compared to decision trees
- Robust to outliers and noise
- Handles high-dimensional data well
- Provides feature importance measures
- Built-in cross-validation through OOB samples

#### Limitations
- Less interpretable than single decision trees
- Computationally more intensive
- Biased for categorical features with different numbers of levels
- May overfit on noisy datasets with many features

## Gradient Boosting

Gradient Boosting combines Gradient Descent with boosting principles.

### Core Concepts

Gradient Boosting frames the ensemble learning process as an optimization problem:

1. **Loss Function**: Define a differentiable loss function to minimize
2. **Weak Learners**: Use simple models (typically shallow decision trees)
3. **Additive Training**: Build models sequentially to minimize the loss function
4. **Gradient Descent**: Each new model fits the negative gradient of the loss function

### Gradient Boosting Algorithm

1. Initialize model with a constant value
2. For m = 1 to M (number of boosting rounds):
   - Compute negative gradient (residual) of the loss function
   - Fit a base learner (decision tree) to the negative gradient
   - Calculate optimal leaf values
   - Update the model by adding the new tree (scaled by learning rate)
3. Return the final model (sum of all trees)

```{python}
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.metrics import confusion_matrix, classification_report

# Create and train Gradient Boosting model
gb = GradientBoostingClassifier(
    n_estimators=100,
    learning_rate=0.1,
    max_depth=3,
    random_state=42
)
gb.fit(X_train, y_train)

# Predict and evaluate
y_pred = gb.predict(X_test)
print(confusion_matrix(y_test, y_pred))
print(classification_report(y_test, y_pred, target_names=iris.target_names))

# Learning curve visualization
train_scores = []
test_scores = []
estimator_range = range(1, 101)

# Train models with different numbers of estimators
for n_estimators in estimator_range:
    gb = GradientBoostingClassifier(
        n_estimators=n_estimators,
        learning_rate=0.1,
        max_depth=3,
        random_state=42
    )
    gb.fit(X_train, y_train)
    train_scores.append(gb.score(X_train, y_train))
    test_scores.append(gb.score(X_test, y_test))

# Plot learning curve
plt.figure(figsize=(10, 6))
plt.plot(estimator_range, train_scores, label='Training Score')
plt.plot(estimator_range, test_scores, label='Testing Score')
plt.xlabel('Number of Estimators')
plt.ylabel('Accuracy')
plt.title('Gradient Boosting Learning Curve')
plt.legend()
plt.grid(True)
plt.show()
```

### How Gradient Boosting Works

1. Initial Model: Start with one model and its predictions
2. Training on Residuals: Train new model on residuals of first model
3. Iterative Correction: Each new model predicts residuals from ensemble of previous models
4. Final Prediction: Sum of predictions from all trees

### Mathematical Formulation

Gradient Boosting minimizes a loss function $L(y, F(x))$ by iteratively adding weak learners:

$F_m(x) = F_{m-1}(x) + \alpha_m h_m(x)$

Where:
- $F_m(x)$ is the model after m iterations
- $h_m(x)$ is the weak learner (decision tree)
- $\alpha_m$ is the step size (learning rate)

The weak learner $h_m$ is trained to approximate the negative gradient of the loss function:

$h_m(x) \approx -\left[\frac{\partial L(y, F(x))}{\partial F(x)}\right]_{F(x)=F_{m-1}(x)}$

### Loss Functions

Different loss functions can be used depending on the task:

- **Regression**: 
  - L2 loss (mean squared error)
  - L1 loss (mean absolute error)
  - Huber loss (robust to outliers)

- **Classification**:
  - Binomial deviance (logistic loss)
  - Multinomial deviance
  - Exponential loss (AdaBoost)

### Types of Gradient Boosting

- **AdaBoost**: Each new model focuses on mistakes of previous models by weighting misclassified instances
- **XGBoost**: Highly efficient implementation with additional optimizations like regularization
- **LightGBM**: Uses gradient-based one-side sampling and exclusive feature bundling for faster training
- **CatBoost**: Handles categorical features automatically and uses ordered boosting

### Regularization Techniques

Gradient Boosting can overfit easily. Common regularization techniques include:

1. **Shrinkage (Learning Rate)**: Scale contribution of each tree by a factor < 1
2. **Subsampling**: Train each tree on a random subset of data
3. **Early Stopping**: Stop training when validation error stops improving
4. **Tree Constraints**: Limit tree depth, minimum samples per leaf, etc.
5. **L1/L2 Regularization**: Penalize large leaf weights

### Key Hyperparameters

- **n_estimators**: Number of boosting stages (trees)
- **learning_rate**: Controls how much each tree influences predictions
- **max_depth**: Limits nodes in each regression estimator (tree)
- **subsample**: Fraction of samples to use for fitting each tree
- **loss**: Loss function to be optimized

### Advantages and Limitations

#### Advantages
- Often provides best predictive accuracy
- Flexible - works with various loss functions
- Handles mixed data types well
- Robust to outliers with robust loss functions
- Automatically handles feature interactions

#### Limitations
- Prone to overfitting without careful tuning
- Sensitive to noisy data and outliers with some loss functions
- Computationally intensive
- Less interpretable than single decision trees
- Sequential nature limits parallelization

## Comparing Ensemble Methods

```{python}
# Compare different ensemble methods
from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier, GradientBoostingClassifier
from sklearn.metrics import accuracy_score

# Initialize models
models = {
    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),
    'AdaBoost': AdaBoostClassifier(n_estimators=100, random_state=42),
    'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, random_state=42)
}

# Train and evaluate each model
results = {}
for name, model in models.items():
    model.fit(X_train, y_train)
    train_acc = accuracy_score(y_train, model.predict(X_train))
    test_acc = accuracy_score(y_test, model.predict(X_test))
    results[name] = {'Train Accuracy': train_acc, 'Test Accuracy': test_acc}

# Display results
results_df = pd.DataFrame(results).T
print(results_df)

# Plot results
plt.figure(figsize=(10, 6))
results_df.plot(kind='bar', figsize=(10, 6))
plt.title('Comparison of Ensemble Methods')
plt.ylabel('Accuracy')
plt.ylim(0.8, 1.0)  # Adjust if needed
plt.tight_layout()
plt.show()
```

### Bagging vs. Boosting

#### Bagging (Random Forest)
- **Goal**: Reduce variance (overfitting)
- **Training**: Parallel (trees are independent)
- **Weighting**: Equal weight for all models
- **Bias-Variance**: Primarily reduces variance
- **Robustness**: Less prone to overfitting
- **Speed**: Can be parallelized easily

#### Boosting (Gradient Boosting)
- **Goal**: Reduce bias and variance
- **Training**: Sequential (each tree depends on previous trees)
- **Weighting**: Different weights for different models
- **Bias-Variance**: Reduces both bias and variance
- **Robustness**: More prone to overfitting
- **Speed**: Generally slower due to sequential nature

### Stacking

Stacking combines multiple models using a meta-learner:

1. Train base models on the original dataset
2. Generate predictions from each base model
3. Use these predictions as features to train a meta-model
4. Final prediction is given by the meta-model

### Choosing the Right Ensemble Method

The choice depends on the problem characteristics:

- **Random Forest**: Good default for most problems, especially with limited data
- **Gradient Boosting**: When maximum performance is needed and you can tune hyperparameters
- **AdaBoost**: Simple boosting algorithm, good for weak learners
- **Stacking**: When you have diverse models and computational resources
- **Voting**: Simple ensemble when you already have several good models

### Practical Considerations

When implementing ensemble methods:

1. **Computational Resources**: Boosting methods are generally more resource-intensive
2. **Model Complexity**: Simpler models may be preferred for production
3. **Interpretability Requirements**: Random forests offer better interpretability than boosting
4. **Dataset Size**: For small datasets, random forests may be more appropriate
5. **Noise Level**: For noisy data, bagging methods are more robust

## Conclusion

- Decision Trees provide interpretable models for both classification and regression
- Ensemble methods like Random Forest and Gradient Boosting improve upon Decision Trees by combining multiple models
- Random Forest reduces variance through bagging and random feature selection
- Gradient Boosting reduces both bias and variance through sequential model building
- Each method has strengths and weaknesses depending on the specific problem and dataset

### Key Takeaways

1. **No Free Lunch**: No single algorithm is best for all problems
2. **Bias-Variance Tradeoff**: Different ensemble methods address different aspects of model error
3. **Hyperparameter Tuning**: Proper tuning is crucial for optimal performance
4. **Interpretability vs. Performance**: More complex ensembles usually offer better performance at the cost of interpretability
5. **Computational Considerations**: Training time and resource requirements vary significantly between methods

```{python}
# Final comprehensive example: train models on different datasets and compare

from sklearn.datasets import load_breast_cancer, load_wine
from sklearn.preprocessing import StandardScaler

datasets = {
    'Iris': load_iris(),
    'Breast Cancer': load_breast_cancer(),
    'Wine': load_wine()
}

results = []

for name, dataset in datasets.items():
    X, y = dataset.data, dataset.target
    
    # Scale features
    scaler = StandardScaler()
    X = scaler.fit_transform(X)
    
    # Split data
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)
    
    # Train models
    dt = DecisionTreeClassifier(max_depth=4, random_state=42)
    rf = RandomForestClassifier(n_estimators=100, max_depth=4, random_state=42)
    gb = GradientBoostingClassifier(n_estimators=100, max_depth=3, random_state=42)
    
    models = {'Decision Tree': dt, 'Random Forest': rf, 'Gradient Boosting': gb}
    
    # Evaluate
    for model_name, model in models.items():
        model.fit(X_train, y_train)
        train_acc = model.score(X_train, y_train)
        test_acc = model.score(X_test, y_test)
        
        results.append({
            'Dataset': name,
            'Model': model_name,
            'Train Accuracy': train_acc,
            'Test Accuracy': test_acc
        })

# Create DataFrame with results
final_results = pd.DataFrame(results)
print(final_results.pivot_table(index='Dataset', columns='Model', values='Test Accuracy'))
```

### Further Research Directions

- **Explainable AI**: Methods to interpret complex ensemble models
- **Automatic Machine Learning (AutoML)**: Automating the selection and tuning of ensemble methods
- **Deep Forest**: Combining deep learning concepts with random forests
- **Online Learning**: Adapting ensemble methods for streaming data
- **Imbalanced Learning**: Specialized ensemble techniques for imbalanced datasets
